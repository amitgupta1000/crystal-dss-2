# -*- coding: utf-8 -*-
"""QUANT_ANALYSIS_MAIN_DSS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1d0V9LCruOwJH3zwsdqGtPE9L2GMD4D9T

# QUANT ANALYSIS

# Configuration
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install --upgrade prophet nest_asyncio yfinance python-dotenv
# !pip install google-cloud-storage
# import os
# import sys
# import urllib.request
# from urllib.request import urlopen
# import yfinance as yf
# from dotenv import load_dotenv
# import pandas as pd
# import numpy as np
# from datetime import date
# import time
# import matplotlib.pyplot as plt
# import matplotlib.lines as mlines
# from sklearn.model_selection import train_test_split
# from sklearn.linear_model import LinearRegression, Ridge  # Import Ridge
# from sklearn.preprocessing import PolynomialFeatures, StandardScaler # Import StandardScaler
# from sklearn.metrics import r2_score, mean_squared_error
# from statsmodels.tsa.stattools import grangercausalitytests
# import gspread
# 
# import asyncio, aiohttp
# import pandas as pd, numpy as np
# from google.colab import drive
# 
# from google.cloud import storage
# from google.colab import auth
# from google.auth import default
# 
# # Authenticate to Google Cloud
# auth.authenticate_user()
# creds, _ = default()
# 
# # Initialize the Google Cloud Storage client
# storage_client = storage.Client(credentials=creds)
# bucket_name = 'crystal-dss'
# print("Google Cloud Storage client initialized.")
# 
# #==================================#============================================
# load_dotenv()

import pandas as pd
from google.cloud import storage
from google.api_core import exceptions # Import for error handling
import io

def save_dataframe_to_gcs(df: pd.DataFrame, bucket_name: str, gcs_prefix: str, validate_rows: bool = True):
    """
    Saves a pandas DataFrame to Google Cloud Storage as a CSV file.

    Args:
        df (pd.DataFrame): The DataFrame to save.
        bucket_name (str): The name of the GCS bucket.
        gcs_prefix (str): The GCS prefix (folder path) and filename for the CSV file (e.g., 'my_folder/my_file.csv').
        validate_rows (bool): If True, rows with any NaN or entirely blank string values are skipped.
    """
    # Create a GCS client (assuming authentication has already been done)
    client = storage.Client()

    # Apply data validation if requested
    if validate_rows:
        # Convert all columns to string type to consistently check for blank strings
        df_str = df.astype(str)

        # Identify rows with any NaN values in the original DataFrame
        nan_rows = df.isnull().any(axis=1)

        # Identify rows with any entirely blank strings in the string-converted DataFrame
        # A blank string is often represented as just whitespace or empty string after stripping
        blank_rows = (df_str.apply(lambda x: x.str.strip() == '', axis=1)).any(axis=1)

        # Combine conditions: drop rows if they have NaN OR entirely blank strings
        rows_to_keep = ~(nan_rows | blank_rows)
        df_validated = df[rows_to_keep].copy()

        num_skipped = len(df) - len(df_validated)
        if num_skipped > 0:
            print(f"Skipped {num_skipped} rows due to NaN or entirely blank values.")
        df = df_validated

    if df.empty:
        print("DataFrame is empty after validation. No file will be uploaded.")
        return

    try:
        # Get the bucket
        bucket = client.get_bucket(bucket_name)
    except exceptions.NotFound:
        print(f"Error: Bucket '{bucket_name}' not found. Please ensure the bucket name is correct.")
        return
    except exceptions.Forbidden:
        print(f"Error: Permission denied for bucket '{bucket_name}'. Please check your GCP permissions.")
        return
    except Exception as e:
        print(f"An unexpected error occurred while accessing bucket '{bucket_name}': {e}")
        return

    # Convert DataFrame to CSV in-memory
    csv_buffer = io.StringIO()
    df.to_csv(csv_buffer, index=False)

    try:
        # Upload the CSV to GCS
        blob = bucket.blob(gcs_prefix)
        blob.upload_from_string(csv_buffer.getvalue(), content_type='text/csv')
        print(f"DataFrame successfully saved to gs://{bucket_name}/{gcs_prefix}")
    except exceptions.Forbidden:
        print(f"Error: Permission denied when uploading to gs://{bucket_name}/{gcs_prefix}. Please check your GCP permissions.")
    except Exception as e:
        print(f"An unexpected error occurred while uploading to GCS: {e}")

print("Utility function `save_dataframe_to_gcs` defined with enhanced error handling.")

"""
gcs_prefix = 'cleaned_data/my_validated_dataframe.csv' # Using the provided prefix
save_dataframe_to_gcs(
    df=dummy_df,
    bucket_name= bucket_name,
    gcs_prefix= gcs_prefix,
    validate_rows=True
)
"""

"""# Create the dataframe"""

from google.colab import files
print("Please upload your Excel workbook (.xlsx file) containing the BA_RAW, PLATTS_RAW, and ICIS_RAW sheets.")
uploaded_file = files.upload()

print("File uploaded successfully.")

#=========1.create the globals dataframe#=================
### List of Global Macro Commodities
list = ['^GSPC', '000001.SS', 'DX=F', 'JPY=X', 'CL=F', 'NG=F', 'GC=F', 'SI=F', 'HG=F', 'ZN=F', ]

# Create a dictionary to map original commodity labels to desired ones
commodity_mapping = {
    'HG=F': 'Copper', 'SI=F': 'Silver', 'GC=F': 'Gold', 'NG=F': 'Natural Gas',
    'BZ=F': 'Brent Crude', 'CL=F': 'Crude Oil', 'INR=X': 'Indian Rupee','JPY=X': 'Japanese Yen',
    'EURUSD=X': 'Euro', 'DX=F': 'USD Index', '^NSEI': 'Nifty 50', '^IXIC': 'NASDAQ',
    '^GSPC': 'S&P 500','000001.SS': 'Shanghai Composite', 'ZN=F': 'US 10-Y BOND PRICE',
}
#==================================#============================================
all_close_data = pd.DataFrame()
for commodity in list:
    try:
        ticker = yf.Ticker(commodity)
        data = ticker.history(period='6y', interval='1wk') # use 1d or m interval depending on the use case
        # Valid intervals: [1m, 2m, 5m, 15m, 30m, 60m, 90m, 1h, 4h, 1d, 5d, 1wk, 1mo, 3mo]

        data = data.reset_index()
        # Format the date column to only show the date
        data['date'] = data['Date'].dt.strftime('%Y-%m-%d')

        # Rename 'Close' and apply commodity mapping using a lambda function
        close_data = data[['date', 'Close']].rename(columns=lambda x: f"{commodity_mapping.get(commodity, commodity)}" if x == 'Close' else x)

        all_close_data = pd.concat([all_close_data, close_data], axis=1)
        # Respectful waiting
        time.sleep(2) # Wait for 2 seconds before the next request
    except Exception as e:
        print(f"Error fetching data for {ticker}: {str(e)}")

# Drop duplicate date columns, keeping the first one
df = all_close_data.loc[:,~all_close_data.columns.duplicated(keep='first')].copy()
#print(df.shape)
#print(df.head(2))
df = df.ffill()
GLOBALS_RAW = df.copy()

#=========2.upload PLATTS sheet#=================
import io

# Get the uploaded filename
if uploaded_file:
    excel_file_name = next(iter(uploaded_file))
    print(f"Uploaded file: {excel_file_name}")
else:
    print("No file was uploaded or the upload failed. Please ensure a file is selected and uploaded successfully in the previous step.")
    excel_file_name = None # Or raise an error, depending on desired control flow

# Get the uploaded filename from the uploaded_file dictionary
if uploaded_file:
    excel_file_name = next(iter(uploaded_file))
    print(f"Detected uploaded file: {excel_file_name}")
else:
    print("No file was uploaded or the upload failed. Please ensure a file is selected and uploaded successfully in the previous step.")
    excel_file_name = None # Or raise an error, depending on desired control flow

#=========2.upload PLATTS sheet#=================
if excel_file_name: # Check if a file was successfully uploaded
    # Use io.BytesIO to read the content of the uploaded Excel file
    excel_data = io.BytesIO(uploaded_file[excel_file_name])

    # Load the 'PLATTS_RAW' sheet from the BytesIO object into a pandas DataFrame
    PLATTS_RAW_DF = pd.read_excel(excel_data, sheet_name='PLATTS_RAW')

    print("PLATTS_RAW sheet loaded successfully into PLATTS_RAW_DF.")
    #print(PLATTS_RAW_DF.head(2))
else:
    print("Cannot load PLATTS_RAW sheet: No Excel file was uploaded or its name could not be determined.")

for col in PLATTS_RAW_DF.columns:
    PLATTS_RAW_DF[col] = PLATTS_RAW_DF[col].astype(str)
#print("All columns in PLATTS_RAW_DF converted to string type.")

# GLOBALS_RAW was already created in a previous step, so ensuring it's processed here
if 'GLOBALS_RAW' in locals() and isinstance(GLOBALS_RAW, pd.DataFrame):
    for col in GLOBALS_RAW.columns:
        GLOBALS_RAW[col] = GLOBALS_RAW[col].astype(str)
    #print("All columns in GLOBALS_RAW converted to string type.")
else:
    print("GLOBALS_RAW DataFrame not found or not a DataFrame. Skipping string conversion for GLOBALS_RAW.")

#=========3. Clean the dataframes#=================
def clean_and_format_df(df, divisor=None):
    # 1. Rename first column as 'date'
    df = df.rename(columns={df.columns[0]: 'date'})

    # Convert 'date' column to datetime objects first for proper sorting
    df['date'] = pd.to_datetime(df['date'], errors='coerce')

    # Drop rows where date conversion failed (if any)
    df = df.dropna(subset=['date'])

    # 2. Sort all data in ascending date order
    df = df.sort_values(by='date', ascending=True)

    # 3. Format date values as mm-yy
    df['date'] = df['date'].dt.strftime('%d-%m-%y')

    # 4. Format all other columns as numeric and format to two decimal places
    for col in df.columns:
        if col != 'date':
            df[col] = pd.to_numeric(df[col], errors='coerce')
            if divisor is not None:
                df[col] = df[col] / divisor
            df[col] = df[col].round(2)

    return df

#print("Utility function `clean_and_format_df` created.")

### GLOBALS CLEANER================================================================
# Make a copy of the GLOBALS_RAW DataFrame
globals = GLOBALS_RAW.copy()
#print(f"Original GLOBALS_RAW_DF Shape: {globals.shape}")

# Apply the clean_and_format_df utility function directly
globals_cleaned = clean_and_format_df(globals)
print(f"Cleaned GLOBALS_DF Shape: {globals_cleaned.shape}")
#print(globals_cleaned.head(2))

### PLATTS CLEANER===============================================================
# 1. Make a copy of the PLATTS_RAW_DF DataFrame
platts = PLATTS_RAW_DF.copy()
print(f"Original Shape : {platts.shape}")

# Rename the 'Unnamed: 0' column to 'date' as it's the date identifier
if 'Unnamed: 0' in platts.columns:
    platts = platts.rename(columns={'Unnamed: 0': 'date'})
    print("Renamed 'Unnamed: 0' column to 'date'.")

# Drop the first 3 rows, which contain unit information and other metadata
# These rows are: index 0 (MT), index 1 (USD), index 2 (c/AssessDate)
platts = platts.iloc[3:].copy()
platts.reset_index(drop=True, inplace=True)
print("Dropped the first 3 rows containing metadata.")

# Apply the clean_and_format_df utility function with divisor
platts_cleaned = clean_and_format_df(platts, divisor=1000)
platts_cleaned = platts_cleaned.ffill()
print(f"Cleaned PLATTS_DF Shape: {platts_cleaned.shape}")

# --- New code to clean PLATTS headers ---
# Define a list of keywords to look for
keywords = ['DAP', 'FCA', 'FOB', 'CIF', 'ARA', 'CFR', 'FCA']

def clean_platts_header(header):
    if header == 'date':  # Keep 'date' column name as is
        return header
    for keyword in keywords:
        if keyword in header.upper():  # Check for keyword (case-insensitive)
            # Find the index of the keyword and take the part before it
            return header.split(keyword)[0].strip()
    return header  # Return original header if no keyword is found

# Apply the cleaning function to all column names except 'date'
platts_cleaned.columns = [clean_platts_header(col) for col in platts_cleaned.columns]
print("Cleaned PLATTS headers.")
#print(platts_cleaned.head(2))

#=======================#========================#========================
### Concatenate the DataFrames. We'll use a left join to keep all dates from the globals dataframe and corresponding data from platts_cleaned.
# Perform a left merge on the 'date' column, keeping all dates from globals_cleaned
concatenated_df = pd.merge(globals_cleaned, platts_cleaned, on='date', how='left')
concatenated_df = concatenated_df.ffill()

# Convert 'date' column back to datetime objects for proper sorting if needed, then reformat to mm-yy
concatenated_df['date'] = pd.to_datetime(concatenated_df['date'], format='%d-%m-%y', errors='coerce')
concatenated_df = concatenated_df.sort_values(by='date', ascending=True)
concatenated_df['date'] = concatenated_df['date'].dt.strftime('%d-%m-%y')

# print the first few rows and shape of the concatenated DataFrame
print("\nHead of the concatenated DataFrame:")
print(concatenated_df.head())
print("\nShape of the concatenated DataFrame:")
print(concatenated_df.shape)

#=======================#========================#========================
# Save the concatenated DataFrame to a CSV file as a precaution
csv_file_path = 'ALL_CLEAN_DATA.csv' # need this for quant analysis
print(f"Saving concatenated DataFrame to '{csv_file_path}'...")

concatenated_df.to_csv(csv_file_path, index=False)

bucket_name = 'crystal-dss'
gcs_prefix = 'cleaned_data/clean_df.csv' # Using the provided prefix

save_dataframe_to_gcs(
    df=concatenated_df,
    bucket_name= bucket_name,
    gcs_prefix= gcs_prefix,
    validate_rows=False
)
print(f"Concatenated data saved to '{csv_file_path}'") and print(f"Concatenated data saved to gs://{bucket_name}/{gcs_prefix}")

"""# Start the Analytical Processing"""

# Load and validate the data
import pandas as pd, numpy as np

# Authenticate to Google Sheets
auth.authenticate_user()
creds, _ = default()
gc = gspread.authorize(creds)

#===============================================================================
# Load the dataset (corrected to load ALL_CLEAN_DATA.csv)
#===============================================================================
# Assuming 'ALL_CLEAN_DATA.csv' was saved successfully in the previous step
prices_df = concatenated_df.copy() #pd.read_csv('ALL_CLEAN_DATA.csv')

# Convert 'date' column to datetime objects
prices_df['date'] = pd.to_datetime(prices_df['date'], format='%d-%m-%y', errors='coerce')
prices_df.set_index('date', inplace=True)
prices_df.index.name = 'Date' # Rename index for consistency

#===============================================================================
# Clean column names: remove 'CFR', 'FCA', 'FOB', 'CIF', 'Refrigerated'
#===============================================================================
prices_df.columns = prices_df.columns.str.replace(' CFR', '', regex=False)
prices_df.columns = prices_df.columns.str.replace(' FCA', '', regex=False)
prices_df.columns = prices_df.columns.str.replace(' FOB', '', regex=False)
prices_df.columns = prices_df.columns.str.replace(' CIF', '', regex=False)
prices_df.columns = prices_df.columns.str.replace(' Refrigerated', '', regex=False)
# Also remove leading/trailing spaces that might result from replacements
prices_df.columns = prices_df.columns.str.strip()

# Ensure data is in ascending order
prices_df.sort_index(ascending=True, inplace=True)
print("Commodity Prices DataFrame Head after sorting by Date (ascending):")
print(prices_df.head(2))

#===============================================================================
# Delete empty columns and rows (as requested by the user)
#===============================================================================
print(f"\nOriginal shape of prices_df: {prices_df.shape}")

# Drop columns that are entirely NaN
prices_df.dropna(axis=1, how='all', inplace=True)
print(f"Shape after dropping entirely empty columns: {prices_df.shape}")

# Drop rows that are entirely NaN
prices_df.dropna(axis=0, how='all', inplace=True)
print(f"Shape after dropping entirely empty rows: {prices_df.shape}")

print("\nPrices DataFrame after cleaning empty columns and rows:")
print(prices_df.head())

"""# STATISTICAL STUDIES
## Correlations, Causality, Seasonality and Regression Parameter Strength

## 1. CORRELATIONS
"""

#===============================================================================
# Weighted Correlation-based
#===============================================================================
import pandas as pd
import numpy as np
import torch

# Use Rolling Averages for smoothing, no Differencing or Scaling required

# WINDOW FOR ROLLING AVERAGES
window = 12 #(0, 5 or 20)

# Exclude the 'date' column before calculating the rolling mean
prices_df = prices_df #defined earlier
prices_df_numeric = prices_df.copy()
average_prices_df = prices_df_numeric.rolling(window=window).mean().dropna()

# Select only the numerical columns from the average DataFrame
numerical_cols_for_correlations_average = average_prices_df.select_dtypes(include=np.number).columns.tolist()

# Create a new DataFrame containing only the selected numerical columns for average causality analysis.
numerical_df_for_correlation_average = average_prices_df[numerical_cols_for_correlations_average]

# Calculate the correlation matrix using only price features
correlation_matrix = numerical_df_for_correlation_average.corr()

# Define the threshold for absolute correlation
weighted_correlation_threshold = 0.5 # You can adjust this value

# Identify which columns are the original price features for indexing # Define price_feature_names here before it's used
price_feature_names = numerical_df_for_correlation_average.columns.tolist()
num_price_features = len(price_feature_names) # Ensure this matches the num_nodes in the model

# Initialize an empty weighted adjacency matrix with the correct shape
num_commodities = num_price_features # Adjacency matrix size should match the number of price features
adjacency_matrix_np_weighted_corr = np.zeros((num_commodities, num_commodities))

print(f"Recalculating correlations for {num_price_features} price features.")

# Iterate through the correlation matrix and set adjacency based on the threshold
# Use the absolute correlation value as the edge weight if it's above the threshold
for i in range(num_price_features):
    for j in range(num_price_features):
        if i != j and abs(correlation_matrix.iloc[i, j]) >= weighted_correlation_threshold:
            # Use the absolute correlation value as the edge weight
            adjacency_matrix_np_weighted_corr[i, j] = abs(correlation_matrix.iloc[i, j])

# Create a DataFrame for the updated adjacency matrix using only price feature names as index/columns
adjacency_matrix_df_weighted_corr = pd.DataFrame(
    adjacency_matrix_np_weighted_corr,
    index=price_feature_names,
    columns=price_feature_names
)

# Convert to PyTorch tensor if needed for the model
adjacency_matrix_tensor_weighted_corr = torch.tensor(adjacency_matrix_df_weighted_corr.values, dtype=torch.float)

print("\nWeighted Correlation-based Adjacency Matrix DataFrame shape:", adjacency_matrix_df_weighted_corr.shape)
print("Weighted Correlation-based Adjacency Matrix Tensor shape:", adjacency_matrix_tensor_weighted_corr.shape)

# Ensure correlation_matrix is available from previous steps
if 'correlation_matrix' in locals() and isinstance(correlation_matrix, pd.DataFrame):
    print("Saving correlation_matrix to GCS...")

    # Define GCS prefix for the correlation matrix
    gcs_prefix_corr_matrix = 'stats_studies_data/correlation_matrix.csv'

    # Use the save_dataframe_to_gcs utility function
    save_dataframe_to_gcs(
        df=correlation_matrix,
        bucket_name=bucket_name, # 'crystal-dss' is already defined
        gcs_prefix=gcs_prefix_corr_matrix,
        validate_rows=False # Correlation matrices might have NaNs (e.g., if a column has no variance), so skip row validation
    )
else:
    print("Error: 'correlation_matrix' DataFrame not found or is not a DataFrame. Cannot save to GCS.")



# Define the target commodities
target_commodities = [
    'Acetic Acid',
    'Butyl Acetate',
    'Toluene',
    'Isomer-MX',
    'Solvent-MX',
    'Methanol',
    'MTBE',
    'Benzene'
]

# Define the other commodities to correlate against
other_commodities = [
    'Gold',
    'Silver',
    'Copper',
    'S&P 500',
    'Shanghai Composite',
    'USD Index',
    'Japanese Yen',
    'US 10-Y BOND PRICE',
    'Crude Oil',
    'Natural Gas',
    'Naphtha',
    'EDC'
    'Ethylene',
    'Propylene',
    'N-Butanol',
    'Paraxylene',
    'OrthXylene',
    'Cyclohexane',
    'Styrene',
    'DEG',
    '2 EH',
    'Acetic Acid',
    'Butyl Acetate',
    'Toluene',
    'Isomer-MX',
    'Solvent-MX',
    'Methanol',
    'MTBE',
    'Benzene'
]


# Ensure correlation_matrix is available from previous steps
if 'correlation_matrix' not in locals():
    print("Error: 'correlation_matrix' DataFrame not found. Please run the correlation analysis section first.")
else:
    print("Filtering correlation matrix...")

    # Filter the correlation matrix to include only specified target and other commodities
    # Ensure all specified commodities exist in the correlation matrix
    valid_target_commodities = [col for col in target_commodities if col in correlation_matrix.index]
    valid_other_commodities = [col for col in other_commodities if col in correlation_matrix.columns]

    if not valid_target_commodities or not valid_other_commodities:
        print("Warning: No valid target or other commodities found in the correlation matrix.")
        filtered_correlation_df = pd.DataFrame() # Create an empty DataFrame
    else:
        # Select rows for target commodities and columns for other commodities
        filtered_correlation_df = correlation_matrix.loc[valid_target_commodities, valid_other_commodities]

        # Remove self-correlations (where target_commodity is also in other_commodities and matches)
        for target_col in valid_target_commodities:
            if target_col in filtered_correlation_df.columns:
                filtered_correlation_df.loc[target_col, target_col] = 0.0 # Set self-correlation to 0 or NaN

        print("Filtered Correlation Table:")
        print(filtered_correlation_df(2))

# Ensure correlation_matrix is available from previous steps
if 'filtered_correlation_df' in locals() and isinstance(correlation_matrix, pd.DataFrame):
    print("Saving filtered_correlation_matrix to GCS...")

    # Define GCS prefix for the correlation matrix
    gcs_prefix_corr_matrix = 'stats_studies_data/filtered_correlation_matrix.csv'

    # Use the save_dataframe_to_gcs utility function
    save_dataframe_to_gcs(
        df=correlation_matrix,
        bucket_name=bucket_name, # 'crystal-dss' is already defined
        gcs_prefix=gcs_prefix_corr_matrix,
        validate_rows=False # Correlation matrices might have NaNs (e.g., if a column has no variance), so skip row validation
    )
else:
    print("Error: 'filtered_correlation_matrix' DataFrame not found or is not a DataFrame. Cannot save to GCS.")

"""## 2. CAUSALITY

"""

#===============================================================================
# 1. Load the dataset and convert to weekly / monthly rolling averages if required
#===============================================================================
# WINDOW FOR ROLLING AVERAGES
window = 12 #(0, 5 or 20)

# Exclude the 'date' column before calculating the rolling mean
prices_df = prices_df.copy() #defined earlier
prices_df_numeric = prices_df.copy()
average_prices_df = prices_df_numeric.rolling(window=window).mean().dropna()

# Select only the numerical columns from the average DataFrame
numerical_cols_average = average_prices_df.select_dtypes(include=np.number).columns.tolist()

# Create a new DataFrame containing only the selected numerical columns for average causality analysis.
numerical_df_for_causality_average = average_prices_df[numerical_cols_average]

# Difference the selected numerical DataFrame and drop any resulting NaN values.
numerical_df_diff_average = numerical_df_for_causality_average.diff().dropna()


# Scale the differenced average data using StandardScaler.
scaler_average = StandardScaler()
numerical_df_diff_scaled_average = pd.DataFrame(scaler_average.fit_transform(numerical_df_diff_average),
                                                columns=numerical_df_diff_average.columns,
                                                index=numerical_df_diff_average.index)

#===============================================================================
# Perform Granger causality tests and identify significant relationships
#===============================================================================
# Define the significance level for the Granger causality tests
alpha = 0.04

# Define the maximum number of lags to consider for the Granger causality tests
max_lag = 10

print(f"Alpha (significance level) set to: {alpha}")
print(f"Max lag for Granger causality tests set to: {max_lag}")
print("\nPerforming Granger causality tests...")

# Store ALL causal relationships and their p-values for all lags
all_granger_test_results = []

# Use the columns from the scaled, differenced DataFrame as they represent the commodities
commodities_for_test = numerical_df_diff_scaled_average.columns
num_commodities_for_test = len(commodities_for_test)

print(f"Performing Granger causality tests for {num_commodities_for_test} commodities up to max_lag of {max_lag}.")

# Re-initialize adjacency matrix for causality based on the number of commodities being tested
# This matrix will indicate if causality exists (1) or not (0)
causality_adjacency_matrix_np = np.zeros((num_commodities_for_test, num_commodities_for_test))

# Perform Granger causality tests between all pairs of commodities Capture and discard output to keep the notebook clean
with open('/dev/null', 'w') as devnull:
    import sys
    old_stdout = sys.stdout
    sys.stdout = devnull

    for i in range(num_commodities_for_test):
        for j in range(num_commodities_for_test):
            if i != j:
                source_commodity = commodities_for_test[i]
                target_commodity = commodities_for_test[j]

                # Prepare the data for the Granger causality test using differenced and scaled data
                data_pair = numerical_df_diff_scaled_average[[source_commodity, target_commodity]]

                try:
                    test_results = grangercausalitytests(data_pair, maxlag=max_lag, verbose=False)

                    # Iterate through all lags and store their p-values
                    for lag in range(1, max_lag + 1):
                        # Using the f-test p-value for each lag
                        if lag in test_results: # Ensure test_results contains data for this lag
                            p_value_ftest = test_results[lag][0]['ssr_ftest'][1]
                            all_granger_test_results.append({
                                'Source': source_commodity,
                                'Target': target_commodity,
                                'Lag': lag,
                                'P-value (F-test)': p_value_ftest
                            })
                            # Check for significance to build the adjacency matrix (for consistency with previous steps)
                            if p_value_ftest < alpha:
                                causality_adjacency_matrix_np[i, j] = 1 # Set adjacency to 1 if causality is found at any lag for any lag

                except Exception as e:
                     # Handle cases where test fails (e.g., insufficient data, non-varying data)
                     # print(f"Could not perform Granger causality test for '{source_commodity}' and '{target_commodity}': {e}")
                     pass # Suppress individual errors for brevity

    sys.stdout = old_stdout # Restore stdout

# Convert the list of all Granger test results into a DataFrame
all_granger_test_results_df = pd.DataFrame(all_granger_test_results)

print("\nHead of DataFrame with ALL Granger Test Results (P-values for all lags):")
print(all_granger_test_results_df.head())

# Create a DataFrame for the causality adjacency matrix based on *any* significant lag
causality_adjacency_matrix_df = pd.DataFrame(causality_adjacency_matrix_np,
                                             index=commodities_for_test,
                                             columns=commodities_for_test)

print("\nCausality Adjacency Matrix (1 if causality found at any lag, 0 otherwise):")

# Save the comprehensive Granger test results to GCS
if 'all_granger_test_results_df' in locals() and isinstance(all_granger_test_results_df, pd.DataFrame):
    print("\nSaving all_granger_test_results_df to GCS...")

    # Define GCS prefix for the comprehensive Granger results
    gcs_prefix_granger_results = 'stats_studies_data/all_granger_test_results.csv'

    # Use the save_dataframe_to_gcs utility function
    save_dataframe_to_gcs(
        df=all_granger_test_results_df,
        bucket_name=bucket_name, # 'crystal-dss' is already defined
        gcs_prefix=gcs_prefix_granger_results,
        validate_rows=False # P-values might be NaN if test failed, so skip validation
    )
else:
    print("Error: 'all_granger_test_results_df' DataFrame not found or is not a DataFrame. Cannot save to GCS.")

import pandas as pd
import gspread
from google.colab import auth
from google.auth import default

# Define the target commodities
target_commodities = [
    'Acetic Acid',
    'Butyl Acetate',
    'Toluene',
    'Isomer-MX',
    'Solvent-MX',
    'Methanol',
    'MTBE',
    'Benzene'
]

# Define the other commodities to correlate against
other_commodities = [
    'Gold',
    'Silver',
    'Copper',
    'S&P 500',
    'Shanghai Composite',
    'USD Index',
    'Japanese Yen',
    'US 10-Y BOND PRICE',
    'Crude Oil',
    'Natural Gas',
    'Naphtha',
    'EDC',
    'Ethylene',
    'Propylene',
    'N-Butanol',
    'Paraxylene',
    'OrthXylene',
    'Cyclohexane',
    'Styrene',
    'DEG',
    '2 EH',
    'Acetic Acid',
    'Butyl Acetate',
    'Toluene',
    'Isomer-MX',
    'Solvent-MX',
    'Methanol',
    'MTBE',
    'Benzene'
]


# Ensure all_granger_test_results_df is available from the previous steps
if 'all_granger_test_results_df' not in locals() or all_granger_test_results_df.empty:
    print("Error: 'all_granger_test_results_df' DataFrame not found or is empty. Cannot filter causality results.")
    filtered_causality_df = pd.DataFrame() # Initialize as empty
else:
    print("Filtering causality relationships to find sources for target commodities...")

    # Group by Source and Target to find the minimum P-value for each pair
    most_significant_lags = all_granger_test_results_df.loc[
        all_granger_test_results_df.groupby(['Source', 'Target'])['P-value (F-test)'].idxmin()
    ].copy()

    # Filter for significance (e.g., alpha = 0.04)
    # Assuming 'alpha' is still defined from the previous cell
    if 'alpha' in locals():
        most_significant_lags = most_significant_lags[most_significant_lags['P-value (F-test)'] < alpha]
    else:
        print("Warning: 'alpha' not defined. Filtering with default alpha=0.05.")
        most_significant_lags = most_significant_lags[most_significant_lags['P-value (F-test)'] < 0.05]


    # Filter the DataFrame: Source must be in other_commodities AND Target must be in target_commodities
    filtered_causality_df = most_significant_lags[
        most_significant_lags['Source'].isin(other_commodities) &
        most_significant_lags['Target'].isin(target_commodities)
    ].copy()

    if not filtered_causality_df.empty:
        # Sort by Target and then by 'P-value (F-test)' for clarity
        filtered_causality_df = filtered_causality_df.sort_values(by=['Target', 'P-value (F-test)']).reset_index(drop=True)

        print("\nFiltered Causality Table (Sources for Target Commodities):")
        print(filtered_causality_df)

        # Define GCS prefix for the correlation matrix
        gcs_prefix_casuality_matrix = 'stats_studies_data/filtered_causality_matrix.csv'

        # Use the save_dataframe_to_gcs utility function
        save_dataframe_to_gcs(
            df=filtered_causality_df,
            bucket_name=bucket_name, # 'crystal-dss' is already defined
            gcs_prefix=gcs_prefix_casuality_matrix,
            validate_rows=False # Correlation matrices might have NaNs (e.g., if a column has no variance), so skip row validation
        )

    else:
        print("No significant filtered causal relationships found for target commodities.")

"""## 3. REGRESSION

Trying to identify variables with the largest explanatory power, not necessarily looking at the fit
"""

# The prices_df DataFrame is already loaded and cleaned in previous steps.

prices_df = prices_df.copy()

# Fill any remaining NaNs in prices_df before selecting numerical features and differencing
# This ensures that differencing does not result in an empty DataFrame due to scattered NaNs
prices_df_filled = prices_df.fillna(method='ffill').fillna(method='bfill')

numerical_price_features_df = prices_df_filled.select_dtypes(include=np.number)
print (numerical_price_features_df.shape)

# apply differencing / scaling is done later as part of the model
numerical_price_features_diff_df = numerical_price_features_df.diff()
print (numerical_price_features_diff_df.shape)

# --- New code: Filter out commodities with less than 300 values ---
min_values_threshold = 300
print(f"Filtering commodities to ensure at least {min_values_threshold} non-missing values...")

# Calculate non-missing values for each column
non_missing_counts = numerical_price_features_diff_df.count()
print(f"Total number of commodities: {len(non_missing_counts)}")


# Identify columns to keep
columns_to_keep = non_missing_counts[non_missing_counts >= min_values_threshold].index.tolist()

# Filter the DataFrame
numerical_price_features_diff_df = numerical_price_features_diff_df[columns_to_keep].copy()
print(f"Remaining commodities after filtering: {len(columns_to_keep)}")

# Get the list of commodity names to use as targets (and features)
commodities = numerical_price_features_diff_df.columns.tolist()

# Define the target commodities
target_commodities = [
    'Acetic Acid',
    'Butyl Acetate',
    'Toluene',
    'Isomer-MX',
    'Solvent-MX',
    'Methanol',
    'MTBE',
    'Benzene'
]

# Filter target_commodities to only include those present in the actual DataFrame columns
target_commodities_present = [c for c in target_commodities if c in numerical_price_features_diff_df.columns]
target_commodities = target_commodities_present

print (len(target_commodities_present))

#===============================================================================
# helper functions
#===============================================================================
# Function to calculate Adjusted R-squared
def adjusted_r2_score(r2, n, k):
    """
    Calculates the adjusted R-squared.

    Args:
        r2 (float): The R-squared value.
        n (int): The number of observations.
        k (int): The number of independent variables (features).

    Returns:
        float: The adjusted R-squared value.
    """
    if n - k - 1 == 0:
        return -np.inf # Avoid division by zero, return negative infinity
    return 1 - (1 - r2) * (n - 1) / (n - k - 1)

# Function to calculate RMSE
def calculate_rmse(y_true, y_pred):
    """
    Calculates the Root Mean Squared Error.

    Args:
        y_true (array): Actual values.
        y_pred (array): Predicted values.

    Returns:
        float: The RMSE value.
    """
    return np.sqrt(mean_squared_error(y_true, y_pred))


#===============================================================================
# Build the Regression model and run it for each commodity
#===============================================================================

# Initialize a list to store regression results for each target commodity
regression_results = []

# Define the polynomial degrees to test
degrees_to_test = [1, 2, 3] # Modified to include degree 3
print(f"Degrees to test for polynomial regression: {degrees_to_test}")

# Define how many top coefficients to save
top_n = 8 # Set top_n to 8 as requested

# Define maximum number of features for polynomial degrees > 1
max_features_for_poly = 8 # Example: limit to top 10 correlated features

# Start loop through each commodity to set it as the target variable
for target_commodity in target_commodities: # Modified to iterate through target_commodities
    print(f"\nProcessing target commodity: {target_commodity}")
    # Define all potential features: all other commodities except the target
    all_potential_features = [col for col in commodities if col != target_commodity]

    # Filter to only include features present in the differenced DataFrame
    all_potential_features = [f for f in all_potential_features if f in numerical_price_features_diff_df.columns]

    if not all_potential_features:
        print(f"  Skipping {target_commodity}: No valid features remaining after filtering.")
        continue

    # Define the data for the current iteration (target + all potential features)
    data_full = numerical_price_features_diff_df[all_potential_features + [target_commodity]].dropna()

    if data_full.empty:
        print(f"  Skipping {target_commodity}: Data is empty after selecting features and dropping NaNs.")
        continue

    # Initialize list to store results for the current target commodity across different degrees
    results_for_target = []

    # Iterate through each degree to test
    for degree in degrees_to_test:
        current_features = []
        if degree == 1:
            current_features = all_potential_features
        else:
            # Dynamically select top correlated features for degree > 1
            # Calculate correlation of all_potential_features with the target
            correlations = data_full[all_potential_features].corrwith(data_full[target_commodity]).abs()
            # Select top N features based on absolute correlation
            top_correlated_features = correlations.nlargest(max_features_for_poly).index.tolist()
            current_features = top_correlated_features
            print(f"    Degree {degree}: Selected top {len(current_features)} features for polynomial transformation: {current_features}")

        if not current_features:
            print(f"    Degree {degree}: No features selected for this degree. Skipping.")
            continue

        # Define X (features) and y (target) using the current set of features
        X = data_full[current_features]
        y = data_full[target_commodity]

        # Split data into training and testing sets (chronological split)
        if len(X) < 2: # Need at least 2 samples for train/test split
            print(f"  Skipping {target_commodity}: Insufficient data points ({len(X)}) for train/test split.")
            continue

        test_size_val = min(0.2, (len(X) - 1) / len(X)) # Ensure test set has at least 1 sample
        if test_size_val == 0: # If only 1 sample, test_size becomes 0, handle this case
            print(f"  Skipping {target_commodity}: Only one data point available after filtering, cannot perform train/test split.")
            continue

        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size_val, shuffle=False)

        print(f"  Degree {degree} - Shape of X_train: {X_train.shape}, X_test: {X_test.shape}")

        # Feature scaling
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)

        # Create polynomial features
        poly = PolynomialFeatures(degree=degree)
        X_poly_train = poly.fit_transform(X_train_scaled)
        X_poly_test = poly.transform(X_test_scaled)

        # Calculate number of features after polynomial transformation (excluding intercept)
        k_train = X_poly_train.shape[1] - 1
        k_test = X_poly_test.shape[1] - 1

        # Check if enough samples for regression after polynomial transformation
        # Number of samples must be >= number of features + 1
        if X_poly_train.shape[0] < k_train + 1:
            print(f"    Degree {degree}: Insufficient training samples ({X_poly_train.shape[0]}) for {k_train + 1} features after polynomial expansion. Skipping.")
            # If this happens, it means max_features_for_poly is still too high for this degree/data combination
            continue

        # Create and train the polynomial regression model (using Ridge)
        model = Ridge(alpha=1.0).fit(X_poly_train, y_train)

        # Evaluate on the training set
        y_pred_train = model.predict(X_poly_train)
        r2_train = r2_score(y_train, y_pred_train)
        adj_r2_train = adjusted_r2_score(r2_train, X_train.shape[0], k_train)
        rmse_train = calculate_rmse(y_train, y_pred_train)

        # Evaluate on the test set
        y_pred_test = model.predict(X_poly_test)
        r2_test = r2_score(y_test, y_pred_test)
        adj_r2_test = adjusted_r2_score(r2_test, X_test.shape[0], k_test)
        rmse_test = calculate_rmse(y_test, y_pred_test)

        print(f"    Degree {degree}: Train Adj R² = {adj_r2_train:.3f}, Test Adj R² = {adj_r2_test:.3f}, Train RMSE = {rmse_train:.3f}, Test RMSE = {rmse_test:.3f}")

        # Get coefficients for the current model
        coef = pd.Series(model.coef_, index=poly.get_feature_names_out(input_features=current_features))

        # Exclude the intercept term '1' when considering top coefficients
        coef_for_ranking = coef[coef.index != '1']

        # Sort coefficients by absolute value and select the top N
        top_coef_sorted = coef_for_ranking.abs().sort_values(ascending=False).head(top_n)

        # Get the actual coefficients for the top features
        top_coefficients_with_values = coef[top_coef_sorted.index].to_dict()

        # Store results for the current degree
        result_entry = {
            'Target Commodity': target_commodity,
            'Degree': degree,
            'Features Used Count': len(current_features),
            'Train Adj R2': adj_r2_train,
            'Test Adj R2': adj_r2_test,
            'Train RMSE': rmse_train,
            'Test RMSE': rmse_test,
            'Intercept': model.intercept_,
        }

        # Add the top N coefficients as separate entries
        for i, (feature_name, coefficient_value) in enumerate(top_coefficients_with_values.items()):
             result_entry[f'Top {i+1} Feature'] = feature_name
             result_entry[f'Top {i+1} Value'] = coefficient_value

        results_for_target.append(result_entry)

    # After testing all degrees for the target commodity, append the list of results
    regression_results.extend(results_for_target)


# After the loop, convert regression_results into a DataFrame
regression_results_df = pd.DataFrame(regression_results)

print("\nRegression Results Summary:")
# printing only relevant columns for summary view
print(regression_results_df[['Target Commodity', 'Degree', 'Features Used Count', 'Train Adj R2', 'Test Adj R2', 'Train RMSE', 'Test RMSE']].head())

#===============================================================================
# Save the output of the study to GCS
#===============================================================================
if not regression_results_df.empty:

    # Define GCS prefix for the correlation matrix
    gcs_prefix_regressions_results = 'stats_studies_data/regression_results_matrix.csv'

    # Use the save_dataframe_to_gcs utility function
    save_dataframe_to_gcs(
        df=regression_results_df,
        bucket_name=bucket_name, # 'crystal-dss' is already defined
        gcs_prefix=gcs_prefix_regressions_results,
        validate_rows=False
    )

else:
    print("No regression results found to save.")

"""## 4. SEASONALITY"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.stattools import acf, pacf
from scipy.fft import fft, fftfreq
from scipy.signal import find_peaks # Import find_peaks for a more robust peak detection

# Assuming prices_df is already loaded and preprocessed with 'Date' as index

# Ensure prices_df is available and has a DatetimeIndex
if 'prices_df' not in locals() or prices_df.empty:
    print("Error: 'prices_df' DataFrame not found or is empty. Cannot perform seasonality analysis.")
    seasonality_analysis_results = {} # Initialize empty results dictionary
else:
    if not isinstance(prices_df.index, pd.DatetimeIndex) or prices_df.index.name != 'Date':
        print("Error: 'prices_df' index is not a DatetimeIndex named 'Date'. Cannot perform seasonality analysis.")
        seasonality_analysis_results = {} # Initialize empty results dictionary
    else:
        print("Starting seasonality analysis for all commodities...")

        # Identify commodity columns (excluding non-numeric and cyclical features)
        all_columns = prices_df.columns.tolist()
        cols_to_exclude = ['date', 'day_of_week', 'month', 'year',
                           'day_of_week_sin', 'day_of_week_cos',
                           'month_sin', 'month_cos',
                           'year_sin', 'year_cos']
        commodity_columns = [col for col in all_columns if col not in cols_to_exclude and pd.api.types.is_numeric_dtype(prices_df[col])]


        if not commodity_columns:
            print("No numeric commodity columns found for seasonality analysis.")
            seasonality_analysis_results = {} # Initialize empty results dictionary
        else:
            # Dictionary to store analysis results for each commodity
            seasonality_analysis_results = {}

            # Define the number of top dominant periods to analyze based on FFT
            num_top_periods_to_analyze = 6 # Analyze the top 6 dominant periods from FFT

            # Iterate through each commodity column
            for commodity in commodity_columns:
                print(f"\nStarting analysis for: {commodity}")

                # Get the time series for the current commodity and drop missing values
                commodity_series = prices_df[commodity].dropna()

                 # Store results for the current commodity
                commodity_results = {
                    'Commodity': commodity,
                    'Overall Seasonality Summary': '', # Will be populated later
                    'Analysis Status': 'Success', # Assume success unless an error occurs
                    'Error Message': None,
                    'FFT Peaks': [],
                    'Analyzed Periods': []
                }

                # --- 1. FFT Analysis ---
                try:
                    # Perform FFT
                    yf = fft(commodity_series.values)
                    xf = fftfreq(len(commodity_series), d=1) # Assuming daily frequency (d=1)

                    # Calculate power spectrum
                    power_spectrum = np.abs(yf)**2

                    # Find dominant frequencies and their power (excluding the zero frequency component)
                    positive_frequencies = xf > 0
                    frequencies = xf[positive_frequencies]
                    powers = power_spectrum[positive_frequencies]

                    # Find peaks in the power spectrum to identify dominant frequencies more robustly
                    # Set a minimum peak height or prominence based on the overall power distribution
                    # A simple approach: find peaks above a certain percentage of the maximum power
                    # Or use find_peaks with prominence
                    peak_indices, _ = find_peaks(powers, prominence=np.mean(powers) * 0.1) # Example prominence threshold

                    if peak_indices.size > 0:
                         peak_frequencies = frequencies[peak_indices]
                         peak_powers = powers[peak_indices]

                         # Combine and sort by power in descending order
                         combined_peaks = sorted(zip(peak_frequencies, peak_powers), key=lambda x: x[1], reverse=True)

                         # Store dominant FFT peaks (frequency, period, power)
                         num_top_peaks_to_store = 30 # Store top 30 peaks as example
                         for i in range(min(num_top_peaks_to_store, len(combined_peaks))):
                             freq, power = combined_peaks[i]
                             # Handle potential division by zero for period if frequency is zero (though xf > 0 filters this)
                             period = 1 / freq if freq != 0 else np.inf
                             commodity_results['FFT Peaks'].append({
                                 'Frequency': freq,
                                 'Period': period,
                                 'Power': power
                             })

                         # Calculate overall dominant FFT power (e.g., sum of top few powers)
                         overall_fft_power = np.sum([peak['Power'] for peak in commodity_results['FFT Peaks'][:min(5, len(commodity_results['FFT Peaks']))]]) # Sum of top 5 powers
                         commodity_results['Overall FFT Power'] = overall_fft_power

                         print("  FFT analysis completed and dominant peaks identified.")
                         # Print dominant FFT frequencies, periods, and their power (sorted by power)
                         print("  Dominant FFT Frequencies, Periods, and their Power for {} (sorted by power):".format(commodity))
                         for i in range(min(10, len(commodity_results['FFT Peaks']))): # Print top 10 for brevity
                             peak = commodity_results['FFT Peaks'][i]
                             period_str = f"{peak['Period']:.2f}" if np.isfinite(peak['Period']) else "inf"
                             print("    Frequency: {:.6f}, Period: {}, Power: {:.2f}".format(peak['Frequency'], period_str, peak['Power']))

                    else:
                         print(f"  No significant FFT peaks found for {commodity}.")
                         commodity_results['FFT Peaks'] = []
                         commodity_results['Overall FFT Power'] = np.nan


                except Exception as e:
                    print(f"  Error during FFT analysis for {commodity}: {e}")
                    commodity_results['Analysis Status'] = 'Partial Success (FFT Failed)'
                    commodity_results['Error Message'] = f"FFT analysis failed: {e}"
                    commodity_results['FFT Peaks'] = [] # Ensure FFT Peaks is an empty list on failure
                    commodity_results['Overall FFT Power'] = np.nan # Ensure overall power is NaN on failure


                # --- Select Top Dominant Periods from FFT for Period-Specific Analysis ---
                periods_to_analyze_for_commodity = []
                if commodity_results['FFT Peaks']:
                    # Get the periods from the top dominant FFT peaks
                    top_peaks_periods = [peak['Period'] for peak in commodity_results['FFT Peaks'][:num_top_periods_to_analyze]]
                    # Filter out non-finite periods (like infinity) and duplicates
                    periods_to_analyze_for_commodity = sorted(__builtins__.list(set([int(p) for p in top_peaks_periods if np.isfinite(p) and p > 1]))) # Use integer periods > 1


                if not periods_to_analyze_for_commodity:
                    print(f"  No valid dominant periods found from FFT to analyze for {commodity}. Skipping period-specific analysis.")
                    # Store the result with empty Analyzed Periods list if no periods were found
                    seasonality_analysis_results[commodity] = commodity_results
                    continue # Move to the next commodity


                # --- 2. Period-Specific Analysis (STL, ACF, PACF, FFT Power at Period, STL Ratio) for Selected Periods ---
                print(f"  Analyzing selected dominant periods from FFT: {periods_to_analyze_for_commodity}")
                for period in periods_to_analyze_for_commodity:
                    period_analysis_result = {
                        'Period': period,
                        'Period Analysis Status': 'Success',
                        'Period Error Message': None,
                        'Seasonality Strength (STL)': np.nan,
                        'Calculated Seasonality Score (STL Ratio)': np.nan,
                        'ACF at Period Lag': np.nan,
                        'PACF at Period Lag': np.nan,
                        'FFT Power at Period': np.nan,
                        'Is Seasonal for Period': False, # Default to False
                        'Is ACF Significant at Period': False, # New field for significance
                        'Is PACF Significant at Period': False # New field for significance
                    }

                    # Check if series is long enough for this period's STL decomposition
                    # Need at least period + 2 observations for STL
                    min_length_for_stl = period + 2
                    if len(commodity_series) < min_length_for_stl:
                        print(f"    Skipping period {period}: Insufficient data ({len(commodity_series)} data points) for STL. Minimum required: {min_length_for_stl}")
                        period_analysis_result['Period Analysis Status'] = 'Skipped (Insufficient Data for STL)'
                        period_analysis_result['Period Error Message'] = f'Insufficient data for STL decomposition (min length: {min_length_for_stl}).'
                        commodity_results['Analyzed Periods'].append(period_analysis_result)
                        continue # Skip to the next period


                    # a. STL Decomposition and Seasonality Strength
                    try:
                        # Use a robust STL decomposition if statsmodels version supports it
                        # Check if 'robust' is a valid argument for seasonal_decompose
                        try:
                            # Try with robust=True first
                            stl = seasonal_decompose(commodity_series, model='additive', period=period, robust=True)
                            print(f"    STL decomposition (robust) completed for period {period}.")
                        except TypeError:
                            # Fallback to without robust if not supported
                            stl = seasonal_decompose(commodity_series, model='additive', period=period)
                            print(f"    STL decomposition completed for period {period} (robust=False).")


                        seasonal_component = stl.seasonal
                        residual_component = stl.resid
                        # trend_component = stl.trend # Not directly used for strength calculation


                        # Calculate seasonality strength (Seasonality / (Seasonality + Residual))
                        # Handle cases where Seasonality + Residual is zero or close to zero
                        seasonality_plus_residual_std = np.nanstd(seasonal_component + residual_component)
                        if seasonality_plus_residual_std > 1e-9: # Avoid division by zero or very small numbers
                             seasonality_strength = np.nanstd(seasonal_component) / seasonality_plus_residual_std
                        else:
                             seasonality_strength = 0.0 # Treat as no seasonality if variation is minimal

                        period_analysis_result['Seasonality Strength (STL)'] = seasonality_strength
                        print(f"    Seasonality Strength (STL) calculated for period {period}: {seasonality_strength:.4f}")

                        # Calculate Seasonality Score (STL Ratio)
                        # STL Ratio = std(seasonal) / std(residual)
                        residual_std = np.nanstd(residual_component)
                        if residual_std > 1e-9: # Avoid division by zero or very small numbers
                             stl_ratio = np.nanstd(seasonal_component) / residual_std
                        else:
                             stl_ratio = np.inf # Indicate strong seasonality if residual is minimal

                        period_analysis_result['Calculated Seasonality Score (STL Ratio)'] = stl_ratio
                        print(f"    Calculated Seasonality Score (STL Ratio) for period {period}: {stl_ratio:.4f}")


                        # Check for seasonality based on STL strength (threshold example)
                        if seasonality_strength > 0.1: # Example threshold
                             period_analysis_result['Is Seasonal for Period'] = True


                    except Exception as e:
                        print(f"    Error during STL decomposition for period {period}: {e}")
                        period_analysis_result['Period Analysis Status'] = 'Failed (STL)'
                        period_analysis_result['Period Error Message'] = f'STL decomposition failed: {e}'


                    # b. ACF and PACF at the specified period lag
                    try:
                        # Calculate ACF and PACF up to the specified period lag
                        n_lags_acf_pacf = period

                        # Ensure enough data points for the lags + 1 (for lag 0)
                        if len(commodity_series) > n_lags_acf_pacf:
                             # Calculate ACF and its confidence intervals
                             acf_values, conf_int_acf = acf(commodity_series.dropna(), nlags=n_lags_acf_pacf, alpha=0.05)
                             # Calculate PACF and its confidence intervals
                             pacf_values, conf_int_pacf = pacf(commodity_series.dropna(), nlags=n_lags_acf_pacf, alpha=0.05)

                             # Find the lag index corresponding to the specified period
                             period_lag_index = period # Use the period itself as the lag index

                             # Store ACF and PACF values at the period lag
                             period_analysis_result['ACF at Period Lag'] = acf_values[period_lag_index]
                             period_analysis_result['PACF at Period Lag'] = pacf_values[period_lag_index]

                             print(f"    ACF and PACF calculated at lag {period}.")

                             # Check for significance based on confidence intervals (if available) or a simple threshold
                             if conf_int_acf is not None and period_lag_index < len(conf_int_acf):
                                 lower_bound_acf, upper_bound_acf = conf_int_acf[period_lag_index]
                                 if period_analysis_result['ACF at Period Lag'] is not np.nan and (period_analysis_result['ACF at Period Lag'] < lower_bound_acf or period_analysis_result['ACF at Period Lag'] > upper_bound_acf):
                                     period_analysis_result['Is ACF Significant at Period'] = True

                             if conf_int_pacf is not None and period_lag_index < len(conf_int_pacf):
                                 lower_bound_pacf, upper_bound_pacf = conf_int_pacf[period_lag_index]
                                 if period_analysis_result['PACF at Period Lag'] is not np.nan and (period_analysis_result['PACF at Period Lag'] < lower_bound_pacf or period_analysis_result['PACF at Period Lag'] > upper_bound_pacf):
                                     period_analysis_result['Is PACF Significant at Period'] = True

                             if period_analysis_result['Is ACF Significant at Period'] or period_analysis_result['Is PACF Significant at Period']:
                                  print(f"    Evidence of significant ACF/PACF detected at period {period}.")


                        else:
                             print(f"    Skipping ACF/PACF for period {period}: Insufficient data ({len(commodity_series)} data points) for {n_lags_acf_pacf} lags.")
                             period_analysis_result['Period Analysis Status'] = 'Skipped (Insufficient Data for ACF/PACF)'
                             period_analysis_result['Period Error Message'] = f'Insufficient data for ACF/PACF calculation (min length: {n_lags_acf_pacf}).'


                    except Exception as e:
                        print(f"    Error during ACF/PACF calculation for period {period}: {e}")
                        period_analysis_result['Period Analysis Status'] = 'Failed (ACF/PACF)'
                        period_analysis_result['Period Error Message'] = f'ACF/PACF calculation failed: {e}'


                    # c. FFT Power at the specified period (or closest frequency)
                    try:
                         # Find the FFT peak closest to the target period from the pre-calculated peaks
                         if commodity_results['FFT Peaks']:
                              fft_periods = np.array([peak['Period'] for peak in commodity_results['FFT Peaks']])
                              # Find the index of the closest period
                              closest_peak_index = np.argmin(np.abs(fft_periods - period))
                              closest_peak = commodity_results['FFT Peaks'][closest_peak_index]

                              # Store the power of the closest peak
                              period_analysis_result['FFT Power at Period'] = closest_peak['Power']
                              print(f"    FFT power found for closest period {closest_peak['Period']:.2f} (target {period}): {closest_peak['Power']:.2f}")

                         else:
                             print(f"    No FFT peaks available to find power at period {period}.")


                    except Exception as e:
                        print(f"    Error finding FFT power at period {period}: {e}")
                        period_analysis_result['Period Analysis Status'] = 'Partial Success (FFT Power Failed)'
                        period_analysis_result['Period Error Message'] = f"FFT power at period calculation failed: {e}"


                    # Append the results for this period to the commodity's analyzed periods list
                    commodity_results['Analyzed Periods'].append(period_analysis_result)

                # --- Overall Seasonality Summary ---
                # Assess overall seasonality based on the period analysis results
                seasonal_periods_found = [p['Period'] for p in commodity_results['Analyzed Periods'] if p['Is Seasonal for Period']]
                significant_acf_periods = [p['Period'] for p in commodity_results['Analyzed Periods'] if p['Is ACF Significant at Period']]
                significant_pacf_periods = [p['Period'] for p in commodity_results['Analyzed Periods'] if p['Is PACF Significant at Period']]

                summary_parts = []
                if seasonal_periods_found:
                    summary_parts.append(f"Evidence of seasonality found at analyzed periods (STL Strength > 0.1): {seasonal_periods_found}")
                if significant_acf_periods:
                     summary_parts.append(f"Significant ACF at analyzed periods: {significant_acf_periods}")
                if significant_pacf_periods:
                     summary_parts.append(f"Significant PACF at analyzed periods: {significant_pacf_periods}")

                if summary_parts:
                    commodity_results['Overall Seasonality Summary'] = ". ".join(summary_parts)
                else:
                    commodity_results['Overall Seasonality Summary'] = "No significant seasonality detected at the analyzed periods based on combined checks."


                # Add status and error message to the overall summary if not fully successful
                if commodity_results['Analysis Status'] != 'Success':
                     commodity_results['Overall Seasonality Summary'] = f"{commodity_results['Analysis Status']}. {commodity_results['Overall Seasonality Summary']}"
                elif any(p['Period Analysis Status'] != 'Success' for p in commodity_results['Analyzed Periods']):
                     failed_periods = [p['Period'] for p in commodity_results['Analyzed Periods'] if p['Period Analysis Status'] != 'Success']
                     commodity_results['Overall Seasonality Summary'] = f"Partial Success (Errors in periods {failed_periods}). {commodity_results['Overall Seasonality Summary']}"
                     commodity_results['Analysis Status'] = 'Partial Success (Period Analysis Errors)'


                # Store the full results for the commodity
                seasonality_analysis_results[commodity] = commodity_results
                print(f"  Finished analysis for: {commodity}")

            print("\nSeasonality analysis complete for all specified commodities.")

            # Flatten the results dictionary into a DataFrame for easier saving/access
            print("\nFlattening Seasonality Analysis Results into DataFrame...")
            seasonality_results_list_flattened = []
            for commodity, result in seasonality_analysis_results.items():
                 base = {
                     'Commodity': result.get('Commodity'),
                     'Overall Seasonality Summary': result.get('Overall Seasonality Summary'),
                     'Analysis Status': result.get('Analysis Status'),
                     'Error Message': result.get('Error Message'),
                     'Overall FFT Power': result.get('Overall FFT Power'),
                     'FFT Peaks': result.get('FFT Peaks', []) # Keep as list
                 }
                 if result.get('Analyzed Periods'):
                      for period_result in result['Analyzed Periods']:
                           flattened_row = base.copy()
                           flattened_row.update(period_result)
                           seasonality_results_list_flattened.append(flattened_row)
                 else:
                      # Add a row even if no periods were analyzed (e.g., skipped commodity)
                      seasonality_results_list_flattened.append(base)


            seasonality_results_df_flattened = pd.DataFrame(seasonality_results_list_flattened)

            print("Flattened Seasonality Analysis Results DataFrame Head:")
            print(seasonality_results_df_flattened.head())



# Save the flattened seasonality results to GCS
if 'seasonality_results_df_flattened' in locals() and not seasonality_results_df_flattened.empty:
    print("Saving seasonality_results_df_flattened to GCS...")

    gcs_prefix_seasonality = 'stats_studies_data/seasonality_results.csv'

    save_dataframe_to_gcs(
        df=seasonality_results_df_flattened,
        bucket_name='crystal-dss',
        gcs_prefix=gcs_prefix_seasonality,
        validate_rows=False # Seasonality results might have NaNs (e.g., if a period could not be analyzed), so skip row validation
    )
else:
    print("Error: 'seasonality_results_df_flattened' DataFrame not found or is empty. Cannot save to GCS.")


import pandas as pd
import numpy as np

# 1. Ensure the seasonality_results_df_flattened DataFrame is available
if 'seasonality_results_df_flattened' not in locals() or seasonality_results_df_flattened.empty:
    print("Error: 'seasonality_results_df_flattened' DataFrame not found or is empty. Cannot generate seasonality summary table.")
    seasonality_summary_table_df = pd.DataFrame() # Initialize as empty DataFrame
else:
    print("Processing 'seasonality_results_df_flattened' to create seasonality summary table...")

    # 2. Initialize an empty list to store the processed summary for each commodity
    summary_entries = []

    # 3. Iterate through each unique commodity
    unique_commodities = seasonality_results_df_flattened['Commodity'].unique()
    print(f"Found {len(unique_commodities)} unique commodities to analyze.")

    for commodity in unique_commodities:
        # a. Filter entries for the current commodity
        commodity_entries = seasonality_results_df_flattened[
            seasonality_results_df_flattened['Commodity'] == commodity
        ].copy()

        # Handle potential empty Analyzed Periods (e.g., if FFT failed for this commodity)
        if commodity_entries.empty or commodity_entries['Period'].isnull().all():
            summary_entries.append({
                'Commodity': commodity,
                'Period 1': np.nan, 'Strength 1': np.nan, 'Ratio 1': np.nan, 'FFT Power 1': np.nan, 'Seasonal 1': False, 'Explanation 1': 'No seasonality data available.',
                'Period 2': np.nan, 'Strength 2': np.nan, 'Ratio 2': np.nan, 'FFT Power 2': np.nan, 'Seasonal 2': False, 'Explanation 2': '',
                'Period 3': np.nan, 'Strength 3': np.nan, 'Ratio 3': np.nan, 'FFT Power 3': np.nan, 'Seasonal 3': False, 'Explanation 3': ''
            })
            continue

        # b. Sort these entries by 'FFT Power at Period' in descending order
        # Fill NaN values with a very small number for sorting to place them at the end
        commodity_entries['FFT Power at Period'] = pd.to_numeric(commodity_entries['FFT Power at Period'], errors='coerce')
        sorted_entries = commodity_entries.sort_values(by='FFT Power at Period', ascending=False, na_position='last')

        # c. Select the top 3 entries
        top_3_patterns = sorted_entries.head(3)

        commodity_summary = {'Commodity': commodity}

        for i, (index, row) in enumerate(top_3_patterns.iterrows()):
            period_num = i + 1
            period = row.get('Period', np.nan)
            strength_stl = row.get('Seasonality Strength (STL)', np.nan)
            ratio_stl = row.get('Calculated Seasonality Score (STL Ratio)', np.nan)
            fft_power = row.get('FFT Power at Period', np.nan)
            is_seasonal = row.get('Is Seasonal for Period', False)

            # e. Construct a 'Significance Explanation' string
            explanation = f"Period: {period}" if pd.notna(period) else ""
            if pd.notna(strength_stl): explanation += f", Strength (STL): {strength_stl:.4f}"
            if pd.notna(ratio_stl): explanation += f", Ratio (STL): {ratio_stl:.4f}"
            if pd.notna(fft_power): explanation += f", FFT Power: {fft_power:.2f}"
            explanation += f". Identified as seasonal: {is_seasonal}."

            commodity_summary[f'Period {period_num}'] = period
            commodity_summary[f'Strength {period_num}'] = strength_stl
            commodity_summary[f'Ratio {period_num}'] = ratio_stl
            commodity_summary[f'FFT Power {period_num}'] = fft_power
            commodity_summary[f'Seasonal {period_num}'] = is_seasonal
            commodity_summary[f'Explanation {period_num}'] = explanation

        # Fill in NaN for any missing top patterns (if less than 3 were found)
        for p in range(len(top_3_patterns) + 1, 4):
            commodity_summary[f'Period {p}'] = np.nan
            commodity_summary[f'Strength {p}'] = np.nan
            commodity_summary[f'Ratio {p}'] = np.nan
            commodity_summary[f'FFT Power {p}'] = np.nan
            commodity_summary[f'Seasonal {p}'] = False
            commodity_summary[f'Explanation {p}'] = ''

        summary_entries.append(commodity_summary)

    # 5. Convert the summary_entries list into a new pandas DataFrame
    seasonality_summary_table_df = pd.DataFrame(summary_entries)
    print("Saving seasonality_summary_table_df to GCS...")

    gcs_prefix_seasonality_summary = 'stats_studies_data/seasonality_summary_table.csv'

    save_dataframe_to_gcs(
        df=seasonality_summary_table_df,
        bucket_name='crystal-dss',
        gcs_prefix=gcs_prefix_seasonality_summary,
        validate_rows=False # Summary tables might have NaNs (e.g., if a commodity had fewer than 3 periods analyzed), so skip row validation
    )


    print("Seasonality summary table generated successfully.")
    print("Head of seasonality_summary_table_df:")
    print(seasonality_summary_table_df.head())
    print("Shape of seasonality_summary_table_df:", seasonality_summary_table_df.shape)

# Assuming seasonality_analysis_results dictionary is available from the previous step

if 'seasonality_analysis_results' not in locals() or not seasonality_analysis_results:
    print("Error: 'seasonality_analysis_results' dictionary not found or is empty. Cannot generate seasonality summary report.")
else:
    print("--- Seasonality Analysis Summary Report ---")

    # Iterate through each commodity's analysis results
    for commodity, result in seasonality_analysis_results.items():
        print(f"\nCommodity: {commodity}")
        print(f"  Overall Analysis Status: {result.get('Analysis Status', 'N/A')}")
        if result.get('Error Message'):
             print(f"  Overall Error Message: {result['Error Message']}")

        print(f"  Overall Seasonality Summary: {result.get('Overall Seasonality Summary', 'No summary available.')}")
        print(f"  Overall Dominant FFT Power: {result.get('Overall FFT Power', 'N/A'):.2f}")

        # Summarize findings for analyzed periods
        analyzed_periods = result.get('Analyzed Periods', [])
        if analyzed_periods:
            print("  Analyzed Periods Summary:")
            for period_result in analyzed_periods:
                period = period_result.get('Period', 'N/A')
                period_status = period_result.get('Period Analysis Status', 'N/A')
                period_error = period_result.get('Period Error Message')
                stl_strength = period_result.get('Seasonality Strength (STL)', np.nan)
                stl_ratio = period_result.get('Calculated Seasonality Score (STL Ratio)', np.nan)
                acf_lag = period_result.get('ACF at Period Lag', np.nan)
                pacf_lag = period_result.get('PACF at Period Lag', np.nan)
                fft_power_at_period = period_result.get('FFT Power at Period', np.nan)
                is_seasonal = period_result.get('Is Seasonal for Period', False)
                is_acf_significant = period_result.get('Is ACF Significant at Period', False)
                is_pacf_significant = period_result.get('Is PACF Significant at Period', False)


                print(f"    Period {period} (Status: {period_status}):")
                if period_error:
                    print(f"      Error: {period_error}")
                print(f"      Seasonality Strength (STL): {stl_strength:.4f}")
                print(f"      Calculated Seasonality Score (STL Ratio): {stl_ratio:.4f}")
                print(f"      ACF at Lag {period}: {acf_lag:.4f}")
                print(f"      PACF at Lag {period}: {pacf_lag:.4f}")
                print(f"      FFT Power at Period: {fft_power_at_period:.2f}")
                print(f"      Is Seasonal for Period (STL+ACF/PACF Check): {is_seasonal}")
                print(f"      Is ACF Significant at Period: {is_acf_significant}")
                print(f"      Is PACF Significant at Period: {is_pacf_significant}")


        else:
            print("  No specific periods were analyzed for this commodity.")

    print("\n--- End of Seasonality Analysis Summary Report ---")

"""# 5. FORECASTER - ARIMA / PROPHET / PROPHET_COVARIATE / SARIMAX / TIMESFM / GRU_MAMBA_KAN

## Forecasting with increasing orders of complexity

"""

import torch
import os

# Define the path to save the model (e.g., in Google Drive)
# Ensure you have mounted Google Drive in a previous cell (e.g., cell SzxtDdaQvm4n) if saving there
mamba_model_save_path = '/content/drive/MyDrive/Colab Notebooks/quant_mamba_model.pth' # You can change the filename and path

# Ensure the directory exists
os.makedirs(os.path.dirname(mamba_model_save_path), exist_ok=True)

# Ensure the model is available from the training step
if 'model_mamba' not in locals() or model_mamba is None:
    print("Error: MAMBA model is not defined. Cannot save the model.")
else:
    try:
        # Save the model's state dictionary
        torch.save(model_mamba.state_dict(), mamba_model_save_path)
        print(f"MAMBA model state dictionary saved successfully to: {mamba_model_save_path}")
    except Exception as e:
        print(f"Error saving MAMBA model: {e}")

# Verify that the prices_df DataFrame is loaded and preprocessed
if 'prices_df' in locals():
    print("prices_df DataFrame is loaded.")

    # Inspect column names to confirm cleaning
    print("\nprices_df columns after cleaning:")
    print(prices_df.columns.tolist())

    # Check if the index is a DatetimeIndex and sorted
    print("\nprices_df index type:", type(prices_df.index))
    print("prices_df index is DatetimeIndex:", isinstance(prices_df.index, pd.DatetimeIndex))
    print("prices_df index is sorted:", prices_df.index.is_monotonic_increasing)


else:
    print("prices_df DataFrame is not loaded. Cannot proceed.")

"""## ARIMA FORECASTER
Rebuild the ARIMA pipeline to iterate through each commodity in `prices_df`, test a set of sample ARIMA parameters, select the best model based on AIC, create a forecast using the best model, and save the combined historical data and forecasts to a new worksheet named "ARIMA Forecasts" in the Google Sheet titled "Commodity Price Forecasts".
"""

import pandas as pd
import numpy as np
import warnings
from statsmodels.tsa.arima.model import ARIMA
from sklearn.model_selection import train_test_split
prices_df = prices_df.copy()

# Define a few sets of (p, q) parameters to test with d=1 if running from scratch
arima_param_sets_to_test = [
    (1, 1),  # ARIMA(1, 1, 1)
    (0, 1),  # ARIMA(0, 1, 1)
    (1, 0),  # ARIMA(1, 1, 0)
    (2, 1),  # ARIMA(2, 1, 1)
    (1, 2),  # ARIMA(1, 1, 2)
    (2, 2)   # ARIMA(2, 1, 2)
]

print("ARIMA parameter sets (p, q) defined for selection (d=1):")
print(arima_param_sets_to_test)
warnings.filterwarnings("ignore") # Suppress warnings during model fitting

# Redefine commodity_columns by extracting from prices_df
all_columns = prices_df.columns.tolist()

commodity_columns = [col for col in all_columns if pd.api.types.is_numeric_dtype(prices_df[col])]

print("Identified commodity columns for ARIMA evaluation:")
print(commodity_columns)


print("\nProceeding with ARIMA model fitting and evaluation for parameter selection...")
# Assuming arima_param_sets_to_test is available from previous cell
if 'arima_param_sets_to_test' not in locals() or not arima_param_sets_to_test:
     print("Error: 'arima_param_sets_to_test' is not defined. Cannot perform evaluation.")
     arima_param_sets_to_test = [] # Initialize as empty to prevent further errors

# Initialize a list to store the evaluation results
arima_evaluation_results = []

if not commodity_columns or not arima_param_sets_to_test:
    print("Cannot perform evaluation due to missing commodity columns or parameter sets.")
else:
    # Loop through each commodity column
    for commodity in commodity_columns:
        print(f"\nProcessing commodity: {commodity}")

        # Prepare the time series data for the current commodity
        # Ensure it's a Series with a DatetimeIndex and no missing values
        series = prices_df[commodity].dropna()

        if series.empty:
            print(f"  Skipping {commodity}: Empty series after dropping NaNs.")
            arima_evaluation_results.append({
                'Commodity': commodity,
                'p,d,q': None,
                'AIC': np.nan,
                'Status': 'Skipped (Empty Series)',
                'Error Message': 'Empty series after dropping NaNs.'
            })
            continue # Skip to the next commodity

        # Split data into training and testing sets (chronological split)
        # Use a fixed test set size (e.g., 20%)
        test_size = 0.2
        train_size = int(len(series) * (1 - test_size))
        train_series, test_series = series[:train_size], series[train_size:]

        print(f"  Train data size: {len(train_series)}, Test data size: {len(test_series)}")

        # Loop through each defined (p, q) parameter set (d is fixed at 1)
        for p, q in arima_param_sets_to_test:
            order = (p, 1, q)
            print(f"  Testing parameters: order={order}")

            try:
                # Attempt to fit an ARIMA model on the training data
                model = ARIMA(train_series, order=order)
                model_fit = model.fit()

                # Calculate AIC for the fitted model
                aic = model_fit.aic
                status = 'Success'
                error_message = None
                print(f"    Fit successful. AIC: {aic:.2f}")


            except Exception as e:
                # Handle errors during model fitting
                aic = np.nan
                status = 'Failed'
                error_message = str(e)
                print(f"    Fit failed: {e}")

            # Store the evaluation results
            arima_evaluation_results.append({
                'Commodity': commodity,
                'p,d,q': order,
                'AIC': aic,
                'Status': status,
                'Error Message': error_message
            })

    print("\nARIMA model fitting and evaluation complete.")

# Convert the list of results into a pandas DataFrame
arima_evaluation_df = pd.DataFrame(arima_evaluation_results)

print("\nARIMA Evaluation Results DataFrame Head:")
print(arima_evaluation_df.head())

#===============================================================================

import pandas as pd
import numpy as np

# Ensure arima_evaluation_df is available from the previous evaluation step
if 'arima_evaluation_df' not in locals() or arima_evaluation_df.empty:
    print("Error: 'arima_evaluation_df' DataFrame not found or is empty. Cannot select best models.")
    # Initialize an empty DataFrame to prevent errors in later steps
    best_arima_models_df = pd.DataFrame()
else:
    print("Selecting the best ARIMA model for each commodity based on AIC...")

    # Filter out failed fits before selecting the best model
    successful_fits_df = arima_evaluation_df[arima_evaluation_df['Status'] == 'Success'].copy()

    if successful_fits_df.empty:
        print("No successful ARIMA model fits found. Cannot select best models.")
        best_arima_models_df = pd.DataFrame()
    else:
        # Find the index of the minimum AIC for each commodity
        # Use idxmin() on the 'AIC' column, grouped by 'Commodity'
        best_models_indices = successful_fits_df.groupby('Commodity')['AIC'].idxmin()

        # Select the rows corresponding to the best models using the indices
        best_arima_models_df = successful_fits_df.loc[best_models_indices].reset_index(drop=True)

        print("\nBest ARIMA Models Selected (based on minimum AIC):")
        print(best_arima_models_df.head())

        # You now have best_arima_models_df containing the best parameters for each commodity.

#===============================================================================
# Use the Best Model for each commodity
#===============================================================================
import pandas as pd
import numpy as np
import warnings
from statsmodels.tsa.arima.model import ARIMA
import asyncio # Keep asyncio for potential future async needs or if running in Colab notebook
from concurrent.futures import ThreadPoolExecutor
from pandas.tseries.frequencies import infer_freq

warnings.filterwarnings("ignore") # Suppress warnings during model fitting

# Ensure prices_df and best_arima_models_df are available from previous steps
if 'prices_df' not in locals():
    print("Error: 'prices_df' DataFrame not found. Cannot proceed with forecasting.")
    # Initialize arima_forecast_results as empty to prevent errors in later steps
    arima_forecast_results = []
elif 'best_arima_models_df' not in locals() or best_arima_models_df.empty:
    print("Error: 'best_arima_models_df' DataFrame not found or is empty. Cannot proceed with forecasting.")
    # Initialize arima_forecast_results as empty to prevent errors in later steps
    arima_forecast_results = []
else:
    print("\nProceeding with forecasting using the best selected ARIMA parameters.")

    # Define the number of steps to forecast
    forecast_steps = 24 # You can adjust this number
    print(f"Forecasting {forecast_steps} steps into the future.")

    # Initialize a list to store the forecast results
    arima_forecast_results = []

    # Configure the thread pool executor for concurrent forecasting
    max_workers = 8 # Example: use 8 worker threads
    executor = ThreadPoolExecutor(max_workers=max_workers)
    tasks = []

    # Iterate through the best_arima_models_df to get the best parameters for each commodity
    if not best_arima_models_df.empty:
        print("\nStarting ARIMA forecasting for all commodities using best parameters...")

        # Create a dictionary mapping commodity name to its best (p, d, q) order
        best_orders_dict = pd.Series(best_arima_models_df['p,d,q'].values, index=best_arima_models_df['Commodity']).to_dict()

        # Loop through each commodity that had a successful best model selection
        for commodity, best_order in best_orders_dict.items():
             # Prepare the full historical time series data for the current commodity
             # Ensure it's a Series with a DatetimeIndex and no missing values
             series = prices_df[commodity].dropna()

             if series.empty:
                 print(f"  Skipping forecasting for {commodity}: Empty series after dropping NaNs.")
                 arima_forecast_results.append({
                     'Commodity': commodity,
                     'Status': 'Skipped (Empty Series)',
                     'Error Message': 'Empty series after dropping NaNs.',
                     'ARIMA Order': best_order, # Include the parameters used
                     'Forecast Dates': None, # Store None if skipped
                     'Forecast Values': None # Store None if skipped
                 })
                 continue # Skip to the next commodity

             # Create an async task for forecasting this commodity
             async def forecast_single_commodity_async(series, commodity_name, order, steps, executor):
                  try:
                     print(f"  Starting fitting and forecasting for {commodity_name}...")
                     loop = asyncio.get_event_loop()
                     model = ARIMA(series, order=order)
                     # Run the synchronous model fitting in a thread pool
                     model_fit = await loop.run_in_executor(executor, lambda: model.fit())

                     # Generate forecast
                     forecast = model_fit.forecast(steps=steps)
                     print(f"  Finished fitting and forecasting for {commodity_name}.")

                     # Explicitly generate future dates based on the last date of the historical series
                     # and the number of forecast steps. Infer the frequency from the original series index.
                     freq = infer_freq(series.index)
                     if freq is None:
                          # If frequency cannot be inferred, assume daily 'D' as a fallback
                          freq = 'W'

                     last_date = series.index[-1]
                     # Generate future dates starting *after* the last historical date
                     forecast_dates = pd.date_range(start=last_date, periods=steps + 1, freq=freq)[1:]


                     return {
                         'Commodity': commodity_name,
                         'Status': 'Success',
                         'Error Message': None,
                         'ARIMA Order': order, # Include the parameters used
                         'Forecast Dates': forecast_dates.tolist(), # Store the generated DatetimeIndex as a list of datetime objects
                         'Forecast Values': forecast.values.tolist() # Convert numpy array to list
                     }
                  except Exception as e:
                     print(f"  Forecasting failed for {commodity_name}: {e}")
                     return {
                         'Commodity': commodity_name,
                         'Status': 'Failed',
                         'Error Message': str(e),
                         'ARIMA Order': order, # Include the parameters used
                         'Forecast Dates': None, # Store None if failed
                         'Forecast Values': None # Store None if failed
                     }

             tasks.append(forecast_single_commodity_async(series, commodity, best_order, forecast_steps, executor))


        # Run the async tasks concurrently
        try:
            all_forecasts = await asyncio.gather(*tasks)
            arima_forecast_results.extend(all_forecasts) # Add results from async tasks

        except Exception as e:
            print(f"\nAn error occurred during concurrent ARIMA forecasting: {e}")
        finally:
            # Ensure the executor is shut down
            executor.shutdown(wait=True)


    else:
        print("No best ARIMA models selected to perform forecasting.")


    # Convert the list of results into a pandas DataFrame
    arima_forecast_df = pd.DataFrame(arima_forecast_results)

    print("\nARIMA Forecast Results DataFrame Head:")
    print(arima_forecast_df.head(1))


#===============================================================================
# Create a combined dataframe
#===============================================================================
import pandas as pd
import numpy as np
import gspread
from google.colab import auth
from google.auth import default
import warnings

warnings.filterwarnings("ignore") # Suppress warnings during data manipulation and saving

# Ensure prices_df (original data) and arima_forecast_df (forecast results) are available
if 'prices_df' not in locals():
    print("Error: 'prices_df' DataFrame not found. Cannot combine data.")
elif 'arima_forecast_df' not in locals() or arima_forecast_df.empty:
    print("Error: 'arima_forecast_df' DataFrame not found or is empty. Cannot combine data.")
else:
    print("Combining original data and ARIMA forecasts...")

    # Prepare the original data: select commodity columns and ensure DatetimeIndex
    # Exclude non-numeric and cyclical features as done before
    all_columns = prices_df.columns.tolist()

    commodity_columns = [col for col in all_columns if pd.api.types.is_numeric_dtype(prices_df[col])]

    # Ensure historical_df has the correct DatetimeIndex
    # Assuming the index of prices_df is already the correct DatetimeIndex
    historical_df = prices_df[commodity_columns].copy()

    # Prepare the forecast data:
    # Create a dictionary to store forecast series for each commodity
    forecast_series_dict = {}
    for index, row in arima_forecast_df.iterrows():
        # Corrected: Use 'Status' instead of 'Forecast Status'
        if row['Status'] == 'Success' and row['Forecast Dates'] is not None and row['Forecast Values'] is not None:
            commodity = row['Commodity']
            # Use the actual list of datetime objects from 'Forecast Dates' and 'Forecast Values'
            forecast_dates = row['Forecast Dates']
            forecast_values = row['Forecast Values']

            # Create a Series for this commodity's forecast with the correct DatetimeIndex
            try:
                forecast_series = pd.Series(forecast_values, index=pd.to_datetime(forecast_dates))
                forecast_series_dict[commodity] = forecast_series
            except Exception as e:
                 print(f"Warning: Could not create forecast Series for {commodity}: {e}")
                 # If Series creation fails, this commodity's forecast won't be included in the combined df

    # Convert the dictionary of forecast series into a DataFrame
    # The index will be the combined DatetimeIndex from all series
    if forecast_series_dict:
        # Concatenate all forecast series into a single DataFrame with DatetimeIndex
        forecast_wide_df = pd.DataFrame(forecast_series_dict)
    else:
        # If no successful forecasts, create an empty DataFrame with the same columns as historical
        print("No successful forecasts to combine.")
        forecast_wide_df = pd.DataFrame(columns=historical_df.columns)

    # Combine the historical data and the forecast data
    # Use outer join to include all dates from both historical and forecast periods
    # The index from both DataFrames (DatetimeIndex) will be used for alignment
    combined_df = pd.concat([historical_df, forecast_wide_df], axis=0, join='outer')

    # Sort by date to ensure chronological order
    combined_df.sort_index(inplace=True)

    # Rename the index to 'Date'
    combined_df.index.name = 'Date'

    print("\nCombined DataFrame Head:")
    print(combined_df.head(2))
    print("\nCombined DataFrame Tail:")
    print(combined_df.tail(2))
    print("\nCombined DataFrame Shape:", combined_df.shape)

    if not combined_df.empty:
        print("\nSaving combined ARIMA forecast results to GCS...")
        gcs_prefix_arima_forecast = 'forecast_data/arima_forecast.csv'

        save_dataframe_to_gcs(
            df=combined_df,
            bucket_name='crystal-dss',
            gcs_prefix=gcs_prefix_arima_forecast,
            validate_rows=False # Summary tables might have NaNs (e.g., if a commodity had fewer than 3 periods analyzed), so skip row validation
        )
        print(f"Combined ARIMA forecast results saved to GCS prefix: {gcs_prefix_arima_forecast}")

    else:
        print("Combined DataFrame is empty. No data to save.")

"""## FB PROPHET"""

# PROPHET

# Imports
import numpy as np
import pandas as pd
from prophet import Prophet
import nest_asyncio
import asyncio
from concurrent.futures import ThreadPoolExecutor
import gspread
from google.colab import auth
from google.auth import default

# Allow nested async in notebooks
nest_asyncio.apply()
import asyncio, aiohttp

df = prices_df
print (f"Original Shape : {df.shape}")

# Forecast function (runs in threads)
def forecast_blocking(col_name, series, periods=24):
    try:
        ts = pd.DataFrame({'ds': series.index, 'y': series.values})
        ts['y'] = ts['y'].astype(float)  # Ensure float for NaNs
        model = Prophet()
        model.fit(ts)
        future = model.make_future_dataframe(periods=periods, freq='w')
        forecast = model.predict(future)
        return forecast[['ds', 'yhat']].rename(columns={'yhat': col_name}).set_index('ds')
    except Exception as e:
        print(f"[{col_name}] forecast error → {e}")
        return pd.DataFrame()


# Async wrapper to run forecast in thread pool
async def run_in_executor(loop, executor, func, *args):
    return await loop.run_in_executor(executor, func, *args)

# Orchestration logic
async def forecast_all_series(df, periods=12, max_workers=8):
    loop = asyncio.get_event_loop()
    executor = ThreadPoolExecutor(max_workers=max_workers)
    # Filter out non-numeric columns before creating tasks
    numeric_cols = df.select_dtypes(include=np.number).columns
    tasks = [
        run_in_executor(loop, executor, forecast_blocking, col, df[col], periods)
        for col in numeric_cols
    ]
    results = await asyncio.gather(*tasks)
    executor.shutdown(wait=True)
    # Filter out empty DataFrames which are returned for skipped series
    results = [res for res in results if not res.empty]
    if results:
        return pd.concat(results, axis=1).reset_index()
    else:
        return pd.DataFrame()


# Run everything
forecast_df = await forecast_all_series(df, periods=24)
print(forecast_df.shape)
print(forecast_df.head())

# Get the last date from the original DataFrame
last_historical_date = df.index.max()

# Filter forecast_df to keep only future dates
# Ensure 'ds' is a datetime type before comparison
if 'ds' in forecast_df.columns:
    forecast_df['ds'] = pd.to_datetime(forecast_df['ds'])
    future_forecast_df = forecast_df[forecast_df['ds'] > last_historical_date].set_index('ds')
else:
    future_forecast_df = pd.DataFrame() # Handle case where forecast_df is empty


# Combine the original DataFrame with the future forecast
# Ensure 'date' column from original df is not included in the concat if it's not the index
df_to_concat = df.copy()
if 'date' in df_to_concat.columns:
    df_to_concat = df_to_concat.drop(columns=['date'])

# Align columns before concatenation, filling missing columns with NaN
combined_historical_and_forecast_df = pd.concat([df_to_concat, future_forecast_df], axis=0, join='outer')


# Convert all columns to numeric, coercing errors
combined_historical_and_forecast_df = combined_historical_and_forecast_df.apply(pd.to_numeric, errors='coerce')

# Rename the index to 'date'
combined_historical_and_forecast_df.index.name = 'date'


print(combined_historical_and_forecast_df.shape)
print(combined_historical_and_forecast_df.head(2))
print(combined_historical_and_forecast_df.tail(2))


#===============================================================================
# Save the combined historical and forecast data to Google Sheets
#===============================================================================
if not combined_historical_and_forecast_df.empty:
    print("\nSaving combined historical and Prophet forecast data to GCS...")
    gcs_prefix_fb_prophet_forecast = 'forecast_data/fb_prophet_forecast.csv'
    save_dataframe_to_gcs(
        df=combined_historical_and_forecast_df,
        bucket_name='crystal-dss',
        gcs_prefix=gcs_prefix_fb_prophet_forecast,
        validate_rows=False
else:
    print("No combined historical and Prophet forecast data found to save.")


#===============================================================================
# Visualize the first 5 time series
#===============================================================================
import matplotlib.pyplot as plt
# Assuming df is the original DataFrame with historical data
start_date = df.index.min()
end_date = df.index.max()

print(f"Start Date: {start_date}")
print(f"End Date: {end_date}")

# Get the list of columns to plot (excluding the index and potentially the original 'date' column if it wasn't dropped earlier)
cols_to_plot = [col for col in combined_historical_and_forecast_df.columns if col != 'date']

# Visualize the first 5 columns (excluding the 'date' column if present)
for i, col in enumerate(cols_to_plot[:5]):
    plt.figure(figsize=(12, 3))
    plt.plot(combined_historical_and_forecast_df.index, combined_historical_and_forecast_df[col])
    plt.title(f'Time Series Forecast for {col}')
    plt.xlabel('Date')
    plt.ylabel(col)
    # Add a vertical red dashed line at the end_date of historical data
    plt.axvline(end_date, color='red', linestyle='--', label='forecast_period_start')
    plt.legend() # Add legend to show the label for the vertical line
    plt.show()

"""## PROPHET WITH COVARIATES"""

import pandas as pd
from prophet import Prophet
import numpy as np
import warnings
import asyncio
from concurrent.futures import ThreadPoolExecutor

warnings.filterwarnings("ignore") # Suppress warnings during model fitting

# ==============================================================================
# Define exog_regressors, create forecast and save df to be called in next step
# ==============================================================================
forecast_steps = 24
prices_df = prices_df

# Identify all columns in the prices_df DataFrame
all_columns = prices_df.columns.tolist()

# Define columns to exclude (non-commodities and cyclical features)
cols_to_exclude = ['date', 'day_of_week', 'month', 'year',
                   'day_of_week_sin', 'day_of_week_cos',
                   'month_sin', 'month_cos',
                   'year_sin', 'year_cos']

# Create a list of commodity columns by excluding the specified columns and keeping only numeric ones
commodity_columns = [col for col in all_columns if col not in cols_to_exclude and pd.api.types.is_numeric_dtype(prices_df[col])]

# Define the list of the first 7 commodities as covariates




# Define the list of the remaining commodities as target commodities
target_commodities = commodity_columns[8:]

target_commodities = [
    'Acetic Acid',
    'Butyl Acetate',
    'Toluene',
    'Isomer-MX',
    'Solvent-MX',
    'Methanol',
    'MTBE',
    'Benzene'
]

# Define the other commodities to correlate against
exog_regressors = [
    'Gold',
    'Silver',
    'Copper',
    'S&P 500',
    'Shanghai Composite',
    'USD Index',
    'Japanese Yen',
    'US 10-Y BOND PRICE',
    'Crude Oil',
    'Natural Gas',
    'Naphtha',
    'EDC'
    'Ethylene',
    'Propylene',
    'N-Butanol',
    'OrthXylene',
    'Cyclohexane',
    'Acetic Acid',
    'Toluene',
    'Methanol',
    'MTBE',
    'Benzene'
]

# Print the lists to verify
print("Exogenous Regressors:")
print(exog_regressors)
print("\nTarget Commodities:")
print(target_commodities)

if 'exog_regressors' not in locals() or not exog_regressors:
    print("Error: 'exog_regressors' list is not defined or is empty. Cannot forecast covariates.")
    # Finish the task with failure if exogenous regressors are not defined
    # Initialize necessary variables to prevent errors in later steps
    covariate_forecasts = {}
    future_exog_data = pd.DataFrame()
else:
    print(f"Forecasting the following covariates using separate Prophet models: {exog_regressors}")

    # Check if forecast_steps is defined
    if 'forecast_steps' not in locals():
         print("Error: 'forecast_steps' is not defined. Cannot forecast covariates.")
         # Finish the task with failure if forecast_steps is not defined
         covariate_forecasts = {}
         future_exog_data = pd.DataFrame()
    else:
        print(f"Forecasting {forecast_steps} steps into the future for each covariate.")

        # Dictionary to store forecast results for each covariate
        covariate_forecasts = {}

        # Configure the thread pool executor for concurrent forecasting
        max_workers = 8 # Example: use 8 worker threads
        executor = ThreadPoolExecutor(max_workers=max_workers)
        tasks = []

        # Async function to fit Prophet and forecast for a single covariate
        async def forecast_single_covariate_async(series, covariate_name, steps, executor):
             """
             Fits a Prophet model and generates a forecast for a single covariate
             in an async context, running synchronous operations in a thread pool.
             """
             print(f"  Starting fitting and forecasting for covariate: {covariate_name}...")
             try:
                loop = asyncio.get_event_loop()

                # Prepare data for Prophet
                covariate_df = series.reset_index().rename(columns={'Date': 'ds', covariate_name: 'y'})
                covariate_df['ds'] = pd.to_datetime(covariate_df['ds'])
                covariate_df['y'] = pd.to_numeric(covariate_df['y'], errors='coerce')
                covariate_df.dropna(inplace=True) # Drop NaNs after coercion

                if covariate_df.empty:
                    print(f"  Skipping fitting for {covariate_name}: Empty data after preparation.")
                    return covariate_name, pd.DataFrame() # Return empty DataFrame if data is insufficient

                # Run the synchronous Prophet fitting in a thread pool
                model = Prophet()
                model_fit = await loop.run_in_executor(executor, lambda: model.fit(covariate_df))
                # print(f"  Prophet model fitted for {covariate_name}.") # Keep print for debugging if needed

                # Create future DataFrame for the covariate forecast
                future_covariate = model_fit.make_future_dataframe(periods=steps)

                # Generate forecast for the covariate
                forecast_covariate = model_fit.predict(future_covariate)
                # print(f"  Forecast generated for {covariate_name}.") # Keep print for debugging if needed

                # Store the forecasted values ('yhat') with the 'ds' (date)
                # We only need the future dates beyond the historical data
                last_historical_date = covariate_df['ds'].max()
                future_covariate_forecast = forecast_covariate[forecast_covariate['ds'] > last_historical_date][['ds', 'yhat']].rename(columns={'yhat': covariate_name})

                # Set 'ds' as index
                future_covariate_forecast.set_index('ds', inplace=True)

                print(f"  Finished fitting and forecasting for covariate: {covariate_name}.")
                return covariate_name, future_covariate_forecast

             except Exception as e:
                print(f"  Error forecasting covariate {covariate_name}: {e}")
                return covariate_name, pd.DataFrame() # Return empty DataFrame on error

        # Create a list of async tasks for forecasting each covariate
        tasks = []
        for covariate in exog_regressors:
             # Check if the covariate column exists and is numeric before creating a task
             if covariate in prices_df.columns and pd.api.types.is_numeric_dtype(prices_df[covariate]):
                 tasks.append(forecast_single_covariate_async(prices_df[covariate].dropna(), covariate, forecast_steps, executor))
             else:
                 print(f"  Skipping {covariate} as it is not found or not numeric in the DataFrame.")
                 # Add an empty DataFrame entry for skipped covariates
                 covariate_forecasts[covariate] = pd.DataFrame()


        # Run the async tasks concurrently and collect the results
        try:
            all_covariate_results = await asyncio.gather(*tasks)
            # Populate the dictionary with results
            for cov_name, forecast_df in all_covariate_results:
                covariate_forecasts[cov_name] = forecast_df

        except Exception as e:
            print(f"\nAn error occurred during concurrent covariate forecasting: {e}")
        finally:
            # Ensure the executor is shut down
            executor.shutdown(wait=True)


        # --- Combine the covariate forecasts into a single DataFrame ---
        # Filter out empty DataFrames before concatenating
        non_empty_forecasts = {k: v for k, v in covariate_forecasts.items() if not v.empty}

        if non_empty_forecasts:
            # Concatenate the individual covariate forecast DataFrames
            future_exog_data = pd.concat(non_empty_forecasts.values(), axis=1)

            print("\nCombined future exogenous data (head):")
            print(future_exog_data.tail(5))
            print("\nCombined future exogenous data (shape):", future_exog_data.shape)

        else:
            print("\nNo successful covariate forecasts were generated.")
            future_exog_data = pd.DataFrame() # Create an empty DataFrame if no forecasts


print("\nCovariate forecasting complete.")

import pandas as pd
import numpy as np

# Ensure prices_df (original historical data) and covariate_forecasts (dictionary of covariate forecasts) are available
# Assuming exog_regressors (list of covariate column names) is available

if 'prices_df' not in locals():
    print("Error: 'prices_df' DataFrame not found. Cannot create covariate DataFrames.")
    historical_data_available = False
else:
    historical_data_available = True
    print("Using 'prices_df' for historical data.")

if 'covariate_forecasts' not in locals() or not covariate_forecasts:
    print("Error: 'covariate_forecasts' dictionary not found or is empty. Cannot create covariate DataFrames.")
    forecast_data_available = False
else:
    forecast_data_available = True
    print("Using 'covariate_forecasts' for covariate forecast data.")

if 'exog_regressors' not in locals() or not exog_regressors:
    print("Error: 'exog_regressors' list not found or is empty. Cannot identify covariates.")
    covariates_defined = False
else:
    covariates_defined = True
    print(f"Using defined exogenous regressors: {exog_regressors}")


# --- Create DataFrame with Covariate Forecasts Only ---
covariates_forecast_only_df = pd.DataFrame() # Initialize as empty

if forecast_data_available and covariates_defined:
    print("\nCreating DataFrame with covariate forecast data only...")

    # The covariate_forecasts dictionary already contains DataFrames indexed by 'ds' (datetime)
    # We can concatenate these directly.
    try:
        covariates_forecast_only_df = pd.concat(covariate_forecasts.values(), axis=1)

        # Rename index for clarity
        covariates_forecast_only_df.index.name = 'Date'
    except Exception as e:
        print(f"Error concatenating covariate forecasts: {e}")


# --- Create DataFrame with Combined History and Forecasts for Covariates ---
combined_covariates_df = pd.DataFrame() # Initialize as empty

if historical_data_available and forecast_data_available and covariates_defined:
    print("\nCreating DataFrame with combined history and forecasts for covariates...")

    # Get the historical data for the covariates
    historical_covariates_df = prices_df[exog_regressors].dropna().copy()

    # Ensure historical_covariates_df has a DatetimeIndex and rename it 'Date'
    historical_covariates_df.index.name = 'Date'

    # The covariates_forecast_only_df already has the forecast data with 'Date' as index

    # Combine the historical and forecast data for covariates
    # Use outer join to include all dates from both historical and forecast periods
    combined_covariates_df = pd.concat([historical_covariates_df, covariates_forecast_only_df], axis=0, join='outer')

    # Sort by date
    combined_covariates_df.sort_index(inplace=True)
    print(combined_covariates_df.tail(2))
    print("\nCombined Covariates History and Forecasts DataFrame Shape:", combined_covariates_df.shape)

else:
    print("\nCannot create combined covariates DataFrame: Required data is not available.")

# You now have 'covariates_forecast_only_df' and 'combined_covariates_df' DataFrames.

import pandas as pd
import numpy as np

# ==============================================================================
# Prepare Data - historical and futures for exog_regressors
# ==============================================================================

# --- Start: Data Preparation ---
# We will now use prices_df for historical target and covariate data,
# and covariates_forecast_only_df for future covariate data.

print("Preparing historical target data and identifying future covariate data...")

# Dictionary to store historical data for each target commodity in Prophet format
# This will now also include historical covariate data
historical_target_data = {}

# Check if prices_df is available
if 'prices_df' not in locals() or prices_df is None:
    print("Error: 'prices_df' DataFrame not found. Cannot prepare historical data.")
else:
    # Ensure prices_df has a DatetimeIndex named 'Date'
    if not isinstance(prices_df.index, pd.DatetimeIndex) or prices_df.index.name != 'Date':
        print("prices_df index is not a DatetimeIndex named 'Date'. Attempting to set index...")
        try:
            # Assuming the 'date' column is the correct datetime column
            if 'date' in prices_df.columns:
                prices_df['Date'] = pd.to_datetime(prices_df['date'])
                prices_df.set_index('Date', inplace=True)
                prices_df.drop(columns=['date'], errors='ignore', inplace=True)
                print("Set 'Date' as index in prices_df.")
            elif prices_df.index.name is None and pd.api.types.is_datetime64_any_dtype(prices_df.index):
                 # If index is datetime but unnamed, just name it
                 prices_df.index.name = 'Date'
                 print("Named index 'Date' in prices_df.")
            else:
                 print("Warning: Could not set 'Date' as DatetimeIndex in prices_df.")
        except Exception as e:
            print(f"Error setting 'Date' as DatetimeIndex in prices_df: {e}")


    # Check if target_commodities and exog_regressors are defined
    if 'target_commodities' not in locals() or not target_commodities:
        print("Error: target_commodities are not defined or are empty. Please define them first.")
    elif 'exog_regressors' not in locals() or not exog_regressors:
         print("Error: exog_regressors are not defined or are empty. Please define them first.")
    else:
        # Prepare the historical covariate data from prices_df
        # Ensure only the defined exogenous regressors are selected
        historical_covariates_df = prices_df[exog_regressors].dropna().copy()
        # Ensure historical_covariates_df has a DatetimeIndex and rename it 'Date' for merging
        historical_covariates_df.index.name = 'Date'


        # Iterate through each target commodity to get historical data and merge with historical covariates
        for target_commodity in target_commodities:
            if target_commodity in prices_df.columns and pd.api.types.is_numeric_dtype(prices_df[target_commodity]):
                # Get the historical series for the target commodity and format for Prophet
                target_series = prices_df[target_commodity].dropna().copy()
                historical_target_df_prophet_format = target_series.reset_index().rename(columns={'Date': 'ds', target_commodity: 'y'})
                # Ensure 'ds' is datetime
                historical_target_df_prophet_format['ds'] = pd.to_datetime(historical_target_df_prophet_format['ds'])


                # Merge the historical target data with the historical covariate data
                # Use 'inner' merge to keep only dates present in both target and all covariates
                # or 'left' merge on target_df to keep all target dates and add covariates where available
                # Let's use 'left' to keep all historical target dates
                historical_data_with_covariates = pd.merge(
                    historical_target_df_prophet_format,
                    historical_covariates_df.reset_index().rename(columns={'Date': 'ds'}), # Merge on 'ds'
                    how='left',
                    on='ds'
                )

                # Ensure the 'ds' column is the first column
                cols = ['ds', 'y'] + [col for col in historical_data_with_covariates.columns if col not in ['ds', 'y']]
                historical_data_with_covariates = historical_data_with_covariates[cols]

                # Store the prepared and merged DataFrame (historical target + historical covariates)
                historical_target_data[target_commodity] = historical_data_with_covariates
                print(f"  Historical data prepared and merged with covariates for {target_commodity}. Shape: {historical_data_with_covariates.shape}")


            else:
                print(f"  Skipping historical data preparation for {target_commodity} as it is not found or not numeric in prices_df.")

        print("\nHistorical data prepared and merged with covariates for target commodities.")
        print(f"Historical dataframes available for {len(historical_target_data)} target commodities.")


# Check if covariates_forecast_only_df is available for future covariates
if 'covariates_forecast_only_df' not in locals() or covariates_forecast_only_df.empty:
    print("Error: 'covariates_forecast_only_df' DataFrame not found or is empty. Cannot provide future covariate data for forecasting.")
    future_covariate_data_available = False
else:
    print("\nUsing 'covariates_forecast_only_df' for future covariate data.")
    # Ensure covariates_forecast_only_df has a DatetimeIndex named 'Date'
    if not isinstance(covariates_forecast_only_df.index, pd.DatetimeIndex) or covariates_forecast_only_df.index.name != 'Date':
        print("covariates_forecast_only_df index is not a DatetimeIndex named 'Date'. Attempting to set index...")
        try:
            # Assuming the index is the date column, just name it
            if covariates_forecast_only_df.index.name is None and pd.api.types.is_datetime64_any_dtype(covariates_forecast_only_df.index):
                 covariates_forecast_only_df.index.name = 'Date'
                 print("Named index 'Date' in covariates_forecast_only_df.")
            else:
                 print("Warning: Could not set 'Date' as DatetimeIndex in covariates_forecast_only_df index.")
                 future_covariate_data_available = False # Mark as not available if date index is problematic

        except Exception as e:
            print(f"Error setting 'Date' as DatetimeIndex in covariates_forecast_only_df: {e}")
            future_covariate_data_available = False # Mark as not available on error
    else:
         print("'Date' is already DatetimeIndex in covariates_forecast_only_df.")
         future_covariate_data_available = True

# --- End: Data Preparation ---

# ==============================================================================
# Run PROPHET with exog_regressors
# ==============================================================================

import pandas as pd
import numpy as np
from prophet import Prophet
import warnings
import asyncio
from concurrent.futures import ThreadPoolExecutor
import gspread
from google.colab import auth
from google.auth import default

warnings.filterwarnings("ignore") # Suppress warnings during model fitting

# --- Start: Prophet Forecasting with Covariates Module ---
# This module assumes historical_target_data (dict of historical target dataframes),
# covariates_forecast_only_df (future covariate data), and exog_regressors (list of covariate names)
# are available from previous steps.

# Check if necessary data is available
if 'historical_target_data' not in locals() or not historical_target_data:
    print("Error: 'historical_target_data' not found or is empty. Cannot proceed with forecasting module.")
    # Initialize results list and forecast DataFrame as empty if data is not available
    prophet_covariates_forecast_results = []
    prophet_covariates_forecast_summary_df = pd.DataFrame()
    all_targets_combined_forecast_df = pd.DataFrame()
    combined_targets_historical_forecast_df = pd.DataFrame()

elif 'covariates_forecast_only_df' not in locals() or covariates_forecast_only_df.empty:
    print("Error: 'covariates_forecast_only_df' DataFrame not found or is empty. Cannot proceed with forecasting module.")
    # Initialize results list and forecast DataFrame as empty if data is not available
    prophet_covariates_forecast_results = []
    prophet_covariates_forecast_summary_df = pd.DataFrame()
    all_targets_combined_forecast_df = pd.DataFrame()
    combined_targets_historical_forecast_df = pd.DataFrame()

elif 'exog_regressors' not in locals() or not exog_regressors:
    print("Error: 'exog_regressors' list not found or is empty. Cannot proceed with forecasting module.")
    # Initialize results list and forecast DataFrame as empty if data is not available
    prophet_covariates_forecast_results = []
    prophet_covariates_forecast_summary_df = pd.DataFrame()
    all_targets_combined_forecast_df = pd.DataFrame()
    combined_targets_historical_forecast_df = pd.DataFrame()

else:
    print("\nStarting Prophet Forecasting with Covariates Module...")

    # Define the number of steps to forecast
    # We will determine this from the number of rows in covariates_forecast_only_df
    forecast_steps = len(covariates_forecast_only_df)
    print(f"Forecasting {forecast_steps} steps based on the number of future covariate data points.")

    # Configure the thread pool executor for concurrent forecasting
    max_workers = 8 # Example: use 8 worker threads
    executor = ThreadPoolExecutor(max_workers=max_workers)

    # Initialize a list to store the forecast results
    prophet_covariates_forecast_results = []

    # Async function to fit Prophet with covariates and forecast for a single commodity
    async def forecast_prophet_with_covariates(historical_df, target_commodity_name, exog_regressor_names, future_exog_df, executor):
        """
        Fits a Prophet model with exogenous regressors using historical data
        and generates a forecast using future exogenous data.
        """
        print(f"  Starting fitting and forecasting for target: {target_commodity_name}...")
        try:
            loop = asyncio.get_event_loop()

            # Initialize Prophet model
            model = Prophet(daily_seasonality=False) # Disable daily seasonality for simplicity

            # Add exogenous regressors to the model and keep track of which ones were added
            added_regressors = []
            for regressor in exog_regressor_names:
                 # Ensure regressor column exists in historical_df before adding
                 if regressor in historical_df.columns:
                      model.add_regressor(regressor)
                      added_regressors.append(regressor) # Add to our list if successfully added
                 else:
                      print(f"    Warning: Covariate '{regressor}' not found in historical data for {target_commodity_name}. Skipping addition.")


            # Fit the Prophet model on historical data in a thread pool
            model_fit = await loop.run_in_executor(executor, lambda: model.fit(historical_df))
            # print(f"  Prophet model with covariates fitted for {target_commodity_name}.") # Optional print


            # Create the future DataFrame for prediction
            # This should contain 'ds' and the regressor columns from future_exog_df
            future_df_for_predict = future_exog_df.reset_index().rename(columns={'Date': 'ds'})
            # Ensure 'ds' is datetime
            future_df_for_predict['ds'] = pd.to_datetime(future_df_for_predict['ds'])

            # Ensure only regressors that were successfully added to the model are in future_df_for_predict
            # and that the 'ds' column is also present
            # Use the 'added_regressors' list we created
            future_regressor_cols_present = [col for col in added_regressors if col in future_df_for_predict.columns]
            cols_for_predict_df = ['ds'] + future_regressor_cols_present

            # Select only the necessary columns for prediction
            future_df_for_predict = future_df_for_predict[cols_for_predict_df].copy()

            # Check if future_df_for_predict has the expected columns for prediction
            # Prophet expects 'ds' and all added regressors
            expected_predict_cols = ['ds'] + added_regressors
            if not set(expected_predict_cols).issubset(future_df_for_predict.columns):
                 missing_cols = set(expected_predict_cols) - set(future_df_for_predict.columns)
                 raise ValueError(f"Future DataFrame for prediction is missing required columns: {missing_cols}")


            # Generate forecast using the model and the future DataFrame
            # Run the synchronous model_fit.predict() in a thread pool
            forecast = await loop.run_in_executor(executor, lambda: model_fit.predict(future_df_for_predict))
            # print(f"  Forecast generated for {target_commodity_name}.") # Optional print


            print(f"  Finished fitting and forecasting for target: {target_commodity_name}.")

            # Return the relevant forecast columns
            return {
                'Commodity': target_commodity_name,
                'Status': 'Success',
                'Error Message': None,
                'Forecast DataFrame': forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']], # Include uncertainty intervals
            }

        except Exception as e:
            print(f"  Forecasting failed for target {target_commodity_name}: {e}")
            return {
                'Commodity': target_commodity_name,
                'Status': 'Failed',
                'Error Message': str(e),
                'Forecast DataFrame': pd.DataFrame() # Return empty DataFrame on error
            }

    # Create a list of async tasks for forecasting each target commodity
    tasks = []
    print("\nStarting concurrent Prophet forecasting with covariates for target commodities...")

    # Iterate through the historical data for each target commodity
    # Assuming historical_target_data is a dictionary where keys are commodity names and values are dataframes
    for target_commodity, historical_df in historical_target_data.items():
         # Ensure future covariate data is available and pass it to the async function
         if 'future_covariate_data_available' in locals() and future_covariate_data_available and 'covariates_forecast_only_df' in locals() and not covariates_forecast_only_df.empty:
              tasks.append(forecast_prophet_with_covariates(historical_df, target_commodity, exog_regressors, covariates_forecast_only_df, executor))
         else:
              print(f"  Skipping forecasting for {target_commodity}: Future covariate data not available.")
              prophet_covariates_forecast_results.append({
                  'Commodity': target_commodity,
                  'Status': 'Skipped (Future Covariate Data Missing)',
                  'Error Message': 'Future covariate data not available for forecasting.',
                  'Forecast DataFrame': pd.DataFrame() # Return empty DataFrame
              })


    # Run the async tasks concurrently and collect the results
    try:
        all_forecasts = await asyncio.gather(*tasks)
        prophet_covariates_forecast_results.extend(all_forecasts) # Add results from async tasks

    except Exception as e:
        print(f"\nAn error occurred during concurrent Prophet forecasting with covariates: {e}")
    finally:
        # Ensure the executor is shut down
        executor.shutdown(wait=True)


    print("\nProphet forecasting with covariates complete for all specified target commodities.")

    # Convert the list of results into a pandas DataFrame
    # This DataFrame will summarize the forecasting status for each commodity
    prophet_covariates_forecast_summary_df = pd.DataFrame([
        {'Commodity': res['Commodity'], 'Status': res['Status'], 'Error Message': res['Error Message']}
        for res in prophet_covariates_forecast_results
    ])

    print("\nProphet with Covariates Forecast Summary DataFrame Head:")
    print(prophet_covariates_forecast_summary_df.head())

    # Combine the successful future forecast DataFrames
    all_targets_combined_forecast_df = pd.DataFrame()

    if prophet_covariates_forecast_results:
         forecast_dfs_list = []
         for res in prophet_covariates_forecast_results:
              if res['Status'] == 'Success' and not res['Forecast DataFrame'].empty:
                   forecast_df = res['Forecast DataFrame'].copy()
                   # Rename the 'yhat' column to the commodity name
                   forecast_df.rename(columns={'yhat': res['Commodity'],
                                              'yhat_lower': f"{res['Commodity']}_lower",
                                              'yhat_upper': f"{res['Commodity']}_upper"}, inplace=True)
                   # Set 'ds' as the index for merging
                   forecast_df.set_index('ds', inplace=True)
                   # Append the forecast DataFrame to the list
                   forecast_dfs_list.append(forecast_df)

         # Concatenate all individual forecast DataFrames
         if forecast_dfs_list:
             all_targets_combined_forecast_df = pd.concat(forecast_dfs_list, axis=1)
             all_targets_combined_forecast_df.index.name = 'Date' # Rename index to 'Date'

             print("\nCombined Prophet with Covariates Forecasts (Future Dates) DataFrame Head:")
             print(all_targets_combined_forecast_df.head())
             print("\nCombined Prophet with Covariates Forecasts (Future Dates) DataFrame Tail:")
             print(all_targets_combined_forecast_df.tail())
             print("\nCombined Prophet with Covariates Forecasts (Future Dates) DataFrame Shape:", all_targets_combined_forecast_df.shape)
         else:
             print("\nNo successful Prophet with Covariates forecasts generated to combine.")


    # Combine historical data with future forecasts for target commodities
    combined_targets_historical_forecast_df = pd.DataFrame()

    if 'prices_df' in locals() and not all_targets_combined_forecast_df.empty:
        print("\nCombining historical target data with Prophet with Covariates forecasts...")

        # Get historical data for target commodities that had successful forecasts
        successful_forecast_commodities = [res['Commodity'] for res in prophet_covariates_forecast_results if res['Status'] == 'Success' and not res['Forecast DataFrame'].empty]

        if successful_forecast_commodities:
             # Filter prices_df to include only the successfully forecasted target commodities
             # Ensure prices_df has 'Date' as index before filtering
             if isinstance(prices_df.index, pd.DatetimeIndex) and prices_df.index.name == 'Date':
                      historical_targets_df = prices_df[successful_forecast_commodities].dropna().copy()
                      historical_targets_df.index.name = 'Date' # Ensure consistent index name

                      # Combine historical and forecast data
                      combined_targets_historical_forecast_df = pd.concat([historical_targets_df, all_targets_combined_forecast_df], axis=0, join='outer')

                      # Sort by date
                      combined_targets_historical_forecast_df.sort_index(inplace=True)


                      print("\nCombined Targets Historical and Prophet with Covariates Forecasts DataFrame Head:")
                      print(combined_targets_historical_forecast_df.head())
                      print("\nCombined Targets Historical and Prophet with Covariates Forecasts DataFrame Tail:")
                      print(combined_targets_historical_forecast_df.tail())
                      print("\nCombined Targets Historical and Prophet with Covariates Forecasts DataFrame Shape:", combined_targets_historical_forecast_df.shape)
             else:
                  print("\nError: prices_df index is not a DatetimeIndex named 'Date'. Cannot combine historical data.")
        else:
              print("\nNo successful forecasts to combine with historical target data.")

    else:
        print("\nCannot combine historical target data with forecasts: Historical data ('prices_df') or combined forecasts ('all_targets_combined_forecast_df') are not available.")


# --- End: Prophet Forecasting with Covariates Module ---

# ==============================================================================
# Store and Visualise Prophet output
# ==============================================================================

import pandas as pd
import gspread
from google.colab import auth
from google.auth import default
import warnings

warnings.filterwarnings("ignore") # Suppress warnings

if 'combined_targets_historical_forecast_df' not in locals() or combined_targets_historical_forecast_df.empty:
    print (f"saving {combined_targets_historical_forecast_df} to gcs")
    gcs_prefix_fb_prophet_covariates_forecast = 'forecast_data/fb_prophet_covariates_forecast.csv'
    save_to_gcs(
        df=combined_targets_historical_forecast_df,
        gcs_prefix = gcs_prefix_fb_prophet_covariates_forecast,
        validate=False
        )
    print(f"saved {combined_targets_historical_forecast_df} to gcs")
else:
    print("combined_targets_historical_forecast_df is empty. Not saving.")

#--- Forecasts and History saved---

import matplotlib.pyplot as plt
import pandas as pd

# Assuming combined_targets_historical_forecast_df is available from previous steps

def plot_prophet_forecast(df_combined, commodity_name):
    """
    Visualizes the historical data and Prophet forecast with uncertainty intervals
    for a specific commodity from the combined DataFrame.

    Args:
        df_combined (pd.DataFrame): DataFrame containing combined historical and
                                    forecast data with 'Date' as index and
                                    columns for commodity, commodity_lower, and
                                    commodity_upper forecasts.
        commodity_name (str): The name of the target commodity to plot.
    """
    if commodity_name not in df_combined.columns:
        print(f"Error: Commodity '{commodity_name}' not found in the DataFrame.")
        return

    # Construct the names of the forecast columns
    forecast_col = commodity_name
    lower_col = f"{commodity_name}_lower"
    upper_col = f"{commodity_name}_upper"

    # Check if forecast columns exist
    if lower_col not in df_combined.columns or upper_col not in df_combined.columns:
        print(f"Warning: Uncertainty interval columns ('{lower_col}' or '{upper_col}') not found for '{commodity_name}'. Plotting forecast only.")
        plot_uncertainty = False
    else:
        plot_uncertainty = True

    plt.figure(figsize=(12, 3))

    # Plot historical data (non-NaN values in the forecast column)
    # We can infer historical points where the forecast column has NaN or is the same as the historical value
    # A simpler approach for plotting is to use the original historical data if available,
    # but since combined_targets_historical_forecast_df has both, we can plot all points and rely on NaN handling
    historical_dates = df_combined[df_combined[forecast_col].notna()].index # Use non-NaN in forecast column as a proxy for dates to plot
    plt.plot(historical_dates, df_combined.loc[historical_dates, forecast_col], label='Historical Data', color='blue')


    # Plot the entire forecast line (including historical period if present)
    plt.plot(df_combined.index, df_combined[forecast_col], label='Prophet Forecast', color='red', linestyle='--')

    # Plot uncertainty intervals if available
    if plot_uncertainty:
        plt.fill_between(df_combined.index, df_combined[lower_col], df_combined[upper_col], color='red', alpha=0.2, label='Forecast Uncertainty Interval (80%)')


    plt.title(f'Prophet Forecast for {commodity_name}')
    plt.xlabel('Date')
    plt.ylabel(commodity_name)
    plt.legend()
    plt.grid(True)
    plt.show()

# Example usage:
# Assuming combined_targets_historical_forecast_df is already loaded

# List of target commodities (assuming this list is available)
# If not available, you can get it from the columns of combined_targets_historical_forecast_df
# excluding the uncertainty interval columns
if 'target_commodities' in locals() and target_commodities:
    commodities_to_visualize = target_commodities[:5] # Visualize the first 5 target commodities
else:
    # Infer potential target commodities from the combined DataFrame if target_commodities list is not defined
    # Exclude columns ending with _lower or _upper and the index name
    if 'combined_targets_historical_forecast_df' in locals() and not combined_targets_historical_forecast_df.empty:
        all_cols = combined_targets_historical_forecast_df.columns.tolist()
        commodities_to_visualize = [col for col in all_cols if not col.endswith('_lower') and not col.endswith('_upper')][:5]
        print(f"Inferred commodities to visualize: {commodities_to_visualize}")
    else:
        print("Error: combined_targets_historical_forecast_df not found or is empty. Cannot visualize.")
        commodities_to_visualize = []


if 'combined_targets_historical_forecast_df' in locals() and not combined_targets_historical_forecast_df.empty and commodities_to_visualize:
    for commodity in commodities_to_visualize:
        plot_prophet_forecast(combined_targets_historical_forecast_df, commodity)
else:
    print("Cannot visualize forecasts: combined_targets_historical_forecast_df is not available or empty, or no commodities to visualize.")

"""## GOOGLE TIMESFM

> Ensure Python 3.11 (use runtime change in colab)


"""

!pip install --upgrade --quiet timesfm
!pip install --upgrade --quiet openpyxl
#!pip install --upgrade --quiet dask[dataframe] -U -q --user
!pip install dask[dataframe]==2024.12.1

# tuples of (import name, install name, min_version)
packages = [('timesfm', 'timesfm'),]

import importlib
install = False
for package in packages:
    if not importlib.util.find_spec(package[0]):
        print(f'installing package {package[1]}')
        install = True
        !pip install {package[1]} -U -q --user
    elif len(package) == 3:
        if importlib.metadata.version(package[0]) < package[2]:
            print(f'updating package {package[1]}')
            install = True
            !pip install {package[1]} -U -q --user

import os
os.environ['JAX_PLATFORMS'] = 'cpu'

import timesfm
import numpy as np
from google.cloud import bigquery
import pandas as pd
from matplotlib import pyplot as plt

from google.colab import userdata
from google.colab import drive
drive.mount("/content/drive", force_remount=True)

import timesfm

# For PyTorch
tfm = timesfm.TimesFm(
      hparams=timesfm.TimesFmHparams(
          backend="cpu",
          per_core_batch_size=32,
          horizon_len=24, # Increased forecast horizon
          input_patch_len=32,
          output_patch_len=128,
          num_layers=50,
          model_dims=1280,
          use_positional_embedding=False,
      ),
      checkpoint=timesfm.TimesFmCheckpoint(
          huggingface_repo_id="google/timesfm-2.0-500m-pytorch"),
  )

type(tfm._model)




## Google Colab Set Up
import gspread
from google.colab import auth, drive
from google.auth import default
import asyncio, aiohttp

# # Authenticate and create the gspread client
auth.authenticate_user()
creds, _ = default()

# INPUT
df = prices_df.copy() # Work on a copy to avoid modifying prices_df for other parts of the notebook
print(df.shape)
print(df.head(2))

# Ensure the 'date' column (string format) is handled if it still exists and is not the index.
# The prices_df now has 'Date' as index (datetime) and 'date' as a string column.
# For melting, we want to use the datetime index.
# Let's create a 'ds' column from the index and then melt.

melted_df = df.reset_index().rename(columns={'Date': 'ds'}) # Make 'Date' index a column named 'ds'
# Now drop the redundant 'date' string column if it exists, before melting.
if 'date' in melted_df.columns:
    melted_df = melted_df.drop(columns=['date'])

# Melt the DataFrame using 'ds' as the identifier
melted_df = pd.melt(melted_df, id_vars=['ds'], var_name='unique_id', value_name='y')

# Ensure 'ds' is timezone-naive as TimesFM expects this sometimes
melted_df['ds'] = pd.to_datetime(melted_df['ds']) # Ensure it's datetime
melted_df['ds'] = melted_df['ds'].dt.tz_localize(None) # Convert to timezone-naive if it somehow became localized

# Clean and convert 'y' column to numeric
melted_df['y'] = melted_df['y'].astype(str).str.replace(r'[^0-9.-]', '', regex=True) # Retained only digits, period, and hyphen
melted_df['y'] = pd.to_numeric(melted_df['y'], errors='coerce')  # 'coerce' handles errors by setting invalid values to NaN

# Drop rows where 'y' is NaN after conversion
melted_df = melted_df.dropna(subset=['y'])

print(melted_df.head())

forecast_df = tfm.forecast_on_df(
        inputs=melted_df,
        freq="w",  # Adjust frequency if your data isn't daily
        value_name='y',
        num_jobs=8  # Adjust for parallel processing if desired
    )

print (forecast_df.shape)
print (forecast_df.head())

import matplotlib.pyplot as plt

combined_df_list = []
# Iterate through the first 5 unique commodities
for commodity in melted_df['unique_id'].unique()[:5]:
    history = melted_df[melted_df['unique_id'] == commodity].set_index('ds')
    horizon = forecast_df[forecast_df['unique_id'] == commodity].set_index('ds')[:12] # Limit to the first 12 time periods

    plt.figure(figsize = (12,3))
    plt.plot(history['y'], linestyle = '-', color = 'blue')
    plt.plot(horizon['timesfm'], linestyle = '--', color = 'red')
    plt.fill_between(horizon.index, horizon['timesfm-q-0.4'], horizon['timesfm-q-0.6'], color = 'green', alpha = 1)
    plt.fill_between(horizon.index, horizon['timesfm-q-0.3'], horizon['timesfm-q-0.7'], color = 'green', alpha = 0.75)
    plt.fill_between(horizon.index, horizon['timesfm-q-0.2'], horizon['timesfm-q-0.8'], color = 'green', alpha = 0.5)
    plt.fill_between(horizon.index, horizon['timesfm-q-0.1'], horizon['timesfm-q-0.9'], color = 'green', alpha = 0.25)
    plt.title(commodity)
    plt.show()

forecast_wide_df = forecast_df.copy()
forecast_wide_df = forecast_wide_df.pivot(index='ds', columns='unique_id', values='timesfm')
forecast_wide_df = forecast_wide_df.rename_axis('AssessDate').reset_index()
forecast_wide_df = forecast_wide_df.rename(columns={'AssessDate': 'date'})

forecast_wide_df.head()
forecast_wide_df.to_csv('./forecasts.csv', index=False)

history_wide_df = melted_df.copy()

# Drop duplicates before pivoting
history_wide_df = history_wide_df.drop_duplicates(subset=['ds', 'unique_id'])
history_wide_df = history_wide_df.pivot(index='ds', columns='unique_id', values='y')
history_wide_df = history_wide_df.rename_axis('date').reset_index()

history_wide_df.head()
history_wide_df.to_csv('./history.csv', index=False)

timesfm_combined_df= pd.concat([history_wide_df, forecast_wide_df], axis=0)
print (timesfm_combined_df.shape)

if not timesfm_combined_df.empty:
    print(f"saving {timesfm_combined_df} to gcs")
    gcs_prefix_timesfm_combined = 'forecast_data/timesfm_combined.csv'
    save_to_gcs(
        df=timesfm_combined_df,
        gcs_prefix = gcs_prefix_timesfm_combined,
        validate=False
        )
    print(f"saved {timesfm_combined_df} to gcs")
else:
    print("timesfm_combined_df is empty. Not saving.")

"""## KAN FORECASTER
Forecast prices using KAN with "prices_df" as historical data.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install pykan

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler
from kan import KAN
import torch

# 1. Identify the numerical commodity columns
all_columns = prices_df.columns.tolist()
commodity_columns = [col for col in all_columns if pd.api.types.is_numeric_dtype(prices_df[col])]

print("Identified commodity columns for KAN data preparation:")
print(commodity_columns)

# Define sequence length (number of past time steps to use for prediction)
sequence_length = 20 # Example: use the past 30 days to predict the next day

# Initialize lists to store sequences and targets for all commodities
all_sequences = []
all_targets = []

# Initialize a dictionary to store scalers for each commodity
scalers = {}

# 2. Iterate through each identified commodity column
print(f"\nPreparing data sequences for {len(commodity_columns)} commodities with sequence length {sequence_length}...")
for commodity in commodity_columns:
    print(f"  Processing commodity: {commodity}")

    # 3. For each commodity, extract the time series data
    series = prices_df[commodity].dropna()

    if len(series) < sequence_length + 1:
        print(f"  Skipping {commodity}: Insufficient data ({len(series)} data points) for sequence length {sequence_length}.")
        continue # Skip to the next commodity

    # 4. Scale the time series data
    scaler = RobustScaler()
    scaled_series = scaler.fit_transform(series.values.reshape(-1, 1)).flatten()
    scalers[commodity] = scaler # Store the scaler for this commodity

    # 5. Create input sequences (X) and corresponding target values (y)
    sequences = []
    targets = []
    for i in range(len(scaled_series) - sequence_length):
        seq = scaled_series[i:i + sequence_length]
        target = scaled_series[i + sequence_length]
        sequences.append(seq)
        targets.append(target)

    # 6. Store the prepared input sequences and target values for the current commodity
    all_sequences.extend(sequences)
    all_targets.extend(targets)
    print(f"    Generated {len(sequences)} sequences for {commodity}.")


# 7. Combine the input sequences and target values into PyTorch tensors
if not all_sequences:
    print("\nNo sequences were generated. Cannot create PyTorch tensors.")
    # Initialize tensors as empty if no data is processed
    X_tensor = torch.empty(0, sequence_length, dtype=torch.float32)
    y_tensor = torch.empty(0, dtype=torch.float32)

else:
    X_tensor = torch.tensor(all_sequences, dtype=torch.float32)
    y_tensor = torch.tensor(all_targets, dtype=torch.float32)
    print("\nCombined sequences and targets into PyTorch tensors.")
    print("Shape of X_tensor:", X_tensor.shape)
    print("Shape of y_tensor:", y_tensor.shape)


# Store the scalers dictionary as it will be needed for inverse transformation of forecasts
# Assuming 'scalers' is already defined and populated
if 'scalers' in locals():
    print("\nScalers for each commodity stored in 'scalers' dictionary.")

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from kan import KAN # Assuming kan library is installed and imported in a previous cell

# Ensure X_tensor and y_tensor are available from the previous data preparation step
if 'X_tensor' not in locals() or 'y_tensor' not in locals() or X_tensor.shape[0] == 0:
    print("Error: X_tensor or y_tensor is not available or is empty. Cannot create Dataset and DataLoader.")
    # Initialize necessary variables to prevent errors in later steps
    train_loader = None
    model = None

else:
    print("X_tensor and y_tensor are available. Proceeding with Dataset, DataLoader, and Model definition.")

    # 1. Define a PyTorch Dataset class
    class TimeSeriesDataset(Dataset):
        def __init__(self, X, y):
            self.X = X
            self.y = y

        def __len__(self):
            return len(self.X)

        def __getitem__(self, idx):
            return self.X[idx], self.y[idx]

    # Instantiate the Dataset
    # Assuming X_tensor and y_tensor are already created from the data preparation step
    dataset = TimeSeriesDataset(X_tensor, y_tensor)
    print(f"\nDataset created with {len(dataset)} samples.")

    # 2. Create a PyTorch DataLoader
    batch_size = 32 # Define your desired batch size
    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
    print(f"DataLoader created with batch size {batch_size}.")
    print(f"Number of batches in DataLoader: {len(train_loader)}")


    # 3. Define the KAN model architecture
    # Assuming sequence_length is available from the data preparation step
    if 'sequence_length' not in locals():
        print("Error: 'sequence_length' is not defined. Cannot define KAN model architecture.")
        model = None # Ensure model is None if sequence_length is missing
    else:
        print(f"\nDefining KAN model architecture with input dimension {sequence_length}...")
        # The KAN model takes a flattened input, so the input dimension is sequence_length
        # The output dimension is 1 for predicting a single next value
        # Added an extra layer with 8 nodes
        model = KAN(width=[sequence_length, 16, 8, 1]) # Example architecture: input -> 16 KAN nodes -> 8 KAN nodes -> 1 output

        print("KAN model architecture defined.")

        # 4. Instantiate the defined KAN model
        # model is already instantiated by the kan.KAN() call above
        print("\nKAN model instantiated.")
        print("Model structure:")
        print(model)

import torch.optim as optim
import torch.nn as nn

# Ensure model and train_loader are available from the previous step
if 'model' not in locals() or model is None:
    print("Error: KAN model is not defined. Cannot proceed with training.")
    # Finish the task with failure if the model is not defined
    # No need to initialize other variables as training won't start

elif 'train_loader' not in locals() or train_loader is None:
    print("Error: train_loader is not defined. Cannot proceed with training.")
    # Finish the task with failure if the train_loader is not defined
    # No need to initialize other variables as training won't start

else:
    print("\nProceeding with KAN model training.")

    # 1. Define the loss function (e.g., Mean Squared Error)
    criterion = nn.MSELoss()
    print("Loss function defined (MSELoss).")

    # 2. Define the optimizer (e.g., Adam)
    # Updated learning rate as requested by the user
    optimizer = optim.Adam(model.parameters(), lr=0.00002) # Define your desired learning rate
    print("Optimizer defined (Adam with learning rate 0.00002).")

    # 3. Set the number of training epochs
    num_epochs = 10 # Define your desired number of epochs
    print(f"Number of training epochs set to: {num_epochs}")

    # 4. Implement the training loop
    print("\nStarting KAN model training...")

    # Move model to the appropriate device if available (CPU or GPU)
    # Check if CUDA is available and use GPU if it is, otherwise use CPU
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    print(f"Using device: {device}")


    for epoch in range(num_epochs):
        model.train() # Set the model to training mode
        running_loss = 0.0

        for i, data in enumerate(train_loader):
            inputs, targets = data

            # Move data to the device
            inputs, targets = inputs.to(device), targets.to(device)

            # Zero the gradients
            optimizer.zero_grad()

            # Forward pass
            outputs = model(inputs)

            # Calculate the loss
            # Ensure targets have the same shape as outputs (usually [batch_size, 1])
            # If targets is [batch_size], unsqueeze it to [batch_size, 1]
            if targets.ndim == 1:
                 targets = targets.unsqueeze(1)

            loss = criterion(outputs, targets)

            # Backward pass
            loss.backward()

            # Update the weights
            optimizer.step()

            # Accumulate the loss
            running_loss += loss.item()

        # Print the loss periodically
        print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}")

    print("\nKAN model training complete.")

import torch
import os

# Define the path to save the model on Google Drive
# Ensure you have mounted Google Drive in a previous cell (e.g., cell Nv0Vvz_e5Ma2)
model_save_path = '/content/drive/MyDrive/Colab Notebooks/quant_kan_model.pth' # You can change the filename

# Ensure the directory exists
os.makedirs(os.path.dirname(model_save_path), exist_ok=True)

# Ensure the model is available from the training step
if 'model' not in locals() or model is None:
    print("Error: KAN model is not defined. Cannot save the model.")
else:
    try:
        # Save the model's state dictionary
        torch.save(model.state_dict(), model_save_path)
        print(f"KAN model state dictionary saved successfully to: {model_save_path}")
    except Exception as e:
        print(f"Error saving KAN model to Google Drive: {e}")

import torch
import pandas as pd
import numpy as np

# Ensure prices_df, commodity_columns, sequence_length, and scalers are available
if 'prices_df' not in locals():
    print("Error: 'prices_df' DataFrame not found. Cannot prepare forecast input sequences.")
    forecast_input_sequences = {} # Initialize as empty to prevent errors
elif not isinstance(prices_df.index, pd.DatetimeIndex) or prices_df.index.name != 'Date':
    print("Error: 'prices_df' index is not a DatetimeIndex named 'Date'. Cannot proceed.")
    forecast_input_sequences = {} # Initialize as empty to prevent errors
elif 'commodity_columns' not in locals() or not commodity_columns:
    print("Error: 'commodity_columns' list not found or is empty. Cannot prepare forecast input sequences.")
    forecast_input_sequences = {} # Initialize as empty to prevent errors
elif 'sequence_length' not in locals():
    print("Error: 'sequence_length' is not defined. Cannot prepare forecast input sequences.")
    forecast_input_sequences = {} # Initialize as empty to prevent errors
elif 'scalers' not in locals() or not scalers:
    print("Error: 'scalers' dictionary not found or is empty. Cannot scale forecast input sequences.")
    forecast_input_sequences = {} # Initialize as empty to prevent errors
else:
    print("\nPreparing forecast input sequences for KAN model...")

    # Initialize an empty dictionary to store the last sequence for each commodity
    forecast_input_sequences = {}

    # Iterate through each identified commodity column
    print(f"Extracting and scaling the last {sequence_length} data points for each commodity...")
    for commodity in commodity_columns:
        # Extract the time series data for the current commodity and drop missing values
        series = prices_df[commodity].dropna()

        # Check if the series has enough data points
        if len(series) < sequence_length:
            print(f"  Warning: Skipping {commodity}. Insufficient data ({len(series)} data points) for sequence length {sequence_length}.")
            continue # Skip to the next commodity

        # Get the last `sequence_length` data points
        last_sequence = series[-sequence_length:]

        # Scale the last sequence using the corresponding scaler
        if commodity in scalers:
            scaler = scalers[commodity]
            # Reshape the sequence to be a 2D array for scaling (MinMaxScaler expects 2D input)
            scaled_sequence = scaler.transform(last_sequence.values.reshape(-1, 1)).flatten()

            # Convert the scaled sequence to a PyTorch tensor
            # Reshape to [1, sequence_length] as the model expects a batch of sequences
            sequence_tensor = torch.tensor(scaled_sequence, dtype=torch.float32).unsqueeze(0)

            # Store the tensor in the dictionary
            forecast_input_sequences[commodity] = sequence_tensor
            print(f"  Prepared input sequence for {commodity}. Shape: {sequence_tensor.shape}")

        else:
             print(f"  Warning: Scaler not found for {commodity}. Cannot prepare forecast input sequence.")


    # Print the number of commodities for which input sequences were prepared
    print(f"\nPrepared input sequences for {len(forecast_input_sequences)} commodities.")

import torch
import pandas as pd
import numpy as np
from kan import KAN # Assuming kan library is installed and imported

# Ensure model, forecast_input_sequences, sequence_length, scalers, and commodity_columns are available
if 'model' not in locals() or model is None:
    print("Error: KAN model is not defined. Cannot generate forecasts.")
    kan_forecasts = {} # Initialize as empty
elif 'forecast_input_sequences' not in locals() or not forecast_input_sequences:
    print("Error: 'forecast_input_sequences' is not available or is empty. Cannot generate forecasts.")
    kan_forecasts = {} # Initialize as empty
elif 'sequence_length' not in locals():
    print("Error: 'sequence_length' is not defined. Cannot generate forecasts.")
    kan_forecasts = {} # Initialize as empty
elif 'scalers' not in locals() or not scalers:
    print("Error: 'scalers' dictionary not found or is empty. Cannot inverse scale forecasts.")
    kan_forecasts = {} # Initialize as empty
elif 'commodity_columns' not in locals() or not commodity_columns:
    print("Error: 'commodity_columns' list not found or is empty. Cannot iterate through commodities for forecasting.")
    kan_forecasts = {} # Initialize as empty
else:
    print("\nGenerating KAN future forecasts for each commodity...")

    # Define the number of steps to forecast
    forecast_steps = 24 # This should align with other forecasters if possible
    print(f"Forecasting {forecast_steps} steps into the future.")

    # Move the model to the appropriate device (CPU or GPU) if available
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    model.eval() # Set the model to evaluation mode

    # Initialize a dictionary to store the generated forecasts for each commodity
    kan_forecasts = {}

    # Iterate through each commodity for which an input sequence was prepared
    print(f"\nGenerating forecasts for {len(forecast_input_sequences)} commodities...")
    with torch.no_grad(): # Disable gradient calculation during inference
        for commodity in forecast_input_sequences.keys():
            print(f"  Forecasting for: {commodity}")
            # Get the initial input sequence tensor for the current commodity
            current_sequence = forecast_input_sequences[commodity].to(device) # Move sequence to device

            # Get the corresponding scaler for inverse scaling
            if commodity not in scalers:
                 print(f"    Warning: Scaler not found for {commodity}. Skipping forecast.")
                 continue # Skip to the next commodity
            scaler = scalers[commodity]

            # Initialize a list to store the unscaled forecast values for this commodity
            commodity_forecast_values = []

            # Generate the forecast step by step
            for _ in range(forecast_steps):
                # Get the KAN model's prediction for the next step (scaled value)
                predicted_scaled_value = model(current_sequence).item() # Get the scalar value

                # Inverse scale the predicted value to the original price scale
                # The scaler expects a 2D array, so reshape the scalar
                predicted_original_value = scaler.inverse_transform([[predicted_scaled_value]])[0][0]

                # Store the inverse-scaled forecast value
                commodity_forecast_values.append(predicted_original_value)

                # Update the input sequence for the next prediction
                # Remove the oldest value and append the new scaled prediction
                new_sequence = torch.cat((current_sequence[:, 1:], torch.tensor([[predicted_scaled_value]], dtype=torch.float32).to(device)), dim=1)
                current_sequence = new_sequence # Use the new sequence for the next step

            # Store the generated forecast values for the current commodity
            kan_forecasts[commodity] = commodity_forecast_values
            print(f"    Generated {len(commodity_forecast_values)} forecast values for {commodity}.")

    print("\nKAN forecasting complete.")

# You now have the generated forecasts in the 'kan_forecasts' dictionary.

"""**Reasoning**:
Combine the generated KAN forecasts with the historical data and save the combined DataFrame to a Google Sheet.


"""

import pandas as pd
import gspread
from google.colab import auth
from google.auth import default
import warnings

warnings.filterwarnings("ignore") # Suppress warnings

# Ensure prices_df (original data) and kan_forecasts (forecast dictionary) are available
if 'prices_df' not in locals():
    print("Error: 'prices_df' DataFrame not found. Cannot combine data.")
    # Finish the task with failure
elif 'kan_forecasts' not in locals() or not kan_forecasts:
    print("Error: 'kan_forecasts' dictionary not found or is empty. Cannot combine data.")
    # Finish the task with failure
else:
    print("\nCombining original data and KAN forecasts...")

    # Prepare the original data: select commodity columns and ensure DatetimeIndex
    # Assuming the index of prices_df is already the correct DatetimeIndex ('Date')
    # and numeric columns are identified in commodity_columns list
    historical_df = prices_df[commodity_columns].copy()

    # Prepare the forecast data:
    # Create a DataFrame from the kan_forecasts dictionary
    if kan_forecasts:
        # Get the last date from the historical data to start the forecast index from the next day
        last_historical_date = prices_df.index.max()

        # Determine the frequency of the historical data
        freq = pd.infer_freq(prices_df.index)
        if freq is None:
             # Fallback to 'D' if frequency cannot be inferred (assuming daily data)
             freq = 'D'
             print(f"Warning: Could not infer frequency from historical data. Assuming '{freq}'.")
        else:
            print(f"Inferred historical data frequency: {freq}")


        # Generate future dates starting from the day after the last historical date
        forecast_dates = pd.date_range(start=last_historical_date, periods=forecast_steps + 1, freq=freq)[1:]

        # Create a DataFrame from the forecast dictionary, using the generated future dates as the index
        forecast_df = pd.DataFrame(kan_forecasts, index=forecast_dates)

        # Rename the index to 'Date' for consistency
        forecast_df.index.name = 'Date'

    else:
        print("No KAN forecasts were generated to combine.")
        forecast_df = pd.DataFrame() # Create an empty DataFrame if no forecasts

    # Combine the historical data and the forecast data
    # Use outer join to include all dates from both historical and forecast periods
    # The index from both DataFrames (DatetimeIndex) will be used for alignment
    combined_df = pd.concat([historical_df, forecast_df], axis=0, join='outer')

    # Sort by date to ensure chronological order
    combined_df.sort_index(inplace=True)

    print("\nCombined KAN DataFrame Head:")
    print(combined_df.head(2))
    print("\nCombined KAN DataFrame Tail:")
    print(combined_df.tail(2))
    print("\nCombined KAN DataFrame Shape:", combined_df.shape)

    if not combined_df.empty:
        print(f"saving {combined_df} to gcs")
        gcs_prefix_kan_forecast = 'forecast_data/kan_forecasts.csv'
        save_to_gcs(
            df=combined_df,
            gcs_prefix=gcs_prefix_kan_forecast,
            validate=False)
        print(f"saved {combined_df} to gcs")
    else:
        print("combined_df is empty. No data to save.")

import matplotlib.pyplot as plt
import pandas as pd

# Assuming combined_df is available from the previous step

def plot_kan_forecast(df_combined, commodity_name):
    """
    Visualizes the historical data and KAN forecast for a specific commodity
    from the combined DataFrame.

    Args:
        df_combined (pd.DataFrame): DataFrame containing combined historical and
                                    forecast data with 'Date' as index.
        commodity_name (str): The name of the commodity to plot.
    """
    if commodity_name not in df_combined.columns:
        print(f"Error: Commodity '{commodity_name}' not found in the DataFrame.")
        return

    plt.figure(figsize=(12, 3))

    # Plot historical data (non-NaN values in the historical period)
    # Assuming historical data ends before the forecast period starts
    # Find the index where the forecast starts (first non-NaN value in the forecast period)
    # Or simply plot all available data in the combined df
    plt.plot(df_combined.index, df_combined[commodity_name], label=f'{commodity_name} Data & KAN Forecast')


    # Add a vertical line at the end of the historical data for visual separation
    # Assuming the end of historical data is the last date before the forecast starts
    # We can infer this from the first non-NaN value in the forecast period in the combined df
    first_forecast_date = df_combined[df_combined[commodity_name].index > prices_df.index.max()].first_valid_index()
    if first_forecast_date:
        plt.axvline(prices_df.index.max(), color='red', linestyle='--', label='Forecast Start')


    plt.title(f'KAN Forecast for {commodity_name}')
    plt.xlabel('Date')
    plt.ylabel(commodity_name)
    plt.legend()
    plt.grid(True)
    plt.show()

# Example usage:
# Assuming combined_df is already loaded

# Get a list of commodities that were forecasted
# We can get this from the keys of the kan_forecasts dictionary
if 'kan_forecasts' in locals() and kan_forecasts:
    commodities_to_visualize = list(kan_forecasts.keys())[:10] # Visualize the first 5 forecasted commodities
    print(f"\nVisualizing KAN forecasts for the following commodities: {commodities_to_visualize}")
else:
    print("Error: 'kan_forecasts' dictionary not found or is empty. Cannot visualize forecasts.")
    commodities_to_visualize = []


if 'combined_df' in locals() and not combined_df.empty and commodities_to_visualize:
    for commodity in commodities_to_visualize:
        plot_kan_forecast(combined_df, commodity)
else:
    print("Cannot visualize forecasts: combined_df is not available or empty, or no commodities to visualize.")

"""## GRU FORECASTER
Build a GRU-based time series forecaster using the data in the `prices_df` DataFrame, generate forecasts, and save the results to a Google Sheet.
"""

import torch
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler

# Ensure prices_df is available
if 'prices_df' not in locals():
    print("Error: 'prices_df' DataFrame not found. Cannot prepare GRU data.")
    # Initialize tensors and scalers as empty if data is not available
    X_tensor_gru = torch.empty(0, dtype=torch.float32)
    y_tensor_gru = torch.empty(0, dtype=torch.float32)
    scalers_gru = {}

else:
    # 1. Identify the numerical commodity columns
    all_columns = prices_df.columns.tolist()
    # Exclude non-numeric and potentially non-relevant columns like 'date' if it wasn't dropped
    cols_to_exclude = ['date', 'day_of_week', 'month', 'year',
                       'day_of_week_sin', 'day_of_week_cos',
                       'month_sin', 'month_cos',
                       'year_sin', 'year_cos']
    commodity_columns = [col for col in all_columns if col not in cols_to_exclude and pd.api.types.is_numeric_dtype(prices_df[col])]

    print("Identified commodity columns for GRU data preparation:")
    print(commodity_columns)

    # 2. Define sequence length (number of past time steps to use for prediction)
    sequence_length = 30 # Example: use the past 30 days to predict the next day

    # 3. Initialize lists to store sequences and targets for all commodities
    all_sequences = []
    all_targets = []

    # 4. Initialize a dictionary to store scalers for each commodity
    scalers_gru = {}

    # 5. Iterate through each identified commodity column
    print(f"\nPreparing data sequences for {len(commodity_columns)} commodities with sequence length {sequence_length}...")
    for commodity in commodity_columns:
        print(f"  Processing commodity: {commodity}")

        # Extract the time series data for the current commodity and drop missing values
        series = prices_df[commodity].dropna()

        if len(series) < sequence_length + 1:
            print(f"  Skipping {commodity}: Insufficient data ({len(series)} data points) for sequence length {sequence_length}.")
            continue # Skip to the next commodity

        # Scale the time series data
        # Using MinMaxScaler as an example, can be changed to StandardScaler or RobustScaler
        scaler = MinMaxScaler()
        scaled_series = scaler.fit_transform(series.values.reshape(-1, 1)).flatten()
        scalers_gru[commodity] = scaler # Store the scaler for this commodity

        # Create input sequences (X) and corresponding target values (y)
        sequences = []
        targets = []
        for i in range(len(scaled_series) - sequence_length):
            seq = scaled_series[i:i + sequence_length]
            target = scaled_series[i + sequence_length]
            sequences.append(seq)
            targets.append(target)

        # Store the prepared input sequences and target values for the current commodity
        all_sequences.extend(sequences)
        all_targets.extend(targets)
        print(f"    Generated {len(sequences)} sequences for {commodity}.")


    # 6. Combine the input sequences and target values into PyTorch tensors
    if not all_sequences:
        print("\nNo sequences were generated. Cannot create PyTorch tensors.")
        X_tensor_gru = torch.empty(0, dtype=torch.float32) # Initialize as empty tensor
        y_tensor_gru = torch.empty(0, dtype=torch.float32) # Initialize as empty tensor
        # scalers_gru is already initialized as an empty dictionary

    else:
        X_tensor_gru = torch.tensor(all_sequences, dtype=torch.float32)
        y_tensor_gru = torch.tensor(all_targets, dtype=torch.float32)
        print("\nCombined sequences and targets into PyTorch tensors.")
        print("Shape of X_tensor_gru:", X_tensor_gru.shape)
        print("Shape of y_tensor_gru:", y_tensor_gru.shape)

    # 7. Confirm that the scalers dictionary is stored
    if 'scalers_gru' in locals():
        print("\nScalers for each commodity stored in 'scalers_gru' dictionary.")

import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

# Ensure X_tensor_gru and y_tensor_gru are available from the previous data preparation step
if 'X_tensor_gru' not in locals() or 'y_tensor_gru' not in locals() or X_tensor_gru.shape[0] == 0:
    print("Error: X_tensor_gru or y_tensor_gru is not available or is empty. Cannot create Dataset and DataLoader.")
    # Initialize necessary variables to prevent errors in later steps
    train_loader_gru = None
    model_gru = None

else:
    print("X_tensor_gru and y_tensor_gru are available. Proceeding with Dataset, DataLoader, and Model definition.")

    # 1. Define a PyTorch Dataset class for GRU
    class TimeSeriesDatasetGRU(Dataset):
        def __init__(self, X, y):
            self.X = X
            self.y = y

        def __len__(self):
            return len(self.X)

        def __getitem__(self, idx):
            # GRU typically expects input shape (batch_size, sequence_length, input_dim)
            # Since we are forecasting a single time series value at a time, input_dim is 1
            # We need to unsqueeze the sequence tensor to add the input_dim dimension
            return self.X[idx].unsqueeze(-1), self.y[idx] # Add last dimension for input_dim

    # Instantiate the Dataset
    dataset_gru = TimeSeriesDatasetGRU(X_tensor_gru, y_tensor_gru)
    print(f"\nDataset for GRU created with {len(dataset_gru)} samples.")

    # 2. Create a PyTorch DataLoader
    batch_size = 64 # Define your desired batch size
    train_loader_gru = DataLoader(dataset_gru, batch_size=batch_size, shuffle=True)
    print(f"DataLoader for GRU created with batch size {batch_size}.")
    print(f"Number of batches in DataLoader: {len(train_loader_gru)}")


    # 3. Define the GRU model architecture
    # Assuming sequence_length is available from the data preparation step
    if 'sequence_length' not in locals():
        print("Error: 'sequence_length' is not defined. Cannot define GRU model architecture.")
        model_gru = None # Ensure model_gru is None if sequence_length is missing
    else:
        print(f"\nDefining GRU model architecture with sequence length {sequence_length}...")
        # GRU model parameters - these are example values and may need tuning
        input_dim = 1       # We are forecasting a single time series value
        hidden_dim = 128    # Number of features in the hidden state
        layer_dim = 3       # Number of recurrent layers
        output_dim = 1      # We are predicting a single next value

        class GRUModel(nn.Module):
            def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):
                super().__init__()
                self.hidden_dim = hidden_dim
                self.layer_dim = layer_dim

                # GRU layers
                self.gru = nn.GRU(
                    input_dim, hidden_dim, layer_dim, batch_first=True # batch_first=True means input is (batch_size, sequence_length, input_dim)
                )

                # Fully connected layer for output
                self.fc = nn.Linear(hidden_dim, output_dim)

            def forward(self, x):
                # x shape: (batch_size, sequence_length, input_dim)

                # Initialize hidden state with zeros
                h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(x.device)

                # We don't need to return the hidden state for this forecasting task, only the output
                # out shape: (batch_size, sequence_length, hidden_dim)
                # hn shape: (layer_dim, batch_size, hidden_dim)
                out, hn = self.gru(x, h0.detach())

                # We only need the output from the last time step to predict the next value
                last_step_out = out[:, -1, :] # Shape: (batch_size, hidden_dim)

                # Pass the output of the last time step through the fully connected layer
                prediction = self.fc(last_step_out) # Shape: (batch_size, output_dim)

                return prediction

        model_gru = GRUModel(input_dim, hidden_dim, layer_dim, output_dim)

        print("GRU model architecture defined.")

        # 4. Instantiate the defined GRU model
        # model_gru is already instantiated by the GRUModel() call above
        print("\nGRU model instantiated.")
        print("Model structure:")
        print(model_gru)

import torch.optim as optim
import torch.nn as nn

# Ensure model_gru and train_loader_gru are available from the previous step
if 'model_gru' not in locals() or model_gru is None:
    print("Error: GRU model is not defined. Cannot proceed with training.")
    # Finish the task with failure if the model is not defined
    # No need to initialize other variables as training won't start

elif 'train_loader_gru' not in locals() or train_loader_gru is None:
    print("Error: train_loader_gru is not defined. Cannot proceed with training.")
    # Finish the task with failure if the train_loader_gru is not defined
    # No need to initialize other variables as training won't start

else:
    print("\nProceeding with GRU model training.")

    # 1. Define the loss function (e.g., Mean Squared Error)
    criterion = nn.MSELoss()
    print("Loss function defined (MSELoss).")

    # 2. Define the optimizer (e.g., Adam)
    # Define your desired learning rate - MAMBA might benefit from different rates
    learning_rate = 0.00001
    optimizer = optim.Adam(model_gru.parameters(), lr=learning_rate)
    print(f"Optimizer defined (Adam with learning rate {learning_rate}).")

    # 3. Set the number of training epochs
    num_epochs = 10 # Define your desired number of epochs - might need more or fewer
    print(f"Number of training epochs set to: {num_epochs}")

    # 4. Implement the training loop
    print("\nStarting GRU model training...")

    # Move model to the appropriate device if available (CPU or GPU)
    # Check if CUDA is available and use GPU if it is, otherwise use CPU
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model_gru.to(device)
    print(f"Using device: {device}")


    for epoch in range(num_epochs):
        model_gru.train() # Set the model to training mode
        running_loss = 0.0

        for i, data in enumerate(train_loader_gru):
            inputs, targets = data

            # Move data to the device
            inputs, targets = inputs.to(device), targets.to(device)

            # Zero the gradients
            optimizer.zero_grad()

            # Forward pass
            outputs = model_gru(inputs)

            # Calculate the loss
            # Ensure targets have the same shape as outputs (usually [batch_size, 1])
            # If targets is [batch_size], unsqueeze it to [batch_size, 1]
            if targets.ndim == 1:
                 targets = targets.unsqueeze(1)

            loss = criterion(outputs, targets)

            # Backward pass
            loss.backward()

            # Update the weights
            optimizer.step()

            # Accumulate the loss
            running_loss += loss.item()

        # Print the loss periodically
        print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader_gru):.4f}")

    print("\nGRU model training complete.")

import torch
import os

# Define the path to save the model (e.g., in Google Drive)
# Ensure you have mounted Google Drive in a previous cell if saving there
gru_model_save_path = '/content/drive/MyDrive/Colab Notebooks/quant_gru_model.pth' # You can change the filename and path

# Ensure the directory exists
os.makedirs(os.path.dirname(gru_model_save_path), exist_ok=True)

# Ensure the model is available from the training step
if 'model_gru' not in locals() or model_gru is None:
    print("Error: GRU model is not defined. Cannot save the model.")
else:
    try:
        # Save the model's state dictionary
        torch.save(model_gru.state_dict(), gru_model_save_path)
        print(f"GRU model state dictionary saved successfully to: {gru_model_save_path}")
    except Exception as e:
        print(f"Error saving GRU model: {e}")

import torch
import pandas as pd
import numpy as np
import os

# Ensure GRU model architecture is defined (assuming it was defined in a previous cell like kacL81qmSp_t)
# Ensure prices_df, commodity_columns, sequence_length, and scalers_gru are available

# Define the path to the saved model
gru_model_save_path = '/content/drive/MyDrive/Colab Notebooks/quant_gru_model.pth' # This should match the save path

# Check if the saved model file exists
if not os.path.exists(gru_model_save_path):
    print(f"Error: Saved GRU model not found at {gru_model_save_path}. Cannot generate forecasts using the saved model.")
    gru_forecasts = {} # Initialize as empty
elif 'GRUModel' not in locals():
    print("Error: GRUModel architecture is not defined. Cannot load the saved model.")
    gru_forecasts = {} # Initialize as empty
elif 'prices_df' not in locals():
    print("Error: 'prices_df' DataFrame not found. Cannot prepare forecast input sequences.")
    gru_forecasts = {} # Initialize as empty
elif not isinstance(prices_df.index, pd.DatetimeIndex) or prices_df.index.name != 'Date':
    print("Error: 'prices_df' index is not a DatetimeIndex named 'Date'. Cannot proceed.")
    gru_forecasts = {} # Initialize as empty
elif 'commodity_columns' not in locals() or not commodity_columns:
    print("Error: 'commodity_columns' list not found or is empty. Cannot prepare forecast input sequences.")
    gru_forecasts = {} # Initialize as empty
elif 'sequence_length' not in locals():
    print("Error: 'sequence_length' is not defined. Cannot generate forecasts.")
    gru_forecasts = {} # Initialize as empty
elif 'scalers_gru' not in locals() or not scalers_gru:
    print("Error: 'scalers_gru' dictionary not found or is empty. Cannot inverse scale forecasts.")
    gru_forecasts = {} # Initialize as empty

else:
    print("\nLoading the saved GRU model and generating future forecasts...")

    # Instantiate the GRU model architecture
    # Ensure input_dim, hidden_dim, layer_dim, output_dim are defined as in the model definition cell (kacL81qmSp_t)
    # Assuming these variables are available in the environment from previous cell executions
    try:
        # Re-instantiate the model with the same architecture parameters used for training
        # These parameters should be available from the model definition cell (kacL81qmSp_t)
        # If not explicitly defined globally, you might need to hardcode them or ensure that cell is run first.
        # Assuming input_dim=1, hidden_dim=128, layer_dim=3, output_dim=1 based on previous successful run
        input_dim = 1
        hidden_dim = 128
        layer_dim = 3
        output_dim = 1

        model_gru = GRUModel(input_dim, hidden_dim, layer_dim, output_dim)
        print("GRU model architecture instantiated for loading.")

        # Load the saved state dictionary
        model_gru.load_state_dict(torch.load(gru_model_save_path))
        print(f"GRU model state dictionary loaded from {gru_model_save_path}.")

        # Move the model to the appropriate device (CPU or GPU) if available
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model_gru.to(device)
        model_gru.eval() # Set the model to evaluation mode
        print(f"Using device for forecasting: {device}")


    except Exception as e:
        print(f"Error loading GRU model or setting up for forecasting: {e}")
        gru_forecasts = {} # Initialize as empty
        model_gru = None # Ensure model_gru is None if loading fails


    # Initialize a dictionary to store the generated forecasts for each commodity
    gru_forecasts = {}

    if model_gru is not None:
         # Define the number of steps to forecast
         forecast_steps = 60 # This should align with other forecasters if possible
         print(f"\nGenerating {forecast_steps} steps into the future for each commodity...")

         # Iterate through each commodity for which an input sequence can be prepared
         print(f"Generating forecasts for {len(commodity_columns)} commodities...")
         with torch.no_grad(): # Disable gradient calculation during inference
             for commodity in commodity_columns:
                 print(f"  Forecasting for: {commodity}")

                 # Get the time series for the current commodity and drop missing values
                 series = prices_df[commodity].dropna()

                 # Check if the series has enough data points for the initial sequence
                 if len(series) < sequence_length:
                     print(f"    Warning: Skipping {commodity}. Insufficient data ({len(series)} data points) for sequence length {sequence_length}.")
                     continue # Skip to the next commodity

                 # Get the last `sequence_length` data points as the initial input sequence
                 last_sequence = series[-sequence_length:]

                 # Get the corresponding scaler for this commodity
                 if commodity not in scalers_gru:
                      print(f"    Warning: Scaler not found for {commodity}. Skipping forecast.")
                      continue # Skip to the next commodity
                 scaler = scalers_gru[commodity]

                 # Scale the last sequence using the corresponding scaler
                 # Reshape the sequence to be a 2D array for scaling
                 scaled_sequence = scaler.transform(last_sequence.values.reshape(-1, 1)).flatten()

                 # Convert the scaled sequence to a PyTorch tensor
                 # Reshape to [1, sequence_length, input_dim] as the model expects
                 sequence_tensor = torch.tensor(scaled_sequence, dtype=torch.float32).unsqueeze(0).unsqueeze(-1).to(device) # Add batch and input_dim dimensions and move to device

                 # Initialize a list to store the unscaled forecast values for this commodity
                 commodity_forecast_values = []

                 # Generate the forecast step by step
                 for _ in range(forecast_steps):
                     # Get the GRU model's prediction for the next step (scaled value)
                     predicted_scaled_value = model_gru(sequence_tensor).item() # Get the scalar value

                     # Inverse scale the predicted value to the original price scale
                     # The scaler expects a 2D array, so reshape the scalar
                     predicted_original_value = scaler.inverse_transform([[predicted_scaled_value]])[0][0]

                     # Store the inverse-scaled forecast value
                     commodity_forecast_values.append(predicted_original_value)

                     # Update the input sequence for the next prediction
                     # Remove the oldest value and append the new scaled prediction
                     # The new prediction needs to be reshaped to match the input tensor shape
                     new_sequence_tensor = torch.cat((sequence_tensor[:, 1:, :], torch.tensor([[predicted_scaled_value]], dtype=torch.float32).unsqueeze(0).to(device)), dim=1)
                     sequence_tensor = new_sequence_tensor # Use the new sequence for the next step

                 # Store the generated forecast values for the current commodity
                 gru_forecasts[commodity] = commodity_forecast_values
                 print(f"    Generated {len(commodity_forecast_values)} forecast values for {commodity}.")

         print("\nGRU forecasting complete.")

    else:
        print("GRU model is not available. Cannot generate forecasts.")


# You now have the generated forecasts in the 'gru_forecasts' dictionary.

import pandas as pd
import numpy as np
import gspread
from google.colab import auth
from google.auth import default
import warnings

warnings.filterwarnings("ignore") # Suppress warnings during data manipulation and saving

# Ensure prices_df (original data) and gru_forecasts (forecast dictionary) are available
if 'prices_df' not in locals():
    print("Error: 'prices_df' DataFrame not found. Cannot combine data.")
    # Finish the task with failure
elif 'gru_forecasts' not in locals() or not gru_forecasts:
    print("Error: 'gru_forecasts' dictionary not found or is empty. Cannot combine data.")
    # Finish the task with failure
else:
    print("\nCombining original data and GRU forecasts...")

    # Prepare the original data: select commodity columns and ensure DatetimeIndex
    # Assuming the index of prices_df is already the correct DatetimeIndex ('Date')
    # and numeric columns are identified in commodity_columns list
    # Ensure commodity_columns is available; if not, try to infer from prices_df
    if 'commodity_columns' not in locals() or not commodity_columns:
         print("Warning: 'commodity_columns' not found. Attempting to infer from prices_df.")
         all_cols = prices_df.columns.tolist()
         cols_to_exclude = ['date', 'day_of_week', 'month', 'year',
                            'day_of_week_sin', 'day_of_week_cos',
                            'month_sin', 'month_cos',
                            'year_sin', 'year_cos']
         commodity_columns = [col for col in all_cols if col not in cols_to_exclude and pd.api.types.is_numeric_dtype(prices_df[col])]
         if not commodity_columns:
             print("Error: Could not identify numeric commodity columns from prices_df.")
             # Finish the task with failure
             combined_gru_df = pd.DataFrame() # Initialize as empty
         else:
             print(f"Inferred commodity columns: {commodity_columns}")
             historical_df = prices_df[commodity_columns].copy()
    # Corrected indentation for this else block
    else:
         historical_df = prices_df[commodity_columns].copy()

    # Ensure historical_df has the correct DatetimeIndex ('Date')
    historical_df.index.name = 'Date'


    # Prepare the forecast data:
    # Create a DataFrame from the gru_forecasts dictionary
    if gru_forecasts:
        # Get the last date from the historical data to start the forecast index from the next day
        last_historical_date = prices_df.index.max()

        # Determine the frequency of the historical data
        # Ensure prices_df index is DatetimeIndex before inferring frequency
        if isinstance(prices_df.index, pd.DatetimeIndex):
            freq = pd.infer_freq(prices_df.index)
            if freq is None:
                 # Fallback to 'D' if frequency cannot be inferred (assuming daily data)
                 freq = 'D'
                 print(f"Warning: Could not infer frequency from historical data. Assuming '{freq}'.")
            else:
                print(f"Inferred historical data frequency: {freq}")

            # Define the number of steps to forecast (from the gru_forecasts dictionary)
            # Assuming all forecast lists in the dictionary have the same length
            forecast_steps = len(list(gru_forecasts.values())[0]) if gru_forecasts else 0

            if forecast_steps > 0:
                 # Generate future dates starting from the day after the last historical date
                 # Use periods = forecast_steps + 1 and slice from 1 to exclude the last historical date itself
                 forecast_dates = pd.date_range(start=last_historical_date, periods=forecast_steps + 1, freq=freq)[1:]

                 # Create a DataFrame from the forecast dictionary, using the generated future dates as the index
                 forecast_df = pd.DataFrame(gru_forecasts, index=forecast_dates)

                 # Rename the index to 'Date' for consistency
                 forecast_df.index.name = 'Date'
            else:
                 print("No forecast steps defined based on gru_forecasts dictionary.")
                 forecast_df = pd.DataFrame() # Create an empty DataFrame

        else:
             print("Error: prices_df index is not a DatetimeIndex. Cannot generate forecast dates.")
             forecast_df = pd.DataFrame() # Create an empty DataFrame


    else:
        print("No GRU forecasts were generated to combine.")
        forecast_df = pd.DataFrame() # Create an empty DataFrame if no forecasts

    # Combine the historical data and the forecast data
    # Use outer join to include all dates from both historical and forecast periods
    # The index from both DataFrames (DatetimeIndex) will be used for alignment
    combined_gru_df = pd.concat([historical_df, forecast_df], axis=0, join='outer')

    # Sort by date to ensure chronological order
    combined_gru_df.sort_index(inplace=True)

    print("\nCombined GRU DataFrame Head:")
    print(combined_gru_df.head(2))
    print("\nCombined GRU DataFrame Tail:")
    print(combined_gru_df.tail(2))
    print("\nCombined GRU DataFrame Shape:", combined_gru_df.shape)


#===============================================================================
# Save the Combined GRU Data to Google Sheets
#===============================================================================
import pandas as pd
import numpy as np
import gspread
from google.colab import auth
from google.auth import default
import warnings

warnings.filterwarnings("ignore") # Suppress warnings during model fitting

# Ensure combined_gru_df is available
if 'combined_gru_df' not in locals() or combined_gru_df.empty:
    print("Error: 'combined_gru_df' DataFrame not found or is empty. Cannot save GRU forecasts.")
else:
    # Ensure gc (gspread client) and sh (Google Sheet object) are available from previous cells
    # Assuming gc and sh are already initialized and authenticated
    if 'sh' not in locals():
        print("Google Sheet object 'sh' not found. Attempting to authenticate and open sheet.")
        try:
            auth.authenticate_user()
            creds, _ = default()
            gc = gspread.authorize(creds)
            sh = gc.open('Quant_Calc_Main')
            print("Authenticated and opened Google Sheet: Quant_Calc_Main")
        except Exception as e:
            print(f"Failed to authenticate or open Google Sheet: {e}")
            sh = None # Ensure sh is None if fallback fails

    if sh and not combined_gru_df.empty:
        try:
            print("\nAttempting to save combined GRU forecast results to Google Sheets...")
            sheet_title = 'GRU_FORECAST' # Specify the desired sheet title

            # Check if the worksheet with the specified title already exists
            try:
                worksheet_gru_forecast = sh.worksheet(sheet_title)
                print(f"Worksheet '{sheet_title}' already exists. Clearing existing data.")
                worksheet_gru_forecast.clear() # Clear existing data
            except gspread.WorksheetNotFound:
                # If not found, create a new one
                # Estimate the number of rows and columns needed
                num_rows = combined_gru_df.shape[0] + 1 # Data rows + header
                num_cols = combined_gru_df.shape[1] + 1 # Add 1 for the index ('Date')

                # gspread has a column limit, check if it's exceeded (usually 256)
                max_gspread_cols = 256
                if num_cols > max_gspread_cols:
                     print(f"Warning: Number of columns ({num_cols}) exceeds Google Sheets limit ({max_gspread_cols}). Saving only the first {max_gspread_cols} columns.")
                     num_cols_to_save = max_gspread_cols
                     # Include the index column + limited data columns
                     combined_df_limited = combined_gru_df.reset_index().iloc[:, :num_cols_to_save].copy()
                else:
                     num_cols_to_save = num_cols
                     combined_df_limited = combined_gru_df.reset_index().copy()


                worksheet_gru_forecast = sh.add_worksheet(title=sheet_title, rows=num_rows, cols=num_cols_to_save)
                print(f"Worksheet '{sheet_title}' created.")

            # Convert the DataFrame to a list of lists for gspread, including headers
            # Ensure 'Date' column is string and format it nicely
            # Use .dt accessor now that we've ensured it's datetime index
            if 'Date' in combined_df_limited.columns:
                 if pd.api.types.is_datetime64_any_dtype(combined_df_limited['Date']):
                      combined_df_limited['Date'] = combined_df_limited['Date'].dt.strftime('%Y-%m-%d')
                 combined_df_limited['Date'].fillna('', inplace=True) # Fill any potential NaT dates with an empty string or placeholder if needed


            # Convert all other data columns to string, handling potential NaN values
            # Replace NaN with empty string for cleaner representation in Google Sheets
            data_to_save = [combined_df_limited.columns.tolist()] + combined_df_limited.fillna('').astype(str).values.tolist()


            # Update the worksheet with the data
            # gspread expects a list of lists where each inner list is a row
            worksheet_gru_forecast.update(data_to_save)
            print(f"Combined historical data and GRU forecasts saved to worksheet '{sheet_title}'.")

        except Exception as e:
            print(f"Error saving combined GRU forecast results to Google Sheet: {e}")
    else:
        print("Google Sheet object 'sh' is not available. Cannot save results.")


#===============================================================================
# Visualize GRU Forecasts
#===============================================================================
import matplotlib.pyplot as plt
import pandas as pd

# Assuming combined_gru_df is available from the previous step

def plot_gru_forecast(df_combined, commodity_name):
    """
    Visualizes the historical data and GRU forecast for a specific commodity
    from the combined DataFrame.

    Args:
        df_combined (pd.DataFrame): DataFrame containing combined historical and
                                    forecast data with 'Date' as index.
        commodity_name (str): The name of the commodity to plot.
    """
    if commodity_name not in df_combined.columns:
        print(f"Error: Commodity '{commodity_name}' not found in the DataFrame.")
        return

    plt.figure(figsize=(12, 3))

    # Plot historical data (non-NaN values in the historical period)
    # Assuming historical data ends before the forecast period starts
    # Find the index where the forecast starts (first non-NaN value in the forecast period)
    # We can infer this from the first non-NaN value in the combined df after the end of original prices_df
    if 'prices_df' in locals() and not prices_df.empty:
        last_historical_date = prices_df.index.max()
        historical_data_to_plot = df_combined[df_combined.index <= last_historical_date][commodity_name].dropna()
        forecast_data_to_plot = df_combined[df_combined.index > last_historical_date][commodity_name].dropna()

        plt.plot(historical_data_to_plot.index, historical_data_to_plot.values, label='Historical Data', color='blue')
        plt.plot(forecast_data_to_plot.index, forecast_data_to_plot.values, label='GRU Forecast', color='red', linestyle='--')

        # Add a vertical line at the end of the historical data
        if last_historical_date:
            plt.axvline(last_historical_date, color='red', linestyle='--', label='Forecast Start')

    else:
        # If original prices_df is not available, just plot all data in combined_gru_df
        print("Warning: Original 'prices_df' not found. Plotting all combined data.")
        plt.plot(df_combined.index, df_combined[commodity_name], label=f'{commodity_name} Data & GRU Forecast')


    plt.title(f'GRU Forecast for {commodity_name}')
    plt.xlabel('Date')
    plt.ylabel(commodity_name)
    plt.legend()
    plt.grid(True)
    plt.show()

# Example usage:
# Assuming combined_gru_df is already loaded

# Get a list of commodities that were forecasted
# We can get this from the keys of the gru_forecasts dictionary
if 'gru_forecasts' in locals() and gru_forecasts:
    commodities_to_visualize = list(gru_forecasts.keys())[:5] # Visualize the first 5 forecasted commodities
    print(f"\nVisualizing GRU forecasts for the following commodities: {commodities_to_visualize}")
else:
    print("Error: 'gru_forecasts' dictionary not found or is empty. Cannot visualize forecasts.")
    commodities_to_visualize = []


if 'combined_gru_df' in locals() and not combined_gru_df.empty and commodities_to_visualize:
    for commodity in commodities_to_visualize:
        plot_gru_forecast(combined_gru_df, commodity)
else:
    print("Cannot visualize forecasts: combined_gru_df is not available or empty, or no commodities to visualize.")

"""**Reasoning**:
Define the architecture of the MAMBA model and create the PyTorch DataLoader for the prepared time series data.

## MAMBA FORECASTER

Implement a MAMBA forecaster for the time series data in `prices_df`, train it, generate forecasts, combine the forecasts with the historical data, save the result to a Google Sheet, and visualize the forecasts.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install mambapy einops

import torch
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler

# 1. Identify the numerical commodity columns
all_columns = prices_df.columns.tolist()
commodity_columns = [col for col in all_columns if pd.api.types.is_numeric_dtype(prices_df[col])]

print("Identified commodity columns for MAMBA data preparation:")
print(commodity_columns)

# 2. Define sequence length (number of past time steps to use for prediction)
sequence_length = 30 # Example: use the past 30 days to predict the next day

# 3. Initialize lists to store sequences and targets for all commodities
all_sequences = []
all_targets = []

# 4. Initialize a dictionary to store scalers for each commodity
scalers_mamba = {}

# 5. Iterate through each identified commodity column
print(f"\nPreparing data sequences for {len(commodity_columns)} commodities with sequence length {sequence_length}...")
for commodity in commodity_columns:
    print(f"  Processing commodity: {commodity}")

    # 3. For each commodity, extract the time series data
    series = prices_df[commodity].dropna()

    if len(series) < sequence_length + 1:
        print(f"  Skipping {commodity}: Insufficient data ({len(series)} data points) for sequence length {sequence_length}.")
        continue # Skip to the next commodity

    # 4. Scale the time series data
    scaler = MinMaxScaler()
    scaled_series = scaler.fit_transform(series.values.reshape(-1, 1)).flatten()
    scalers_mamba[commodity] = scaler # Store the scaler for this commodity

    # 5. Create input sequences (X) and corresponding target values (y)
    sequences = []
    targets = []
    for i in range(len(scaled_series) - sequence_length):
        seq = scaled_series[i:i + sequence_length]
        target = scaled_series[i + sequence_length]
        sequences.append(seq)
        targets.append(target)

    # 6. Store the prepared input sequences and target values for the current commodity
    all_sequences.extend(sequences)
    all_targets.extend(targets)
    print(f"    Generated {len(sequences)} sequences for {commodity}.")


# 7. Combine the input sequences and target values into PyTorch tensors
if not all_sequences:
    print("\nNo sequences were generated. Cannot create PyTorch tensors.")
    # Initialize tensors as empty if no data is processed
    X_tensor_mamba = torch.empty(0, sequence_length, dtype=torch.float32)
    y_tensor_mamba = torch.empty(0, dtype=torch.float32)

else:
    X_tensor_mamba = torch.tensor(all_sequences, dtype=torch.float32)
    y_tensor_mamba = torch.tensor(all_targets, dtype=torch.float32)
    print("\nCombined sequences and targets into PyTorch tensors.")
    print("Shape of X_tensor_mamba:", X_tensor_mamba.shape)
    print("Shape of y_tensor_mamba:", y_tensor_mamba.shape)


# 8. Confirm that the scalers dictionary is stored
if 'scalers_mamba' in locals():
    print("\nScalers for each commodity stored in 'scalers_mamba' dictionary.")

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from mambapy.mamba import Mamba, MambaConfig # Import MambaConfig

# Ensure X_tensor_mamba and y_tensor_mamba are available from the previous data preparation step
if 'X_tensor_mamba' not in locals() or 'y_tensor_mamba' not in locals() or X_tensor_mamba.shape[0] == 0:
    print("Error: X_tensor_mamba or y_tensor_mamba is not available or is empty. Cannot create Dataset and DataLoader.")
    # Initialize necessary variables to prevent errors in later steps
    train_loader_mamba = None
    model_mamba = None

else:
    print("X_tensor_mamba and y_tensor_mamba are available. Proceeding with Dataset, DataLoader, and Model definition.")

    # 1. Define a PyTorch Dataset class for MAMBA
    class TimeSeriesDatasetMamba(Dataset):
        def __init__(self, X, y):
            self.X = X
            self.y = y

        def __len__(self):
            return len(self.X)

        def __getitem__(self, idx):
            # MAMBA expects input shape (batch_size, sequence_length, input_dim)
            # Since we are forecasting a single time series value at a time, input_dim is 1
            # We need to unsqueeze the sequence tensor to add the input_dim dimension
            return self.X[idx].unsqueeze(-1), self.y[idx] # Add last dimension for input_dim

    # Instantiate the Dataset
    dataset_mamba = TimeSeriesDatasetMamba(X_tensor_mamba, y_tensor_mamba)
    print(f"\nDataset for MAMBA created with {len(dataset_mamba)} samples.")

    # 2. Create a PyTorch DataLoader
    batch_size = 32 # Define your desired batch size (can be the same as KAN)
    train_loader_mamba = DataLoader(dataset_mamba, batch_size=batch_size, shuffle=True)
    print(f"DataLoader for MAMBA created with batch size {batch_size}.")
    print(f"Number of batches in DataLoader: {len(train_loader_mamba)}")


    # 3. Define the MAMBA model architecture
    # Assuming sequence_length is available from the data preparation step
    if 'sequence_length' not in locals():
        print("Error: 'sequence_length' is not defined. Cannot define MAMBA model architecture.")
        model_mamba = None # Ensure model_mamba is None if sequence_length is missing
    else:
        print(f"\nDefining MAMBA model architecture with sequence length {sequence_length}...")

        # Define the configuration for the Mamba model using MambaConfig
        # Adjust d_model and n_layers as needed for your specific task and data complexity
        # d_model: The dimension of the model embeddings (must match the last dimension of the input tensor)
        # n_layers: The number of Mamba blocks
        # Increased n_layers from 2 to 4
        mamba_config = MambaConfig(d_model=1, n_layers=4) # Increased complexity

        # Instantiate the base Mamba model with the defined configuration
        try:
             base_mamba_model = Mamba(mamba_config) # Instantiate with MambaConfig
             print("Base Mamba model instantiated with config.")

             # Add a final linear layer to map MAMBA output to the prediction for the next time step
             # The output of Mamba is (batch_size, sequence_length, d_model)
             # We are interested in the last time step's output for prediction
             class MambaForecaster(nn.Module):
                 def __init__(self, mamba_model, output_dim=1):
                     super().__init__()
                     self.mamba = mamba_model
                     # The input dimension to the linear layer is the d_model of the Mamba output
                     self.fc = nn.Linear(mamba_model.config.d_model, output_dim) # Use mamba_model.config.d_model

                 def forward(self, x):
                     # x shape: (batch_size, sequence_length, input_dim) where input_dim is 1
                     mamba_output = self.mamba(x)
                     # We need the output from the last time step to predict the next value
                     last_step_output = mamba_output[:, -1, :] # Shape: (batch_size, mamba_model.config.d_model)
                     prediction = self.fc(last_step_output) # Shape: (batch_size, output_dim)
                     return prediction

             # Instantiate the MambaForecaster
             model_mamba = MambaForecaster(base_mamba_model, output_dim=1)


             print("MAMBA model architecture defined.")

             # 4. Instantiate the defined MAMBA model
             # model_mamba is already instantiated by the MambaForecaster call above if successful
             if model_mamba is not None:
                 print("\nMAMBA model instantiated.")
                 print("Model structure:")
                 print(model_mamba)
             else:
                 print("\nMAMBA model instantiation failed.")

        except Exception as e:
             print(f"Error instantiating Mamba model with config: {e}")
             model_mamba = None # Set to None if model creation fails

import torch.optim as optim
import torch.nn as nn

# Ensure model_mamba and train_loader_mamba are available from the previous step
if 'model_mamba' not in locals() or model_mamba is None:
    print("Error: MAMBA model is not defined. Cannot proceed with training.")
    # Finish the task with failure if the model is not defined
    # No need to initialize other variables as training won't start

elif 'train_loader_mamba' not in locals() or train_loader_mamba is None:
    print("Error: train_loader_mamba is not defined. Cannot proceed with training.")
    # Finish the task with failure if the train_loader_mamba is not defined
    # No need to initialize other variables as training won't start

else:
    print("\nProceeding with MAMBA model training.")

    # 1. Define the loss function (e.g., Mean Squared Error)
    criterion = nn.MSELoss()
    print("Loss function defined (MSELoss).")

    # 2. Define the optimizer (e.g., Adam)
    # Define your desired learning rate - MAMBA might benefit from different rates
    learning_rate = 0.0001 # Example learning rate, might need tuning
    optimizer = optim.Adam(model_mamba.parameters(), lr=learning_rate)
    print(f"Optimizer defined (Adam with learning rate {learning_rate}).")

    # 3. Set the number of training epochs
    num_epochs = 10 # Define your desired number of epochs - might need more or fewer
    print(f"Number of training epochs set to: {num_epochs}")

    # 4. Implement the training loop
    print("\nStarting MAMBA model training...")

    # Move model to the appropriate device if available (CPU or GPU)
    # Check if CUDA is available and use GPU if it is, otherwise use CPU
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model_mamba.to(device)
    print(f"Using device: {device}")


    for epoch in range(num_epochs):
        model_mamba.train() # Set the model to training mode
        running_loss = 0.0

        for i, data in enumerate(train_loader_mamba):
            inputs, targets = data

            # Move data to the device
            inputs, targets = inputs.to(device), targets.to(device)

            # Zero the gradients
            optimizer.zero_grad()

            # Forward pass
            outputs = model_mamba(inputs)

            # Calculate the loss
            # Ensure targets have the same shape as outputs (usually [batch_size, 1])
            # If targets is [batch_size], unsqueeze it to [batch_size, 1]
            if targets.ndim == 1:
                 targets = targets.unsqueeze(1)

            loss = criterion(outputs, targets)

            # Backward pass
            loss.backward()

            # Update the weights
            optimizer.step()

            # Accumulate the loss
            running_loss += loss.item()

        # Print the loss periodically
        print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader_mamba):.4f}")

    print("\nMAMBA model training complete.")

import torch
import os

# Define the path to save the model (e.g., in Google Drive)
# Ensure you have mounted Google Drive in a previous cell (e.g., cell Nv0Vvz_e5Ma2 or SzxtDdaQvm4n) if saving there
# This path should be consistent when loading the model later
mamba_model_save_path = '/content/drive/MyDrive/Colab Notebooks/quant_mamba_model.pth' # You can change the filename and path

# Ensure the directory exists
os.makedirs(os.path.dirname(mamba_model_save_path), exist_ok=True)

# Ensure the model is available from the training step
if 'model_mamba' not in locals() or model_mamba is None:
    print("Error: MAMBA model is not defined. Cannot save the model.")
else:
    try:
        # Save the model's state dictionary
        torch.save(model_mamba.state_dict(), mamba_model_save_path)
        print(f"MAMBA model state dictionary saved successfully to: {mamba_model_save_path}")
    except Exception as e:
        print(f"Error saving MAMBA model: {e}")

import torch
import pandas as pd
import numpy as np
import os
from mambapy.mamba import Mamba, MambaConfig # Import MambaConfig

# Define the path to the saved model
mamba_model_save_path = '/content/drive/MyDrive/Colab Notebooks/quant_mamba_model.pth' # This should match the save path

# Ensure MAMBA model architecture classes (Mamba, MambaConfig, MambaForecaster, etc.) are defined as in the model definition cell (cd723792)
# Ensure prices_df, commodity_columns, sequence_length, and scalers_mamba are available

# Check if the saved model file exists
if not os.path.exists(mamba_model_save_path):
    print(f"Error: Saved MAMBA model not found at {mamba_model_save_path}. Cannot generate forecasts using the saved model.")
    mamba_forecasts = {} # Initialize as empty
elif 'Mamba' not in locals() or 'MambaConfig' not in locals() or 'MambaForecaster' not in locals():
     print("Error: MAMBA model architecture classes (Mamba, MambaConfig, MambaForecaster) are not defined. Cannot load the saved model.")
     mamba_forecasts = {} # Initialize as empty
elif 'prices_df' not in locals():
    print("Error: 'prices_df' DataFrame not found. Cannot prepare forecast input sequences.")
    mamba_forecasts = {} # Initialize as empty
elif not isinstance(prices_df.index, pd.DatetimeIndex) or prices_df.index.name != 'Date':
    print("Error: 'prices_df' index is not a DatetimeIndex named 'Date'. Cannot proceed.")
    mamba_forecasts = {} # Initialize as empty
elif 'commodity_columns' not in locals() or not commodity_columns:
    print("Error: 'commodity_columns' list not found or is empty. Cannot prepare forecast input sequences.")
    mamba_forecasts = {} # Initialize as empty
elif 'sequence_length' not in locals():
    print("Error: 'sequence_length' is not defined. Cannot generate forecasts.")
    mamba_forecasts = {} # Initialize as empty
elif 'scalers_mamba' not in locals() or not scalers_mamba:
    print("Error: 'scalers_mamba' dictionary not found or is empty. Cannot inverse scale forecasts.")
    mamba_forecasts = {} # Initialize as empty

else:
    print("\nLoading the saved MAMBA model and generating future forecasts...")

    # Instantiate the MAMBA model architecture
    # Ensure the same MambaConfig is used as during training (d_model=1, n_layers=4 based on previous step)
    try:
        mamba_config = MambaConfig(d_model=1, n_layers=4)
        base_mamba_model = Mamba(mamba_config)
        # Instantiate the MambaForecaster with the base Mamba model
        model_mamba_loaded = MambaForecaster(base_mamba_model, output_dim=1) # Assuming MambaForecaster expects base model and output_dim

        print("MAMBA model architecture instantiated for loading.")

        # Load the saved state dictionary
        model_mamba_loaded.load_state_dict(torch.load(mamba_model_save_path))
        print(f"MAMBA model state dictionary loaded from {mamba_model_save_path}.")

        # Move the model to the appropriate device (CPU or GPU) if available
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model_mamba_loaded.to(device)
        model_mamba_loaded.eval() # Set the model to evaluation mode
        print(f"Using device for forecasting: {device}")


    except Exception as e:
        print(f"Error loading MAMBA model or setting up for forecasting: {e}")
        mamba_forecasts = {} # Initialize as empty
        model_mamba_loaded = None # Ensure model_mamba_loaded is None if loading fails


    # Initialize a dictionary to store the generated forecasts for each commodity
    mamba_forecasts = {}

    if model_mamba_loaded is not None:
         # Define the number of steps to forecast
         forecast_steps = 60 # This should align with other forecasters if possible
         print(f"\nGenerating {forecast_steps} steps into the future for each commodity...")

         # Iterate through each commodity for which an input sequence can be prepared
         print(f"Generating forecasts for {len(commodity_columns)} commodities...")
         with torch.no_grad(): # Disable gradient calculation during inference
             for commodity in commodity_columns:
                 print(f"  Forecasting for: {commodity}")

                 # Get the time series for the current commodity and drop missing values
                 series = prices_df[commodity].dropna()

                 # Check if the series has enough data points for the initial sequence
                 if len(series) < sequence_length:
                     print(f"    Warning: Skipping {commodity}. Insufficient data ({len(series)} data points) for sequence length {sequence_length}.")
                     continue # Skip to the next commodity

                 # Get the last `sequence_length` data points as the initial input sequence
                 last_sequence = series[-sequence_length:]

                 # Get the corresponding scaler for this commodity
                 if commodity not in scalers_mamba:
                      print(f"    Warning: Scaler not found for {commodity}. Skipping forecast.")
                      continue # Skip to the next commodity
                 scaler = scalers_mamba[commodity]

                 # Scale the last sequence using the corresponding scaler
                 # Reshape the sequence to be a 2D array for scaling
                 scaled_sequence = scaler.transform(last_sequence.values.reshape(-1, 1)).flatten()

                 # Convert the scaled sequence to a PyTorch tensor
                 # Reshape to [1, sequence_length, input_dim] as the model expects
                 # input_dim is 1 for single time series
                 sequence_tensor = torch.tensor(scaled_sequence, dtype=torch.float32).unsqueeze(0).unsqueeze(-1).to(device) # Add batch and input_dim dimensions and move to device

                 # Initialize a list to store the unscaled forecast values for this commodity
                 commodity_forecast_values = []

                 # Generate the forecast step by step
                 for _ in range(forecast_steps):
                     # Get the MAMBA model's prediction for the next step (scaled value)
                     predicted_scaled_value = model_mamba_loaded(sequence_tensor).item() # Get the scalar value

                     # Inverse scale the predicted value to the original price scale
                     # The scaler expects a 2D array, so reshape the scalar
                     predicted_original_value = scaler.inverse_transform([[predicted_scaled_value]])[0][0]

                     # Store the inverse-scaled forecast value
                     commodity_forecast_values.append(predicted_original_value)

                     # Update the input sequence for the next prediction
                     # Remove the oldest value and append the new scaled prediction
                     # The new prediction needs to be reshaped to match the input tensor shape
                     new_sequence_tensor = torch.cat((sequence_tensor[:, 1:, :], torch.tensor([[predicted_scaled_value]], dtype=torch.float32).unsqueeze(0).to(device)), dim=1)
                     sequence_tensor = new_sequence_tensor # Use the new sequence for the next step

                 # Store the generated forecast values for the current commodity
                 mamba_forecasts[commodity] = commodity_forecast_values
                 print(f"    Generated {len(commodity_forecast_values)} forecast values for {commodity}.")

         print("\nMAMBA forecasting complete.")

    else:
        print("MAMBA model is not available. Cannot generate forecasts.")


# You now have the generated forecasts in the 'mamba_forecasts' dictionary.

import pandas as pd
import numpy as np
import gspread
from google.colab import auth
from google.auth import default
import warnings

warnings.filterwarnings("ignore") # Suppress warnings during data manipulation and saving

# Ensure prices_df (original data) and mamba_forecasts (forecast dictionary) are available
if 'prices_df' not in locals():
    print("Error: 'prices_df' DataFrame not found. Cannot combine data.")
    # Finish the task with failure
elif 'mamba_forecasts' not in locals() or not mamba_forecasts:
    print("Error: 'mamba_forecasts' dictionary not found or is empty. Cannot combine data.")
    # Finish the task with failure
else:
    print("\nCombining original data and MAMBA forecasts...")

    # Prepare the original data: select commodity columns and ensure DatetimeIndex
    # Assuming the index of prices_df is already the correct DatetimeIndex ('Date')
    # and numeric columns are identified in commodity_columns list
    # Ensure commodity_columns is available; if not, try to infer from prices_df
    if 'commodity_columns' not in locals() or not commodity_columns:
         print("Warning: 'commodity_columns' not found. Attempting to infer from prices_df.")
         all_cols = prices_df.columns.tolist()
         cols_to_exclude = ['date', 'day_of_week', 'month', 'year',
                            'day_of_week_sin', 'day_of_week_cos',
                            'month_sin', 'month_cos',
                            'year_sin', 'year_cos']
         commodity_columns = [col for col in all_cols if col not in cols_to_exclude and pd.api.types.is_numeric_dtype(prices_df[col])]
         if not commodity_columns:
             print("Error: Could not identify numeric commodity columns from prices_df.")
             # Finish the task with failure
             combined_mamba_df = pd.DataFrame() # Initialize as empty
         else:
             print(f"Inferred commodity columns: {commodity_columns}")
             historical_df = prices_df[commodity_columns].copy()
    # Corrected indentation for this else block
    else:
         historical_df = prices_df[commodity_columns].copy()

    # Ensure historical_df has the correct DatetimeIndex ('Date')
    historical_df.index.name = 'Date'


    # Prepare the forecast data:
    # Create a DataFrame from the mamba_forecasts dictionary
    if mamba_forecasts:
        # Get the last date from the historical data to start the forecast index from the next day
        last_historical_date = prices_df.index.max()

        # Determine the frequency of the historical data
        # Ensure prices_df index is DatetimeIndex before inferring frequency
        if isinstance(prices_df.index, pd.DatetimeIndex):
            freq = pd.infer_freq(prices_df.index)
            if freq is None:
                 # Fallback to 'D' if frequency cannot be inferred (assuming daily data)
                 freq = 'D'
                 print(f"Warning: Could not infer frequency from historical data. Assuming '{freq}'.")
            else:
                print(f"Inferred historical data frequency: {freq}")

            # Define the number of steps to forecast (from the mamba_forecasts dictionary)
            # Assuming all forecast lists in the dictionary have the same length
            forecast_steps = len(list(mamba_forecasts.values())[0]) if mamba_forecasts else 0

            if forecast_steps > 0:
                 # Generate future dates starting from the day after the last historical date
                 # Use periods = forecast_steps + 1 and slice from 1 to exclude the last historical date itself
                 forecast_dates = pd.date_range(start=last_historical_date, periods=forecast_steps + 1, freq=freq)[1:]

                 # Create a DataFrame from the forecast dictionary, using the generated future dates as the index
                 forecast_df = pd.DataFrame(mamba_forecasts, index=forecast_dates)

                 # Rename the index to 'Date' for consistency
                 forecast_df.index.name = 'Date'
            else:
                 print("No forecast steps defined based on mamba_forecasts dictionary.")
                 forecast_df = pd.DataFrame() # Create an empty DataFrame

        else:
             print("Error: prices_df index is not a DatetimeIndex. Cannot generate forecast dates.")
             forecast_df = pd.DataFrame() # Create an empty DataFrame


    else:
        print("No MAMBA forecasts were generated to combine.")
        forecast_df = pd.DataFrame() # Create an empty DataFrame if no forecasts

    # Combine the historical data and the forecast data
    # Use outer join to include all dates from both historical and forecast periods
    # The index from both DataFrames (DatetimeIndex) will be used for alignment
    combined_mamba_df = pd.concat([historical_df, forecast_df], axis=0, join='outer')

    # Sort by date to ensure chronological order
    combined_mamba_df.sort_index(inplace=True)

    print("\nCombined MAMBA DataFrame Head:")
    print(combined_mamba_df.head(2))
    print("\nCombined MAMBA DataFrame Tail:")
    print(combined_mamba_df.tail(2))
    print("\nCombined MAMBA DataFrame Shape:", combined_mamba_df.shape)


#===============================================================================
# Save the Combined MAMBA Data to Google Sheets
#===============================================================================
import pandas as pd
import numpy as np
import gspread
from google.colab import auth
from google.auth import default
import warnings

warnings.filterwarnings("ignore") # Suppress warnings during model fitting

# Ensure combined_mamba_df is available
if 'combined_mamba_df' not in locals() or combined_mamba_df.empty:
    print("Error: 'combined_mamba_df' DataFrame not found or is empty. Cannot save MAMBA forecasts.")
else:
    # Ensure gc (gspread client) and sh (Google Sheet object) are available from previous cells
    # Assuming gc and sh are already initialized and authenticated
    if 'sh' not in locals():
        print("Google Sheet object 'sh' not found. Attempting to authenticate and open sheet.")
        try:
            auth.authenticate_user()
            creds, _ = default()
            gc = gspread.authorize(creds)
            sh = gc.open('Quant_Calc_Main')
            print("Authenticated and opened Google Sheet: Quant_Calc_Main")
        except Exception as e:
            print(f"Failed to authenticate or open Google Sheet: {e}")
            sh = None # Ensure sh is None if fallback fails

    if sh and not combined_mamba_df.empty:
        try:
            print("\nAttempting to save combined MAMBA forecast results to Google Sheets...")
            sheet_title = 'MAMBA_FORECAST' # Specify the desired sheet title

            # Check if the worksheet with the specified title already exists
            try:
                worksheet_mamba_forecast = sh.worksheet(sheet_title)
                print(f"Worksheet '{sheet_title}' already exists. Clearing existing data.")
                worksheet_mamba_forecast.clear() # Clear existing data
            except gspread.WorksheetNotFound:
                # If not found, create a new one
                # Estimate the number of rows and columns needed
                num_rows = combined_mamba_df.shape[0] + 1 # Data rows + header
                num_cols = combined_mamba_df.shape[1] + 1 # Add 1 for the index ('Date')

                # gspread has a column limit, check if it's exceeded (usually 256)
                max_gspread_cols = 256
                if num_cols > max_gspread_cols:
                     print(f"Warning: Number of columns ({num_cols}) exceeds Google Sheets limit ({max_gspread_cols}). Saving only the first {max_gspread_cols} columns.")
                     num_cols_to_save = max_gspread_cols
                     # Include the index column + limited data columns
                     combined_df_limited = combined_mamba_df.reset_index().iloc[:, :num_cols_to_save].copy()
                else:
                     num_cols_to_save = num_cols
                     combined_df_limited = combined_mamba_df.reset_index().copy()


                worksheet_mamba_forecast = sh.add_worksheet(title=sheet_title, rows=num_rows, cols=num_cols_to_save)
                print(f"Worksheet '{sheet_title}' created.")

            # Convert the DataFrame to a list of lists for gspread, including headers
            # Ensure 'Date' column is string and format it nicely
            # Use .dt accessor now that we've ensured it's datetime index
            if 'Date' in combined_df_limited.columns:
                 if pd.api.types.is_datetime64_any_dtype(combined_df_limited['Date']):
                      combined_df_limited['Date'] = combined_df_limited['Date'].dt.strftime('%Y-%m-%d')
                 combined_df_limited['Date'].fillna('', inplace=True) # Fill any potential NaT dates with an empty string or placeholder if needed


            # Convert all other data columns to string, handling potential NaN values
            # Replace NaN with empty string for cleaner representation in Google Sheets
            data_to_save = [combined_df_limited.columns.tolist()] + combined_df_limited.fillna('').astype(str).values.tolist()


            # Update the worksheet with the data
            # gspread expects a list of lists where each inner list is a row
            worksheet_mamba_forecast.update(data_to_save)
            print(f"Combined historical data and MAMBA forecasts saved to worksheet '{sheet_title}'.")

        except Exception as e:
            print(f"Error saving combined MAMBA forecast results to Google Sheet: {e}")
    else:
        print("Google Sheet object 'sh' is not available. Cannot save results.")

import matplotlib.pyplot as plt
import pandas as pd

# Assuming combined_mamba_df is available from the previous step

def plot_mamba_forecast(df_combined, commodity_name):
    """
    Visualizes the historical data and MAMBA forecast for a specific commodity
    from the combined DataFrame.

    Args:
        df_combined (pd.DataFrame): DataFrame containing combined historical and
                                    forecast data with 'Date' as index.
        commodity_name (str): The name of the commodity to plot.
    """
    if commodity_name not in df_combined.columns:
        print(f"Error: Commodity '{commodity_name}' not found in the DataFrame.")
        return

    plt.figure(figsize=(12, 3))

    # Plot historical data (non-NaN values in the historical period)
    # Assuming historical data ends before the forecast period starts
    # Find the index where the forecast starts (first non-NaN value in the forecast period)
    # We can infer this from the first non-NaN value in the combined df after the end of original prices_df
    if 'prices_df' in locals() and not prices_df.empty:
        last_historical_date = prices_df.index.max()
        historical_data_to_plot = df_combined[df_combined.index <= last_historical_date][commodity_name].dropna()
        forecast_data_to_plot = df_combined[df_combined.index > last_historical_date][commodity_name].dropna()

        plt.plot(historical_data_to_plot.index, historical_data_to_plot.values, label='Historical Data', color='blue')
        plt.plot(forecast_data_to_plot.index, forecast_data_to_plot.values, label='MAMBA Forecast', color='red', linestyle='--')

        # Add a vertical line at the end of the historical data
        if last_historical_date:
            plt.axvline(last_historical_date, color='red', linestyle='--', label='Forecast Start')

    else:
        # If original prices_df is not available, just plot all data in combined_mamba_df
        print("Warning: Original 'prices_df' not found. Plotting all combined data.")
        plt.plot(df_combined.index, df_combined[commodity_name], label=f'{commodity_name} Data & MAMBA Forecast')


    plt.title(f'MAMBA Forecast for {commodity_name}')
    plt.xlabel('Date')
    plt.ylabel(commodity_name)
    plt.legend()
    plt.grid(True)
    plt.show()

# Example usage:
# Assuming combined_mamba_df is already loaded

# Get a list of commodities that were forecasted
# We can get this from the keys of the mamba_forecasts dictionary
if 'mamba_forecasts' in locals() and mamba_forecasts:
    commodities_to_visualize = list(mamba_forecasts.keys())[:5] # Visualize the first 5 forecasted commodities
    print(f"\nVisualizing MAMBA forecasts for the following commodities: {commodities_to_visualize}")
else:
    print("Error: 'mamba_forecasts' dictionary not found or is empty. Cannot visualize forecasts.")
    commodities_to_visualize = []


if 'combined_mamba_df' in locals() and not combined_mamba_df.empty and commodities_to_visualize:
    for commodity in commodities_to_visualize:
        plot_mamba_forecast(combined_mamba_df, commodity)
else:
    print("Cannot visualize forecasts: combined_mamba_df is not available or empty, or no commodities to visualize.")

"""## Summary:

### Data Analysis Key Findings
*   The `mambapy` library was successfully installed.
*   An error occurred in the MAMBA forecaster section where the data preparation step failed due to the `prices_df` variable not being defined, indicating an interruption in the execution flow.

### Insights or Next Steps
*   The immediate next step is to re-run the data preparation cell to ensure `prices_df` and other essential variables for the MAMBA forecaster are correctly initialized before proceeding with further analysis.

# Task
```python
import torch
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler

# 1. Identify the numerical commodity columns
all_columns = prices_df.columns.tolist()
commodity_columns = [col for col in all_columns if pd.api.types.is_numeric_dtype(prices_df[col])]

print("Identified commodity columns for MAMBA data preparation:")
print(commodity_columns)

# 2. Define sequence length (number of past time steps to use for prediction)
sequence_length = 30 # Example: use the past 30 days to predict the next day

# 3. Initialize lists to store sequences and targets for all commodities
all_sequences = []
all_targets = []

# 4. Initialize a dictionary to store scalers for each commodity
scalers_mamba = {}

# 5. Iterate through each identified commodity column
print(f"\nPreparing data sequences for {len(commodity_columns)} commodities with sequence length {sequence_length}...")
for commodity in commodity_columns:
    print(f"  Processing commodity: {commodity}")

    # 3. For each commodity, extract the time series data
    series = prices_df[commodity].dropna()

    if len(series) < sequence_length + 1:
        print(f"  Skipping {commodity}: Insufficient data ({len(series)} data points) for sequence length {sequence_length}.")
        continue # Skip to the next commodity

    # 4. Scale the time series data
    scaler = MinMaxScaler()
    scaled_series = scaler.fit_transform(series.values.reshape(-1, 1)).flatten()
    scalers_mamba[commodity] = scaler # Store the scaler for this commodity

    # 5. Create input sequences (X) and corresponding target values (y)
    sequences = []
    targets = []
    for i in range(len(scaled_series) - sequence_length):
        seq = scaled_series[i:i + sequence_length]
        target = scaled_series[i + sequence_length]
        sequences.append(seq)
        targets.append(target)

    # 6. Store the prepared input sequences and target values for the current commodity
    all_sequences.extend(sequences)
    all_targets.extend(targets)
    print(f"    Generated {len(sequences)} sequences for {commodity}.")


# 7. Combine the input sequences and target values into PyTorch tensors
if not all_sequences:
    print("\nNo sequences were generated. Cannot create PyTorch tensors.")
    # Initialize tensors as empty if no data is processed
    X_tensor_mamba = torch.empty(0, sequence_length, dtype=torch.float32)
    y_tensor_mamba = torch.empty(0, dtype=torch.float32)

else:
    X_tensor_mamba = torch.tensor(all_sequences, dtype=torch.float32)
    y_tensor_mamba = torch.tensor(all_targets, dtype=torch.float32)
    print("\nCombined sequences and targets into PyTorch tensors.")
    print("Shape of X_tensor_mamba:", X_tensor_mamba.shape)
    print("Shape of y_tensor_mamba:", y_tensor_mamba.shape)


# 8. Confirm that the scalers dictionary is stored
if 'scalers_mamba' in locals():
    print("\nScalers for each commodity stored in 'scalers_mamba' dictionary.")
```
"""

# Load and validate the data
import pandas as pd, numpy as np

# Authenticate to Google Sheets
auth.authenticate_user()
creds, _ = default()
gc = gspread.authorize(creds)

#===============================================================================
# Load the dataset (corrected to load ALL_CLEAN_DATA.csv)
#===============================================================================
# Assuming 'ALL_CLEAN_DATA.csv' was saved successfully in the previous step
prices_df = concatenated_df.copy() #pd.read_csv('ALL_CLEAN_DATA.csv')

# Convert 'date' column to datetime objects
prices_df['date'] = pd.to_datetime(prices_df['date'], format='%d-%m-%y', errors='coerce')
prices_df.set_index('date', inplace=True)
prices_df.index.name = 'Date' # Rename index for consistency

#===============================================================================
# Clean column names: remove 'CFR', 'FCA', 'FOB', 'CIF', 'Refrigerated'
#===============================================================================
prices_df.columns = prices_df.columns.str.replace(' CFR', '', regex=False)
prices_df.columns = prices_df.columns.str.replace(' FCA', '', regex=False)
prices_df.columns = prices_df.columns.str.replace(' FOB', '', regex=False)
prices_df.columns = prices_df.columns.str.replace(' CIF', '', regex=False)
prices_df.columns = prices_df.columns.str.replace(' Refrigerated', '', regex=False)
# Also remove leading/trailing spaces that might result from replacements
prices_df.columns = prices_df.columns.str.strip()

# Ensure data is in ascending order
prices_df.sort_index(ascending=True, inplace=True)
print("Commodity Prices DataFrame Head after sorting by Date (ascending):")
print(prices_df.head(2))

#===============================================================================
# Delete empty columns and rows (as requested by the user)
#===============================================================================
print(f"\nOriginal shape of prices_df: {prices_df.shape}")

# Drop columns that are entirely NaN
prices_df.dropna(axis=1, how='all', inplace=True)
print(f"Shape after dropping entirely empty columns: {prices_df.shape}")

# Drop rows that are entirely NaN
prices_df.dropna(axis=0, how='all', inplace=True)
print(f"Shape after dropping entirely empty rows: {prices_df.shape}")

print("\nPrices DataFrame after cleaning empty columns and rows:")
print(prices_df.head())